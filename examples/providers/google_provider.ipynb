{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Provider Guide ðŸŒŸ\n",
    "\n",
    "This notebook demonstrates how to use Google's Gemini models through different hosting options:\n",
    "- **AI Studio**: Google's direct API service for developers\n",
    "- **Vertex AI**: Enterprise-grade platform through Google Cloud\n",
    "\n",
    "## Overview\n",
    "\n",
    "Google offers Gemini models through multiple platforms, each designed for different use cases:\n",
    "\n",
    "### AI Studio (Direct Google Hosting)\n",
    "- **Easy setup** with simple API key authentication\n",
    "- **Latest models** available immediately upon release\n",
    "- **Developer-friendly** with minimal configuration\n",
    "- **Free tier** available for experimentation\n",
    "- **Global access** without regional restrictions\n",
    "\n",
    "### Vertex AI (Google Cloud Hosting)\n",
    "- **Enterprise features** including VPC integration\n",
    "- **Regional deployment** for compliance and latency\n",
    "- **Advanced monitoring** and logging capabilities\n",
    "- **Custom model fine-tuning** options\n",
    "- **IAM integration** for access control\n",
    "- **Batch processing** and high-throughput scenarios\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Studio (Direct Google Hosting)\n",
    "\n",
    "### Authentication\n",
    "AI Studio uses the `GEMINI_API_KEY` environment variable for authentication. You can get your API key from [Google AI Studio](https://aistudio.google.com/).\n",
    "\n",
    "### Available Models\n",
    "- `gemini-2.0-flash-001` - Latest and fastest Gemini model\n",
    "- `gemini-1.5-pro-001` - Most capable model for complex tasks\n",
    "- `gemini-1.5-flash-001` - Balanced performance and speed\n",
    "- `gemini-1.0-pro` - Previous generation model\n",
    "\n",
    "### Key Features\n",
    "- **Multimodal capabilities** (text, images, audio, video)\n",
    "- **Function calling** for tool integration\n",
    "- **Large context windows** (up to 2M tokens)\n",
    "- **Code generation** and execution\n",
    "- **Reasoning capabilities** with chain-of-thought\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://github.com/googleapis/python-genai)\n",
    "- [AI Studio](https://aistudio.google.com/)\n",
    "- [Model Documentation](https://ai.google.dev/models/gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI Studio Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: Okay, imagine you're trying to find the best way to get through a maze.\n",
      "\n",
      "**Classical Computing is like:**\n",
      "\n",
      "You're a single person, and you try one path at a time. You go down one route, hit a dead end, backtrack, and try another route. You have to explore each path individually until you find the right one.\n",
      "\n",
      "**Quantum Computing is like:**\n",
      "\n",
      "Instead of you being one person, you're a **super-powered ghost** that can exist in multiple places *at the same time*. You can explore **all possible paths through the maze simultaneously**.  You're not just going down one route; you're going down *every* route, *at the same time*, as a probability.\n",
      "\n",
      "*   **Classical bit:** Like a light switch - either ON (1) or OFF (0).\n",
      "*   **Qubit (quantum bit):** Like a dimmer switch. It can be ON (1), OFF (0), or somewhere *in between* (represented as a combination of both, called a *superposition*).\n",
      "\n",
      "Because of this \"in-between\" state, qubits can hold much more information than a classical bit.\n",
      "\n",
      "*   **Classical Computing:** You try each path one after another until you find the solution.\n",
      "*   **Quantum Computing:** All paths are tried at once, and the *probability* of the solution increases while the probability of wrong paths decreases. Finally, when you \"measure\" the quantum state, the solution with the highest probability is revealed.\n",
      "\n",
      "**So, in summary:**\n",
      "\n",
      "Quantum computers don't just try each option one at a time. They use the principles of quantum mechanics to explore many possibilities *simultaneously*, potentially finding solutions to complex problems much faster than traditional computers.\n",
      "\n",
      "**Important Considerations:**\n",
      "\n",
      "*   **Fragile:** Like a very sensitive instrument, quantum states are easily disrupted (by heat, vibration, etc.), which can cause errors. This is a major challenge in building practical quantum computers.\n",
      "*   **Specific Problems:** Quantum computers aren't a replacement for regular computers. They are better suited for specific types of problems, such as simulating molecules, breaking encryption, and optimizing complex systems.\n",
      "\n",
      "I hope this analogy helps!\n",
      "\n",
      "Finish reason: FinishReason.STOP\n",
      "Usage: 11 + 463 = 474 tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candidates': [{'content': {'parts': [{'video_metadata': None,\n",
       "      'thought': None,\n",
       "      'inline_data': None,\n",
       "      'code_execution_result': None,\n",
       "      'executable_code': None,\n",
       "      'file_data': None,\n",
       "      'function_call': None,\n",
       "      'function_response': None,\n",
       "      'text': 'Okay, imagine you\\'re trying to find the best way to get through a maze.\\n\\n**Classical Computing is like:**\\n\\nYou\\'re a single person, and you try one path at a time. You go down one route, hit a dead end, backtrack, and try another route. You have to explore each path individually until you find the right one.\\n\\n**Quantum Computing is like:**\\n\\nInstead of you being one person, you\\'re a **super-powered ghost** that can exist in multiple places *at the same time*. You can explore **all possible paths through the maze simultaneously**.  You\\'re not just going down one route; you\\'re going down *every* route, *at the same time*, as a probability.\\n\\n*   **Classical bit:** Like a light switch - either ON (1) or OFF (0).\\n*   **Qubit (quantum bit):** Like a dimmer switch. It can be ON (1), OFF (0), or somewhere *in between* (represented as a combination of both, called a *superposition*).\\n\\nBecause of this \"in-between\" state, qubits can hold much more information than a classical bit.\\n\\n*   **Classical Computing:** You try each path one after another until you find the solution.\\n*   **Quantum Computing:** All paths are tried at once, and the *probability* of the solution increases while the probability of wrong paths decreases. Finally, when you \"measure\" the quantum state, the solution with the highest probability is revealed.\\n\\n**So, in summary:**\\n\\nQuantum computers don\\'t just try each option one at a time. They use the principles of quantum mechanics to explore many possibilities *simultaneously*, potentially finding solutions to complex problems much faster than traditional computers.\\n\\n**Important Considerations:**\\n\\n*   **Fragile:** Like a very sensitive instrument, quantum states are easily disrupted (by heat, vibration, etc.), which can cause errors. This is a major challenge in building practical quantum computers.\\n*   **Specific Problems:** Quantum computers aren\\'t a replacement for regular computers. They are better suited for specific types of problems, such as simulating molecules, breaking encryption, and optimizing complex systems.\\n\\nI hope this analogy helps!\\n'}],\n",
       "    'role': 'model'},\n",
       "   'citation_metadata': None,\n",
       "   'finish_message': None,\n",
       "   'token_count': None,\n",
       "   'finish_reason': <FinishReason.STOP: 'STOP'>,\n",
       "   'url_context_metadata': None,\n",
       "   'avg_logprobs': -0.5814466703015321,\n",
       "   'grounding_metadata': None,\n",
       "   'index': None,\n",
       "   'logprobs_result': None,\n",
       "   'safety_ratings': None}],\n",
       " 'create_time': None,\n",
       " 'response_id': None,\n",
       " 'model_version': 'gemini-2.0-flash-001',\n",
       " 'prompt_feedback': None,\n",
       " 'usage_metadata': {'cache_tokens_details': None,\n",
       "  'cached_content_token_count': None,\n",
       "  'candidates_token_count': 463,\n",
       "  'candidates_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 463}],\n",
       "  'prompt_token_count': 11,\n",
       "  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 11}],\n",
       "  'thoughts_token_count': None,\n",
       "  'tool_use_prompt_token_count': None,\n",
       "  'tool_use_prompt_tokens_details': None,\n",
       "  'total_token_count': 474,\n",
       "  'traffic_type': None},\n",
       " 'automatic_function_calling_history': [],\n",
       " 'parsed': None}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Initialize AI Studio client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Basic content generation\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='Explain quantum computing in simple terms, using an analogy.'\n",
    ")\n",
    "\n",
    "print(\"=== AI Studio Response ===\")\n",
    "print(f\"Model: {response.model_version}\")\n",
    "\n",
    "if response.candidates and len(response.candidates) > 0:\n",
    "    candidate = response.candidates[0]\n",
    "    print(f\"Content: {candidate.content.parts[0].text}\")\n",
    "    print(f\"Finish reason: {candidate.finish_reason}\")\n",
    "    \n",
    "    # Safety ratings (if available)\n",
    "    if candidate.safety_ratings:\n",
    "        print(f\"Safety ratings: {len(candidate.safety_ratings)} categories checked\")\n",
    "\n",
    "print(f\"Usage: {response.usage_metadata.prompt_token_count} + {response.usage_metadata.candidates_token_count} = {response.usage_metadata.total_token_count} tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Configuration with Messages\n",
    "\n",
    "For more complex interactions, you can use the message format with system instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Advanced Configuration Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: The neon rain of Neo-Kyoto slicked the chrome streets, reflecting a kaleidoscope of fractured light onto Detective Kaito Ishikawa's trench coat. He stood on the precipice of the Sky-Gardens, the artificial flora shimmering unnaturally under the electric downpour, a stark contrast to the lifeless corpse sprawled at his feet. It wasn't the synthetic cherry blossoms raining down on the scene, nor the panoramic view of the sprawling cityscape that caught his attention, but the single, archaic katana plunged through the victim's chest, a rusty relic in a world of laser pistols and bio-engineered weaponry. This was more than just a murder; it was a message, scrawled in blood and steel, a ghost from the past haunting the hyper-modern present.\n",
      "\n",
      "Average log probability: -0.3911\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Create a conversation with system instructions\n",
    "system_instruction = \"You are a helpful AI assistant specializing in creative writing. Always provide vivid, engaging responses.\"\n",
    "\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Write a compelling opening paragraph for a mystery novel set in a futuristic city.\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Configuration for more control\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=system_instruction,\n",
    "    temperature=0.8,  # More creative responses\n",
    "    top_p=0.95,\n",
    "    max_output_tokens=200,\n",
    "    candidate_count=1\n",
    ")\n",
    "\n",
    "advanced_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=messages,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Advanced Configuration Response ===\")\n",
    "print(f\"Model: {advanced_response.model_version}\")\n",
    "if advanced_response.candidates:\n",
    "    print(f\"Content: {advanced_response.candidates[0].content.parts[0].text}\")\n",
    "    print(f\"Average log probability: {advanced_response.candidates[0].avg_logprobs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "Google's Gemini models support function calling for interacting with external tools and APIs.\n",
    "\n",
    "### Key Features:\n",
    "- **Function declarations** using JSON schemas\n",
    "- **Parallel function calls** for efficiency\n",
    "- **Automatic function calling** mode\n",
    "- **Complex parameter schemas** with nested objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Function Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Call Response ===\n",
      "Model: gemini-2.0-flash\n",
      "\n",
      "Part 1: Part\n",
      "  Function: get_weather\n",
      "  Arguments: {'location': 'Tokyo, Japan'}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather function\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Tokyo, Japan\"\n",
    "            },\n",
    "            \"units\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"Temperature units\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create function tool\n",
    "tools = types.Tool(function_declarations=[weather_function])\n",
    "config = types.GenerateContentConfig(tools=[tools])\n",
    "\n",
    "# Initial request with function\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"What is the weather like in Tokyo today?\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "function_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=messages,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Function Call Response ===\")\n",
    "print(f\"Model: {function_response.model_version}\")\n",
    "\n",
    "if function_response.candidates and len(function_response.candidates) > 0:\n",
    "    candidate = function_response.candidates[0]\n",
    "    \n",
    "    for i, part in enumerate(candidate.content.parts):\n",
    "        print(f\"\\nPart {i+1}: {part.__class__.__name__}\")\n",
    "        \n",
    "        if part.text:\n",
    "            print(f\"  Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"  Function: {part.function_call.name}\")\n",
    "            print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append(candidate.content)\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Execution ===\n",
      "Function: get_weather\n",
      "Location: Tokyo, Japan\n",
      "Units: celsius\n",
      "Result: 22Â°C, partly cloudy with light breeze\n",
      "\n",
      "=== Final Response ===\n",
      "Gemini's response: It is 22Â°C in Tokyo, partly cloudy with a light breeze.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate function execution\n",
    "def execute_weather_function(location, units=\"celsius\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"22Â°C, partly cloudy with light breeze\"\n",
    "        else:\n",
    "            return \"72Â°F, partly cloudy with light breeze\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Find function call in the response\n",
    "function_call = None\n",
    "if function_response.candidates:\n",
    "    for part in function_response.candidates[0].content.parts:\n",
    "        if part.function_call:\n",
    "            function_call = part.function_call\n",
    "            break\n",
    "\n",
    "if function_call:\n",
    "    # Execute the function\n",
    "    location = function_call.args.get(\"location\")\n",
    "    units = function_call.args.get(\"units\", \"celsius\")\n",
    "    weather_result = execute_weather_function(location, units)\n",
    "    \n",
    "    print(f\"=== Function Execution ===\")\n",
    "    print(f\"Function: {function_call.name}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Units: {units}\")\n",
    "    print(f\"Result: {weather_result}\")\n",
    "    \n",
    "    # Create function response\n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name=function_call.name,\n",
    "        response={\"result\": weather_result}\n",
    "    )\n",
    "    \n",
    "    messages.append(\n",
    "        types.Content(role=\"user\", parts=[function_response_part])\n",
    "    )\n",
    "    \n",
    "    # Get final response with function results\n",
    "    final_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=messages,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    if final_response.candidates:\n",
    "        print(f\"Gemini's response: {final_response.candidates[0].content.parts[0].text}\")\n",
    "else:\n",
    "    print(\"No function calls found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multiple Functions Response ===\n",
      "Model: gemini-2.0-flash\n",
      "\n",
      "Content parts: 2\n",
      "\n",
      "Part 1: Part\n",
      "  Function: calculator\n",
      "  Arguments: {'operation': 'multiply', 'b': 12, 'a': 35}\n",
      "\n",
      "Part 2: Part\n",
      "  Function: get_current_time\n",
      "  Arguments: {'timezone': 'Asia/Tokyo'}\n",
      "\n",
      "Total function calls: 2\n",
      "Usage: 113 + 16 = 129 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define multiple functions\n",
    "calculator_function = {\n",
    "    \"name\": \"calculator\",\n",
    "    \"description\": \"Perform basic mathematical operations\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"description\": \"The mathematical operation\"\n",
    "            },\n",
    "            \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "            \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "        },\n",
    "        \"required\": [\"operation\", \"a\", \"b\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "time_function = {\n",
    "    \"name\": \"get_current_time\",\n",
    "    \"description\": \"Get the current time in a specified timezone\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"timezone\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Timezone (e.g., UTC, America/New_York, Asia/Tokyo)\",\n",
    "                \"default\": \"UTC\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create tools with multiple functions\n",
    "multi_tools = types.Tool(function_declarations=[\n",
    "    weather_function,\n",
    "    calculator_function,\n",
    "    time_function\n",
    "])\n",
    "multi_config = types.GenerateContentConfig(tools=[multi_tools])\n",
    "\n",
    "# Request that might use multiple functions\n",
    "multi_function_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"Calculate 35 * 12, and also tell me what time it is in Tokyo\")]\n",
    "        )\n",
    "    ],\n",
    "    config=multi_config\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Functions Response ===\")\n",
    "print(f\"Model: {multi_function_response.model_version}\")\n",
    "\n",
    "if multi_function_response.candidates:\n",
    "    candidate = multi_function_response.candidates[0]\n",
    "    \n",
    "    print(f\"\\nContent parts: {len(candidate.content.parts)}\")\n",
    "    function_calls = []\n",
    "    \n",
    "    for i, part in enumerate(candidate.content.parts):\n",
    "        print(f\"\\nPart {i+1}: {part.__class__.__name__}\")\n",
    "        \n",
    "        if part.text:\n",
    "            print(f\"  Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"  Function: {part.function_call.name}\")\n",
    "            print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "            function_calls.append(part.function_call)\n",
    "    \n",
    "    print(f\"\\nTotal function calls: {len(function_calls)}\")\n",
    "    print(f\"Usage: {multi_function_response.usage_metadata.prompt_token_count} + {multi_function_response.usage_metadata.candidates_token_count} = {multi_function_response.usage_metadata.total_token_count} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and PDF-files\n",
    "\n",
    "### Images\n",
    "First let's try sending an image from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a close-up of an ant standing on a rough surface. The ant is dark in color, with visible segments on its body and long, thin legs. Its antennae are also noticeable. The background is blurry, with warm tones of brown and red. The depth of field is shallow, focusing attention on the ant.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "image1_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "\n",
    "# Download image from URL instead of trying to open it as a local file\n",
    "response = requests.get(image1_url)\n",
    "image_bytes = response.content\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=[\n",
    "        types.Part.from_bytes(\n",
    "            data=image_bytes,\n",
    "            mime_type='image/jpeg',\n",
    "        ),\n",
    "        \"What's this image about? \"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just pass a URL if it's a public image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a pipe organ console. It has multiple keyboards (manuals), rows of stops, foot pedals, and other controls.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "image_path = \"https://goo.gle/instrument-img\"\n",
    "image_bytes = requests.get(image_path).content\n",
    "image = types.Part.from_bytes(\n",
    "  data=image_bytes, mime_type=\"image/jpeg\"\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\"What is this image?\", image],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is the instruction manual for the Nintendo Game Boy Color. It outlines the components of the device, how to install batteries, use Game Paks, change the screen color, and connect with other Game Boy systems using the Game Link cable or the infrared communication port. It also provides troubleshooting tips, warranty information, and a parts list with an order form. The manual emphasizes safety and compatibility and directs users to contact Nintendo's Consumer Assistance Hotline or authorized repair centers for additional help.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize this document\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=open(\"./assets/gameboy_color.pdf\", \"rb\").read(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the instruction booklet for the Game Boy Color, outlining the features of the device, how to use it, and troubleshooting steps. It also includes warranty information, and a parts list and order form for replacement parts or accessories.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myfile = client.files.upload(file=\"./assets/gameboy_color.pdf\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=[\"Describe this PDF in 2 sentences\", myfile]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Hosting\n",
    "\n",
    "### Setup and Authentication\n",
    "Vertex AI requires Google Cloud project configuration and uses service account authentication or Application Default Credentials (ADC).\n",
    "\n",
    "### Key Differences:\n",
    "- **Project ID**: Must specify your Google Cloud project\n",
    "- **Location**: Choose deployment region for compliance/latency\n",
    "- **Enterprise Features**: VPC integration, audit logging, and advanced monitoring\n",
    "- **IAM Integration**: Fine-grained access control\n",
    "- **Custom Models**: Support for fine-tuned and custom models\n",
    "\n",
    "### Available Regions:\n",
    "- `us-central1` - United States (primary)\n",
    "- `us-east4` - United States (secondary)\n",
    "- `europe-west1` - Europe (primary)\n",
    "- `asia-northeast1` - Asia Pacific (Tokyo)\n",
    "- See [Vertex AI Locations](https://cloud.google.com/vertex-ai/docs/general/locations) for complete list\n",
    "\n",
    "### Resources:\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Gemini on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini)\n",
    "- [Google Cloud Console](https://console.cloud.google.com/vertex-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vertex AI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI Configuration:\n",
      "  Project: vectrix-401014\n",
      "  Location: europe-west1\n",
      "\n",
      "=== Vertex AI Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: While both Vertex AI and AI Studio (formerly known as Colab Enterprise) offer tools for building and deploying machine learning models, **Vertex AI is generally the preferred choice for enterprise applications due to its comprehensive features, scalability, and governance capabilities.**  Here's a breakdown of the benefits:\n",
      "\n",
      "**1. End-to-End ML Lifecycle Support:**\n",
      "\n",
      "*   **Vertex AI:** Designed to manage the entire ML lifecycle, from data preparation and model training to deployment, monitoring, and continuous improvement. It provides a unified platform for all stages, promoting collaboration and efficiency.\n",
      "*   **AI Studio:** Primarily focuses on interactive development and experimentation. While it offers some deployment capabilities, it's not as comprehensive or geared towards managing production-level ML pipelines.\n",
      "\n",
      "**2. Scalability and Reliability:**\n",
      "\n",
      "*   **Vertex AI:** Built for enterprise-scale deployments. It leverages Google Cloud's infrastructure to provide scalable and reliable resources for training and serving models, ensuring high availability and performance.  Offers robust auto-scaling options.\n",
      "*   **AI Studio:** Scalability is more limited. It's better suited for smaller-scale projects or prototyping, not designed for handling massive datasets or high-volume prediction requests.\n",
      "\n",
      "**3. Enterprise-Grade Security and Governance:**\n",
      "\n",
      "*   **Vertex AI:** Integrates tightly with Google Cloud's security and governance controls. Features like Identity and Access Management (IAM), encryption, audit logging, and compliance certifications provide a secure and compliant environment for sensitive data and applications. Vertex AI provides better governance and control over the model development and deployment processes.\n",
      "*   **AI Studio:** While it inherits some security features from Google Cloud, its focus is less on enterprise-grade governance and compliance. It's generally less suited for applications that require strict security or regulatory oversight.\n",
      "\n",
      "**4. Collaboration and Team Management:**\n",
      "\n",
      "*   **Vertex AI:** Provides robust collaboration features, allowing teams to work together on ML projects effectively. Version control, access control, and centralized model management streamline the development process. Includes features like project sharing, access control lists, and experiment tracking, making it easier for teams to collaborate and manage their work.\n",
      "*   **AI Studio:** Collaboration is more limited, typically relying on sharing notebooks or code snippets.  Version control and team management are less mature compared to Vertex AI.\n",
      "\n",
      "**5. Model Management and Monitoring:**\n",
      "\n",
      "*   **Vertex AI:** Offers comprehensive model management capabilities, including model versioning, registry, and deployment management. Model monitoring tools provide insights into model performance, drift detection, and data quality, enabling proactive maintenance and improvement. Includes model versioning, deployment tracking, and real-time monitoring of model performance and data drift.\n",
      "*   **AI Studio:** Lacks the robust model management and monitoring features found in Vertex AI. It's more challenging to track model versions, monitor performance in production, or detect issues like data drift.\n",
      "\n",
      "**6. Pre-Built Services and Integrations:**\n",
      "\n",
      "*   **Vertex AI:** Offers a rich set of pre-built services and integrations with other Google Cloud products, such as BigQuery, Cloud Storage, and Cloud Dataflow. This simplifies data preparation, feature engineering, and model deployment.  Provides a wide range of pre-trained models, AutoML capabilities, and integration with other Google Cloud services for data storage, processing, and analysis.\n",
      "*   **AI Studio:** Focuses more on code-based development, requiring users to build more of their solutions from scratch.\n",
      "\n",
      "**7. Experiment Tracking and Reproducibility:**\n",
      "\n",
      "*   **Vertex AI:** Features advanced experiment tracking capabilities, allowing data scientists to log experiments, track metrics, and compare results. This promotes reproducibility and helps identify the best performing models. Includes dedicated tools for tracking experiments, hyperparameter tuning, and model evaluation, making it easier to reproduce and optimize ML models.\n",
      "*   **AI Studio:** While you can track experiments, it's less structured and requires more manual effort compared to Vertex AI.\n",
      "\n",
      "**8. Support and SLAs:**\n",
      "\n",
      "*   **Vertex AI:** As a core Google Cloud service, Vertex AI comes with robust support and service level agreements (SLAs), ensuring business continuity and peace of mind.\n",
      "*   **AI Studio:** While supported by Google, the support levels and SLAs may be less comprehensive than those offered for Vertex AI.\n",
      "\n",
      "**In summary:**\n",
      "\n",
      "| Feature          | Vertex AI                                     | AI Studio (Colab Enterprise)                  |\n",
      "|-------------------|-----------------------------------------------|---------------------------------------------|\n",
      "| **Scope**        | End-to-end ML lifecycle, production focus     | Interactive development, prototyping        |\n",
      "| **Scalability**   | Enterprise-scale                              | Limited                                     |\n",
      "| **Security**      | Enterprise-grade, comprehensive controls      | Basic, inherits from Google Cloud            |\n",
      "| **Governance**    | Robust, built-in compliance                 | Limited                                     |\n",
      "| **Collaboration** | Advanced team management, version control    | Basic, notebook sharing                      |\n",
      "| **Model Mgmt**    | Comprehensive versioning, registry, monitoring| Limited, manual tracking                   |\n",
      "| **Integrations**  | Extensive with Google Cloud ecosystem       | Fewer built-in integrations                  |\n",
      "| **Pricing**       | Pay-as-you-go, tailored to production workloads| More limited pricing options                |\n",
      "\n",
      "**When to use AI Studio:**\n",
      "\n",
      "*   Exploring new datasets\n",
      "*   Prototyping ML models quickly\n",
      "*   Educational purposes\n",
      "*   Smaller-scale personal projects\n",
      "\n",
      "**When to use Vertex AI:**\n",
      "\n",
      "*   Building and deploying production-grade ML applications\n",
      "*   Managing large datasets and complex ML pipelines\n",
      "*   Requiring enterprise-grade security and governance\n",
      "*   Collaborating with teams on ML projects\n",
      "*   Monitoring and maintaining model performance in production\n",
      "\n",
      "For enterprise applications that demand scalability, security, governance, and end-to-end lifecycle management, **Vertex AI is the clear winner.** It provides a robust and comprehensive platform for building, deploying, and managing machine learning models at scale. AI Studio can serve as a helpful tool in the initial stages of experimentation and prototyping before transitioning to Vertex AI for production-ready deployments.\n",
      "\n",
      "Finish reason: FinishReason.STOP\n",
      "Usage: 14 + 1270 = 1284 tokens\n",
      "Create time: 2025-05-30 12:56:08.801958+00:00\n",
      "Response ID: 6Ko5aKb5MNuprNcP2Ky5kAI\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text=\"While both Vertex AI and AI Studio (formerly known as Colab Enterprise) offer tools for building and deploying machine learning models, **Vertex AI is generally the preferred choice for enterprise applications due to its comprehensive features, scalability, and governance capabilities.**  Here's a breakdown of the benefits:\\n\\n**1. End-to-End ML Lifecycle Support:**\\n\\n*   **Vertex AI:** Designed to manage the entire ML lifecycle, from data preparation and model training to deployment, monitoring, and continuous improvement. It provides a unified platform for all stages, promoting collaboration and efficiency.\\n*   **AI Studio:** Primarily focuses on interactive development and experimentation. While it offers some deployment capabilities, it's not as comprehensive or geared towards managing production-level ML pipelines.\\n\\n**2. Scalability and Reliability:**\\n\\n*   **Vertex AI:** Built for enterprise-scale deployments. It leverages Google Cloud's infrastructure to provide scalable and reliable resources for training and serving models, ensuring high availability and performance.  Offers robust auto-scaling options.\\n*   **AI Studio:** Scalability is more limited. It's better suited for smaller-scale projects or prototyping, not designed for handling massive datasets or high-volume prediction requests.\\n\\n**3. Enterprise-Grade Security and Governance:**\\n\\n*   **Vertex AI:** Integrates tightly with Google Cloud's security and governance controls. Features like Identity and Access Management (IAM), encryption, audit logging, and compliance certifications provide a secure and compliant environment for sensitive data and applications. Vertex AI provides better governance and control over the model development and deployment processes.\\n*   **AI Studio:** While it inherits some security features from Google Cloud, its focus is less on enterprise-grade governance and compliance. It's generally less suited for applications that require strict security or regulatory oversight.\\n\\n**4. Collaboration and Team Management:**\\n\\n*   **Vertex AI:** Provides robust collaboration features, allowing teams to work together on ML projects effectively. Version control, access control, and centralized model management streamline the development process. Includes features like project sharing, access control lists, and experiment tracking, making it easier for teams to collaborate and manage their work.\\n*   **AI Studio:** Collaboration is more limited, typically relying on sharing notebooks or code snippets.  Version control and team management are less mature compared to Vertex AI.\\n\\n**5. Model Management and Monitoring:**\\n\\n*   **Vertex AI:** Offers comprehensive model management capabilities, including model versioning, registry, and deployment management. Model monitoring tools provide insights into model performance, drift detection, and data quality, enabling proactive maintenance and improvement. Includes model versioning, deployment tracking, and real-time monitoring of model performance and data drift.\\n*   **AI Studio:** Lacks the robust model management and monitoring features found in Vertex AI. It's more challenging to track model versions, monitor performance in production, or detect issues like data drift.\\n\\n**6. Pre-Built Services and Integrations:**\\n\\n*   **Vertex AI:** Offers a rich set of pre-built services and integrations with other Google Cloud products, such as BigQuery, Cloud Storage, and Cloud Dataflow. This simplifies data preparation, feature engineering, and model deployment.  Provides a wide range of pre-trained models, AutoML capabilities, and integration with other Google Cloud services for data storage, processing, and analysis.\\n*   **AI Studio:** Focuses more on code-based development, requiring users to build more of their solutions from scratch.\\n\\n**7. Experiment Tracking and Reproducibility:**\\n\\n*   **Vertex AI:** Features advanced experiment tracking capabilities, allowing data scientists to log experiments, track metrics, and compare results. This promotes reproducibility and helps identify the best performing models. Includes dedicated tools for tracking experiments, hyperparameter tuning, and model evaluation, making it easier to reproduce and optimize ML models.\\n*   **AI Studio:** While you can track experiments, it's less structured and requires more manual effort compared to Vertex AI.\\n\\n**8. Support and SLAs:**\\n\\n*   **Vertex AI:** As a core Google Cloud service, Vertex AI comes with robust support and service level agreements (SLAs), ensuring business continuity and peace of mind.\\n*   **AI Studio:** While supported by Google, the support levels and SLAs may be less comprehensive than those offered for Vertex AI.\\n\\n**In summary:**\\n\\n| Feature          | Vertex AI                                     | AI Studio (Colab Enterprise)                  |\\n|-------------------|-----------------------------------------------|---------------------------------------------|\\n| **Scope**        | End-to-end ML lifecycle, production focus     | Interactive development, prototyping        |\\n| **Scalability**   | Enterprise-scale                              | Limited                                     |\\n| **Security**      | Enterprise-grade, comprehensive controls      | Basic, inherits from Google Cloud            |\\n| **Governance**    | Robust, built-in compliance                 | Limited                                     |\\n| **Collaboration** | Advanced team management, version control    | Basic, notebook sharing                      |\\n| **Model Mgmt**    | Comprehensive versioning, registry, monitoring| Limited, manual tracking                   |\\n| **Integrations**  | Extensive with Google Cloud ecosystem       | Fewer built-in integrations                  |\\n| **Pricing**       | Pay-as-you-go, tailored to production workloads| More limited pricing options                |\\n\\n**When to use AI Studio:**\\n\\n*   Exploring new datasets\\n*   Prototyping ML models quickly\\n*   Educational purposes\\n*   Smaller-scale personal projects\\n\\n**When to use Vertex AI:**\\n\\n*   Building and deploying production-grade ML applications\\n*   Managing large datasets and complex ML pipelines\\n*   Requiring enterprise-grade security and governance\\n*   Collaborating with teams on ML projects\\n*   Monitoring and maintaining model performance in production\\n\\nFor enterprise applications that demand scalability, security, governance, and end-to-end lifecycle management, **Vertex AI is the clear winner.** It provides a robust and comprehensive platform for building, deploying, and managing machine learning models at scale. AI Studio can serve as a helpful tool in the initial stages of experimentation and prototyping before transitioning to Vertex AI for production-ready deployments.\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=-0.5013558935931348, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=datetime.datetime(2025, 5, 30, 12, 56, 8, 801958, tzinfo=TzInfo(UTC)), response_id='6Ko5aKb5MNuprNcP2Ky5kAI', model_version='gemini-2.0-flash-001', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=1270, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1270)], prompt_token_count=14, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=14)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=1284, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Vertex AI client\n",
    "project_id = os.environ.get(\"GCP_PROJECT_ID\")\n",
    "location = os.environ.get(\"GCP_LOCATION\", \"us-central1\")\n",
    "\n",
    "print(f\"Vertex AI Configuration:\")\n",
    "print(f\"  Project: {project_id}\")\n",
    "print(f\"  Location: {location}\")\n",
    "\n",
    "vertex_client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=project_id,\n",
    "    location=location\n",
    ")\n",
    "\n",
    "# Basic content generation on Vertex AI\n",
    "vertex_response = vertex_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='Explain the benefits of using Vertex AI over AI Studio for enterprise applications.'\n",
    ")\n",
    "\n",
    "print(\"\\n=== Vertex AI Response ===\")\n",
    "print(f\"Model: {vertex_response.model_version}\")\n",
    "\n",
    "if vertex_response.candidates and len(vertex_response.candidates) > 0:\n",
    "    candidate = vertex_response.candidates[0]\n",
    "    print(f\"Content: {candidate.content.parts[0].text}\")\n",
    "    print(f\"Finish reason: {candidate.finish_reason}\")\n",
    "\n",
    "print(f\"Usage: {vertex_response.usage_metadata.prompt_token_count} + {vertex_response.usage_metadata.candidates_token_count} = {vertex_response.usage_metadata.total_token_count} tokens\")\n",
    "\n",
    "# Additional Vertex AI metadata\n",
    "if hasattr(vertex_response, 'create_time'):\n",
    "    print(f\"Create time: {vertex_response.create_time}\")\n",
    "if hasattr(vertex_response, 'response_id'):\n",
    "    print(f\"Response ID: {vertex_response.response_id}\")\n",
    "\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "vertex_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Function Calling\n",
    "\n",
    "Function calling works identically on Vertex AI as with AI Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vertex AI Function Call ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Function: calculator\n",
      "Operation: 88 divide 11\n",
      "Arguments: {'operation': 'divide', 'a': 88, 'b': 11}\n",
      "\n",
      "âœ… Function calling works identically on Vertex AI!\n"
     ]
    }
   ],
   "source": [
    "# Function calling works the same on Vertex AI\n",
    "vertex_tools = types.Tool(function_declarations=[calculator_function])\n",
    "vertex_config = types.GenerateContentConfig(tools=[vertex_tools])\n",
    "\n",
    "vertex_function_response = vertex_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"Calculate 88 divided by 11\")]\n",
    "        )\n",
    "    ],\n",
    "    config=vertex_config\n",
    ")\n",
    "\n",
    "print(\"=== Vertex AI Function Call ===\")\n",
    "print(f\"Model: {vertex_function_response.model_version}\")\n",
    "\n",
    "if vertex_function_response.candidates:\n",
    "    candidate = vertex_function_response.candidates[0]\n",
    "    \n",
    "    for part in candidate.content.parts:\n",
    "        if part.text:\n",
    "            print(f\"Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"Function: {part.function_call.name}\")\n",
    "            args = dict(part.function_call.args)\n",
    "            print(f\"Operation: {args['a']} {args['operation']} {args['b']}\")\n",
    "            print(f\"Arguments: {args}\")\n",
    "\n",
    "print(f\"\\nâœ… Function calling works identically on Vertex AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: AI Studio vs Vertex AI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison: AI Studio vs Vertex AI ===\n",
      "\n",
      "ðŸŸ¡ AI Studio:\n",
      "Model: gemini-2.0-flash-001\n",
      "Response: Artificial intelligence is the development of computer systems capable of performing tasks that typically require human intelligence, such as learning, problem-solving, and decision-making. These systems often achieve this through algorithms and models that allow them to analyze data, identify patterns, and adapt to new information.\n",
      "\n",
      "Tokens: 9 + 57\n",
      "\n",
      "ðŸ”µ Vertex AI:\n",
      "Model: gemini-2.0-flash-001\n",
      "Response: Artificial intelligence is the simulation of human intelligence processes by machines, especially computer systems. These processes include learning, reasoning, and problem-solving, enabling machines to perform tasks that typically require human intelligence.\n",
      "\n",
      "Tokens: 9 + 40\n",
      "\n",
      "ðŸ’¡ Both use identical APIs and response formats!\n",
      "ðŸ’¡ Key differences: authentication, enterprise features, and regional deployment\n"
     ]
    }
   ],
   "source": [
    "test_content = \"Explain artificial intelligence in exactly 2 sentences.\"\n",
    "\n",
    "print(\"=== Comparison: AI Studio vs Vertex AI ===\")\n",
    "\n",
    "# AI Studio\n",
    "studio_response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=test_content\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ¡ AI Studio:\")\n",
    "print(f\"Model: {studio_response.model_version}\")\n",
    "if studio_response.candidates:\n",
    "    print(f\"Response: {studio_response.candidates[0].content.parts[0].text}\")\n",
    "print(f\"Tokens: {studio_response.usage_metadata.prompt_token_count} + {studio_response.usage_metadata.candidates_token_count}\")\n",
    "\n",
    "# Vertex AI\n",
    "vertex_response = vertex_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=test_content\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Vertex AI:\")\n",
    "print(f\"Model: {vertex_response.model_version}\")\n",
    "if vertex_response.candidates:\n",
    "    print(f\"Response: {vertex_response.candidates[0].content.parts[0].text}\")\n",
    "print(f\"Tokens: {vertex_response.usage_metadata.prompt_token_count} + {vertex_response.usage_metadata.candidates_token_count}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Key differences: authentication, enterprise features, and regional deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
