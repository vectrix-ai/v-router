{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Provider Guide ðŸŒŸ\n",
    "\n",
    "This notebook demonstrates how to use Google's Gemini models through different hosting options:\n",
    "- **AI Studio**: Google's direct API service for developers\n",
    "- **Vertex AI**: Enterprise-grade platform through Google Cloud\n",
    "\n",
    "## Overview\n",
    "\n",
    "Google offers Gemini models through multiple platforms, each designed for different use cases:\n",
    "\n",
    "### AI Studio (Direct Google Hosting)\n",
    "- **Easy setup** with simple API key authentication\n",
    "- **Latest models** available immediately upon release\n",
    "- **Developer-friendly** with minimal configuration\n",
    "- **Free tier** available for experimentation\n",
    "- **Global access** without regional restrictions\n",
    "\n",
    "### Vertex AI (Google Cloud Hosting)\n",
    "- **Enterprise features** including VPC integration\n",
    "- **Regional deployment** for compliance and latency\n",
    "- **Advanced monitoring** and logging capabilities\n",
    "- **Custom model fine-tuning** options\n",
    "- **IAM integration** for access control\n",
    "- **Batch processing** and high-throughput scenarios\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Studio (Direct Google Hosting)\n",
    "\n",
    "### Authentication\n",
    "AI Studio uses the `GEMINI_API_KEY` environment variable for authentication. You can get your API key from [Google AI Studio](https://aistudio.google.com/).\n",
    "\n",
    "### Available Models\n",
    "- `gemini-2.0-flash-001` - Latest and fastest Gemini model\n",
    "- `gemini-1.5-pro-001` - Most capable model for complex tasks\n",
    "- `gemini-1.5-flash-001` - Balanced performance and speed\n",
    "- `gemini-1.0-pro` - Previous generation model\n",
    "\n",
    "### Key Features\n",
    "- **Multimodal capabilities** (text, images, audio, video)\n",
    "- **Function calling** for tool integration\n",
    "- **Large context windows** (up to 2M tokens)\n",
    "- **Code generation** and execution\n",
    "- **Reasoning capabilities** with chain-of-thought\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://github.com/googleapis/python-genai)\n",
    "- [AI Studio](https://aistudio.google.com/)\n",
    "- [Model Documentation](https://ai.google.dev/models/gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI Studio Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: Okay, let's explain quantum computing with a coin analogy.\n",
      "\n",
      "**Classical Computing (Regular Coins):**\n",
      "\n",
      "Imagine you have a regular coin. It can be either Heads (0) or Tails (1). That's it.  A classical computer uses bits, which are like these coins. Each bit can only be in one state at a time â€“ either 0 or 1.  Think of a light switch: it's either on (1) or off (0).  You can only do one thing at a time with each switch. If you want to represent the number 3, you need to use multiple switches turned on and off in a specific pattern (like 11 in binary).  Every calculation is done step-by-step, using these \"on/off\" switches in a determined sequence.\n",
      "\n",
      "**Quantum Computing (Magical Spinning Coins):**\n",
      "\n",
      "Now, imagine a magical coin that is spinning in the air.  While it's spinning, it's *both* Heads and Tails *at the same time*. It's not *either* heads *or* tails, but a **superposition** of both.  Think of it as a combination of both possibilities, with a certain probability of landing on heads or tails when you finally stop it and look.\n",
      "\n",
      "These magical coins are like **qubits**, the building blocks of a quantum computer.  Instead of just 0 or 1, a qubit can be 0, 1, or a combination of both states simultaneously thanks to superposition.\n",
      "\n",
      "Here's where it gets even more interesting:\n",
      "\n",
      "*   **Entanglement:** Imagine you have two of these spinning coins.  When you entangle them, they become linked.  If you stop one coin and it lands on Heads, the other coin *instantly* lands on Tails (even if they are miles apart).  This interconnectedness allows quantum computers to perform calculations in ways classical computers can't.\n",
      "*   **Many Coins at Once:** With classical computers, if you want to explore multiple solutions to a problem, you have to test each solution one at a time.  With quantum computers, because each qubit can be in multiple states at once, you can explore many solutions *simultaneously* using the power of superposition.\n",
      "\n",
      "**The Advantage:**\n",
      "\n",
      "*   **Classical Computer:** Think of it like searching a maze one path at a time. If you hit a dead end, you back up and try another path.\n",
      "*   **Quantum Computer:** Think of it like flooding the maze with water. The water explores *all* paths simultaneously.  You just need a way to filter out the water that leads to dead ends and find the path that leads to the exit.\n",
      "\n",
      "**In simpler terms:**\n",
      "\n",
      "Classical computers are like simple light switches that can be either on or off. Quantum computers are like magical dials that can be set to many positions at once, and when these dials are linked together they can solve immensely complicated problems in ways that classical computers can't.\n",
      "\n",
      "**Important Notes:**\n",
      "\n",
      "*   Quantum computers are not going to replace your laptop or phone. They are designed for specific types of complex calculations that are impossible or impractical for classical computers.\n",
      "*   Building and programming quantum computers is extremely challenging.\n",
      "*   The \"spinning coin\" analogy is a simplification. Qubits aren't literally spinning, and the physics of superposition and entanglement are far more complex.\n",
      "\n",
      "Finish reason: FinishReason.STOP\n",
      "Usage: 11 + 705 = 716 tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candidates': [{'content': {'parts': [{'video_metadata': None,\n",
       "      'thought': None,\n",
       "      'inline_data': None,\n",
       "      'code_execution_result': None,\n",
       "      'executable_code': None,\n",
       "      'file_data': None,\n",
       "      'function_call': None,\n",
       "      'function_response': None,\n",
       "      'text': 'Okay, let\\'s explain quantum computing with a coin analogy.\\n\\n**Classical Computing (Regular Coins):**\\n\\nImagine you have a regular coin. It can be either Heads (0) or Tails (1). That\\'s it.  A classical computer uses bits, which are like these coins. Each bit can only be in one state at a time â€“ either 0 or 1.  Think of a light switch: it\\'s either on (1) or off (0).  You can only do one thing at a time with each switch. If you want to represent the number 3, you need to use multiple switches turned on and off in a specific pattern (like 11 in binary).  Every calculation is done step-by-step, using these \"on/off\" switches in a determined sequence.\\n\\n**Quantum Computing (Magical Spinning Coins):**\\n\\nNow, imagine a magical coin that is spinning in the air.  While it\\'s spinning, it\\'s *both* Heads and Tails *at the same time*. It\\'s not *either* heads *or* tails, but a **superposition** of both.  Think of it as a combination of both possibilities, with a certain probability of landing on heads or tails when you finally stop it and look.\\n\\nThese magical coins are like **qubits**, the building blocks of a quantum computer.  Instead of just 0 or 1, a qubit can be 0, 1, or a combination of both states simultaneously thanks to superposition.\\n\\nHere\\'s where it gets even more interesting:\\n\\n*   **Entanglement:** Imagine you have two of these spinning coins.  When you entangle them, they become linked.  If you stop one coin and it lands on Heads, the other coin *instantly* lands on Tails (even if they are miles apart).  This interconnectedness allows quantum computers to perform calculations in ways classical computers can\\'t.\\n*   **Many Coins at Once:** With classical computers, if you want to explore multiple solutions to a problem, you have to test each solution one at a time.  With quantum computers, because each qubit can be in multiple states at once, you can explore many solutions *simultaneously* using the power of superposition.\\n\\n**The Advantage:**\\n\\n*   **Classical Computer:** Think of it like searching a maze one path at a time. If you hit a dead end, you back up and try another path.\\n*   **Quantum Computer:** Think of it like flooding the maze with water. The water explores *all* paths simultaneously.  You just need a way to filter out the water that leads to dead ends and find the path that leads to the exit.\\n\\n**In simpler terms:**\\n\\nClassical computers are like simple light switches that can be either on or off. Quantum computers are like magical dials that can be set to many positions at once, and when these dials are linked together they can solve immensely complicated problems in ways that classical computers can\\'t.\\n\\n**Important Notes:**\\n\\n*   Quantum computers are not going to replace your laptop or phone. They are designed for specific types of complex calculations that are impossible or impractical for classical computers.\\n*   Building and programming quantum computers is extremely challenging.\\n*   The \"spinning coin\" analogy is a simplification. Qubits aren\\'t literally spinning, and the physics of superposition and entanglement are far more complex.\\n'}],\n",
       "    'role': 'model'},\n",
       "   'citation_metadata': None,\n",
       "   'finish_message': None,\n",
       "   'token_count': None,\n",
       "   'finish_reason': <FinishReason.STOP: 'STOP'>,\n",
       "   'url_context_metadata': None,\n",
       "   'avg_logprobs': -0.5484822591145834,\n",
       "   'grounding_metadata': None,\n",
       "   'index': None,\n",
       "   'logprobs_result': None,\n",
       "   'safety_ratings': None}],\n",
       " 'create_time': None,\n",
       " 'response_id': None,\n",
       " 'model_version': 'gemini-2.0-flash-001',\n",
       " 'prompt_feedback': None,\n",
       " 'usage_metadata': {'cache_tokens_details': None,\n",
       "  'cached_content_token_count': None,\n",
       "  'candidates_token_count': 705,\n",
       "  'candidates_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 705}],\n",
       "  'prompt_token_count': 11,\n",
       "  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 11}],\n",
       "  'thoughts_token_count': None,\n",
       "  'tool_use_prompt_token_count': None,\n",
       "  'tool_use_prompt_tokens_details': None,\n",
       "  'total_token_count': 716,\n",
       "  'traffic_type': None},\n",
       " 'automatic_function_calling_history': [],\n",
       " 'parsed': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Initialize AI Studio client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Basic content generation\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='Explain quantum computing in simple terms, using an analogy.'\n",
    ")\n",
    "\n",
    "print(\"=== AI Studio Response ===\")\n",
    "print(f\"Model: {response.model_version}\")\n",
    "\n",
    "if response.candidates and len(response.candidates) > 0:\n",
    "    candidate = response.candidates[0]\n",
    "    print(f\"Content: {candidate.content.parts[0].text}\")\n",
    "    print(f\"Finish reason: {candidate.finish_reason}\")\n",
    "    \n",
    "    # Safety ratings (if available)\n",
    "    if candidate.safety_ratings:\n",
    "        print(f\"Safety ratings: {len(candidate.safety_ratings)} categories checked\")\n",
    "\n",
    "print(f\"Usage: {response.usage_metadata.prompt_token_count} + {response.usage_metadata.candidates_token_count} = {response.usage_metadata.total_token_count} tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Configuration with Messages\n",
    "\n",
    "For more complex interactions, you can use the message format with system instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Advanced Configuration Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: The neon rain tasted like static and regret on my tongue as I stood on the Skywalk, the shimmering arteries of Neo-Kyoto pulsing beneath my worn boots. A glitch in the matrix of light, a flicker in the holographic geishas advertising synth-sake, that was all it took to distract me for a heartbeat. A heartbeat was all it took for her to fall. Now, all that remained of Anya Volkov, the darling of the Zaibatsu elite, was a crimson stain blooming on the chrome pavement a thousand stories below, and the nagging suspicion that gravity hadn't acted alone.\n",
      "\n",
      "Average log probability: -0.5384\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Create a conversation with system instructions\n",
    "system_instruction = \"You are a helpful AI assistant specializing in creative writing. Always provide vivid, engaging responses.\"\n",
    "\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Write a compelling opening paragraph for a mystery novel set in a futuristic city.\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Configuration for more control\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=system_instruction,\n",
    "    temperature=0.8,  # More creative responses\n",
    "    top_p=0.95,\n",
    "    max_output_tokens=200,\n",
    "    candidate_count=1\n",
    ")\n",
    "\n",
    "advanced_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=messages,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Advanced Configuration Response ===\")\n",
    "print(f\"Model: {advanced_response.model_version}\")\n",
    "if advanced_response.candidates:\n",
    "    print(f\"Content: {advanced_response.candidates[0].content.parts[0].text}\")\n",
    "    print(f\"Average log probability: {advanced_response.candidates[0].avg_logprobs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "Google's Gemini models support function calling for interacting with external tools and APIs.\n",
    "\n",
    "### Key Features:\n",
    "- **Function declarations** using JSON schemas\n",
    "- **Parallel function calls** for efficiency\n",
    "- **Automatic function calling** mode\n",
    "- **Complex parameter schemas** with nested objects\n",
    "\n",
    "### Force function calling\n",
    "The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the function_calling_config.\n",
    "\n",
    "- AUTO (Default): The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n",
    "- ANY: The model is constrained to always predict a function call and guarantee function schema adherence. If allowed_function_names is not specified, the model can choose from any of the provided function declarations. If allowed_function_names is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n",
    "- NONE: The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Function Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Call Response ===\n",
      "Model: gemini-2.0-flash\n",
      "\n",
      "Part 1: Part\n",
      "  Function: get_weather\n",
      "  Arguments: {'location': 'Tokyo, Japan'}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather function\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Tokyo, Japan\"\n",
    "            },\n",
    "            \"units\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"Temperature units\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create function tool\n",
    "tools = types.Tool(function_declarations=[weather_function])\n",
    "\n",
    "# Create the config\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config=types.FunctionCallingConfig(\n",
    "        mode=\"ANY\", allowed_function_names=[\"get_weather\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "config = types.GenerateContentConfig(tools=[tools], tool_config=types.ToolConfig)\n",
    "\n",
    "# Initial request with function\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"What is the weather like in Tokyo today?\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "function_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=messages,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Function Call Response ===\")\n",
    "print(f\"Model: {function_response.model_version}\")\n",
    "\n",
    "if function_response.candidates and len(function_response.candidates) > 0:\n",
    "    candidate = function_response.candidates[0]\n",
    "    \n",
    "    for i, part in enumerate(candidate.content.parts):\n",
    "        print(f\"\\nPart {i+1}: {part.__class__.__name__}\")\n",
    "        \n",
    "        if part.text:\n",
    "            print(f\"  Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"  Function: {part.function_call.name}\")\n",
    "            print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append(candidate.content)\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Execution ===\n",
      "Function: get_weather\n",
      "Location: Tokyo, Japan\n",
      "Units: celsius\n",
      "Result: 22Â°C, partly cloudy with light breeze\n",
      "\n",
      "=== Final Response ===\n",
      "Gemini's response: It is 22Â°C in Tokyo, partly cloudy with a light breeze.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simulate function execution\n",
    "def execute_weather_function(location, units=\"celsius\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"22Â°C, partly cloudy with light breeze\"\n",
    "        else:\n",
    "            return \"72Â°F, partly cloudy with light breeze\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Find function call in the response\n",
    "function_call = None\n",
    "if function_response.candidates:\n",
    "    for part in function_response.candidates[0].content.parts:\n",
    "        if part.function_call:\n",
    "            function_call = part.function_call\n",
    "            break\n",
    "\n",
    "if function_call:\n",
    "    # Execute the function\n",
    "    location = function_call.args.get(\"location\")\n",
    "    units = function_call.args.get(\"units\", \"celsius\")\n",
    "    weather_result = execute_weather_function(location, units)\n",
    "    \n",
    "    print(f\"=== Function Execution ===\")\n",
    "    print(f\"Function: {function_call.name}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Units: {units}\")\n",
    "    print(f\"Result: {weather_result}\")\n",
    "    \n",
    "    # Create function response\n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name=function_call.name,\n",
    "        response={\"result\": weather_result}\n",
    "    )\n",
    "    \n",
    "    messages.append(\n",
    "        types.Content(role=\"user\", parts=[function_response_part])\n",
    "    )\n",
    "    \n",
    "    # Get final response with function results\n",
    "    final_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=messages,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    if final_response.candidates:\n",
    "        print(f\"Gemini's response: {final_response.candidates[0].content.parts[0].text}\")\n",
    "else:\n",
    "    print(\"No function calls found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multiple Functions Response ===\n",
      "Model: gemini-2.0-flash\n",
      "\n",
      "Content parts: 2\n",
      "\n",
      "Part 1: Part\n",
      "  Function: calculator\n",
      "  Arguments: {'a': 35, 'b': 12, 'operation': 'multiply'}\n",
      "\n",
      "Part 2: Part\n",
      "  Function: get_current_time\n",
      "  Arguments: {'timezone': 'Asia/Tokyo'}\n",
      "\n",
      "Total function calls: 2\n",
      "Usage: 113 + 16 = 129 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define multiple functions\n",
    "calculator_function = {\n",
    "    \"name\": \"calculator\",\n",
    "    \"description\": \"Perform basic mathematical operations\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"description\": \"The mathematical operation\"\n",
    "            },\n",
    "            \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "            \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "        },\n",
    "        \"required\": [\"operation\", \"a\", \"b\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "time_function = {\n",
    "    \"name\": \"get_current_time\",\n",
    "    \"description\": \"Get the current time in a specified timezone\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"timezone\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Timezone (e.g., UTC, America/New_York, Asia/Tokyo)\",\n",
    "                \"default\": \"UTC\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create tools with multiple functions\n",
    "multi_tools = types.Tool(function_declarations=[\n",
    "    weather_function,\n",
    "    calculator_function,\n",
    "    time_function\n",
    "])\n",
    "multi_config = types.GenerateContentConfig(tools=[multi_tools])\n",
    "\n",
    "# Request that might use multiple functions\n",
    "multi_function_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"Calculate 35 * 12, and also tell me what time it is in Tokyo\")]\n",
    "        )\n",
    "    ],\n",
    "    config=multi_config\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Functions Response ===\")\n",
    "print(f\"Model: {multi_function_response.model_version}\")\n",
    "\n",
    "if multi_function_response.candidates:\n",
    "    candidate = multi_function_response.candidates[0]\n",
    "    \n",
    "    print(f\"\\nContent parts: {len(candidate.content.parts)}\")\n",
    "    function_calls = []\n",
    "    \n",
    "    for i, part in enumerate(candidate.content.parts):\n",
    "        print(f\"\\nPart {i+1}: {part.__class__.__name__}\")\n",
    "        \n",
    "        if part.text:\n",
    "            print(f\"  Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"  Function: {part.function_call.name}\")\n",
    "            print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "            function_calls.append(part.function_call)\n",
    "    \n",
    "    print(f\"\\nTotal function calls: {len(function_calls)}\")\n",
    "    print(f\"Usage: {multi_function_response.usage_metadata.prompt_token_count} + {multi_function_response.usage_metadata.candidates_token_count} = {multi_function_response.usage_metadata.total_token_count} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and PDF-files\n",
    "\n",
    "### Images\n",
    "First let's try sending an image from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image features a close-up of a black ant on a rough, beige surface. The ant is standing with its front legs raised, as if in a defensive or inquisitive posture. The background is blurred, with hints of brown and red, focusing attention on the ant itself. The lighting and macro focus highlight the details of the ant's body, including its segmented abdomen and spiny legs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "image1_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "\n",
    "# Download image from URL instead of trying to open it as a local file\n",
    "response = requests.get(image1_url)\n",
    "image_bytes = response.content\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=[\n",
    "        types.Part.from_bytes(\n",
    "            data=image_bytes,\n",
    "            mime_type='image/jpeg',\n",
    "        ),\n",
    "        \"What's this image about? \"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just pass a URL if it's a public image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows the console of a pipe organ. It is made of wood and has multiple keyboards, stop controls, and a pedalboard.\n"
     ]
    }
   ],
   "source": [
    "image_path = \"https://goo.gle/instrument-img\"\n",
    "image_bytes = requests.get(image_path).content\n",
    "image = types.Part.from_bytes(\n",
    "  data=image_bytes, mime_type=\"image/jpeg\"\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\"What is this image?\", image],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is an instruction booklet for the Nintendo Game Boy Color video game system. It includes:\n",
      "\n",
      "*   **Introduction:** Features and capabilities of the Game Boy Color.\n",
      "*   **Components:** Lists and describes the various parts of the system.\n",
      "*   **Installation:** Instructions on how to install batteries.\n",
      "*   **Cautions:** Information on which accessories are not compatible.\n",
      "*   **Game Paks:** Describes the different types of Game Paks that can be used with the Game Boy Color.\n",
      "*   **Usage:** Instructions on how to insert and remove Game Paks, and how to turn the system on/off.\n",
      "*   **Screen Color:** Instructions on how to change the screen color for original Game Boy Game Paks.\n",
      "*   **Multiplayer:** Instructions on how to play two-player games.\n",
      "*   **Troubleshooting:** Solutions to common problems.\n",
      "*   **Warranty & Service:** Information on the warranty and how to get service.\n",
      "*   **Parts List:** A list of parts that can be ordered.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Summarize this document\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=open(\"./assets/gameboy_color.pdf\", \"rb\").read(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the instruction booklet for the original Game Boy Color video game system by Nintendo that discusses the components, batteries, game paks, multi-player options, troubleshooting, warranty, and more. It also includes a warning about not using the Game Boy Rechargeable Battery Pack.\n"
     ]
    }
   ],
   "source": [
    "myfile = client.files.upload(file=\"./assets/gameboy_color.pdf\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=[\"Describe this PDF in 2 sentences\", myfile]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Documents (.docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a summary of the provided document:\n",
      "\n",
      "**Document Type:** Purchase Order\n",
      "\n",
      "**Issuing Company:** &lt;Company Name&gt; (&lt;Address&gt;, &lt;Contact Number&gt;, &lt;Default Email Address&gt;, &lt;Website URL&gt;)\n",
      "\n",
      "**Purchase Order Number:** 23781\n",
      "**Date:** nn/dd/yyyy\n",
      "\n",
      "**Vendor:**\n",
      "*   **Name:** &lt;Sales Person&gt;\n",
      "*   **Company Name:** &lt;Company Name&gt;\n",
      "*   **Address:** &lt;Address&gt;\n",
      "*   **Phone:** &lt;Phone&gt;\n",
      "*   **Email:** &lt;Email Address&gt;\n",
      "\n",
      "**Customer:**\n",
      "*   **Name:** John Smith\n",
      "*   **Company Name:** Redline Auto Center\n",
      "*   **Address:** 16040 S. US 27, Lansing, Michigan 48906\n",
      "*   **Phone:** 517-367-7010\n",
      "*   **Email:** johnsmith@redline.com\n",
      "\n",
      "**Shipping Terms:** Freight on Board\n",
      "**Shipping Method:** Air & Land\n",
      "\n",
      "**Items Ordered:**\n",
      "\n",
      "*   **304-98632:** Brake Discs, Pads & Calipers (Quantity: 4, Unit Price: 111.36, Amount: 445.44)\n",
      "*   **501-35587:** Control Arm (Quantity: 2, Unit Price: 60.93, Amount: 121.86)\n",
      "*   **886-19386:** Suspension Lift Kit (Quantity: 2, Unit Price: 399.83, Amount: 799.66)\n",
      "\n",
      "**Financial Summary:**\n",
      "\n",
      "*   **Subtotal:** $1,366.96\n",
      "*   **Discount (10%):** $136.70\n",
      "*   **Sales Tax (12%):** $164.04\n",
      "*   **Other Cost:** $500.00\n",
      "*   **Shipping & Handling:** $800.00\n",
      "*   **Total Amount:** $2,694.30\n",
      "\n",
      "**Payment Terms:** Payment due 30 days upon receipt of the items.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mammoth\n",
    "from google.genai import types\n",
    "\n",
    "# Convert with default options\n",
    "with open(\"./assets/order.docx\", \"rb\") as docx_file:\n",
    "    result = mammoth.convert_to_html(docx_file)\n",
    "    html = result.value  # The HTML\n",
    "    messages = result.messages  # Any warnings\n",
    "\n",
    "prompt = \"Summarize this document\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_text(\n",
    "        text=html,\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Hosting\n",
    "\n",
    "### Setup and Authentication\n",
    "Vertex AI requires Google Cloud project configuration and uses service account authentication or Application Default Credentials (ADC).\n",
    "\n",
    "### Key Differences:\n",
    "- **Project ID**: Must specify your Google Cloud project\n",
    "- **Location**: Choose deployment region for compliance/latency\n",
    "- **Enterprise Features**: VPC integration, audit logging, and advanced monitoring\n",
    "- **IAM Integration**: Fine-grained access control\n",
    "- **Custom Models**: Support for fine-tuned and custom models\n",
    "\n",
    "### Available Regions:\n",
    "- `us-central1` - United States (primary)\n",
    "- `us-east4` - United States (secondary)\n",
    "- `europe-west1` - Europe (primary)\n",
    "- `asia-northeast1` - Asia Pacific (Tokyo)\n",
    "- See [Vertex AI Locations](https://cloud.google.com/vertex-ai/docs/general/locations) for complete list\n",
    "\n",
    "### Resources:\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Gemini on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini)\n",
    "- [Google Cloud Console](https://console.cloud.google.com/vertex-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vertex AI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertex AI Configuration:\n",
      "  Project: vectrix-401014\n",
      "  Location: europe-west1\n",
      "\n",
      "=== Vertex AI Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: While both Vertex AI and AI Studio are Google Cloud Platform (GCP) offerings for machine learning (ML) development, Vertex AI offers significant advantages over AI Studio for enterprise applications. Here's a breakdown of the benefits:\n",
      "\n",
      "**Vertex AI: The Enterprise-Grade ML Platform**\n",
      "\n",
      "Vertex AI is a unified platform designed for the entire ML lifecycle, from data preparation to model deployment and monitoring. It provides a comprehensive set of tools and services for building, training, and deploying ML models at scale.\n",
      "\n",
      "**Key Benefits of Vertex AI over AI Studio for Enterprise Applications:**\n",
      "\n",
      "*   **Full ML Lifecycle Management:**\n",
      "    *   **Vertex AI:** Handles the entire ML lifecycle from data ingestion, preparation, and labeling to model training, evaluation, deployment, monitoring, and management.\n",
      "    *   **AI Studio:** Primarily focused on model building and experimentation, especially for prompt engineering with Generative AI models. It lacks robust features for data management, model deployment, and production-level monitoring.  You might use AI Studio to rapidly prototype and then migrate the refined concept to Vertex AI for production.\n",
      "\n",
      "*   **Scalability and Reliability:**\n",
      "    *   **Vertex AI:** Built on Google Cloud's robust infrastructure, it's designed for scalability and reliability.  Easily handles large datasets, complex models, and high-volume predictions.  Supports autoscaling and other features for production environments.\n",
      "    *   **AI Studio:** While suitable for experimentation, it's not designed for the same level of scalability and reliability required for enterprise applications.\n",
      "\n",
      "*   **Collaboration and Version Control:**\n",
      "    *   **Vertex AI:** Offers robust collaboration features, including shared workspaces, version control for models and pipelines, and access control. This is crucial for enterprise teams working on complex ML projects.\n",
      "    *   **AI Studio:** Collaboration features are more limited compared to Vertex AI.  Version control is less integrated into the workflow.\n",
      "\n",
      "*   **Integration with Google Cloud Ecosystem:**\n",
      "    *   **Vertex AI:** Seamlessly integrates with other GCP services like BigQuery, Dataflow, Cloud Storage, and AI Platform Pipelines. This allows for building end-to-end ML solutions that leverage the full power of the Google Cloud ecosystem.\n",
      "    *   **AI Studio:** While it can access some GCP services, the integration is not as deep or comprehensive as with Vertex AI.\n",
      "\n",
      "*   **Security and Compliance:**\n",
      "    *   **Vertex AI:** Benefits from Google Cloud's robust security infrastructure and compliance certifications (e.g., HIPAA, GDPR). Provides features like encryption, access controls, and audit logging to meet enterprise security requirements.\n",
      "    *   **AI Studio:** Security and compliance features are not as explicitly designed for regulated industries as they are in Vertex AI.\n",
      "\n",
      "*   **Model Management and Governance:**\n",
      "    *   **Vertex AI:** Offers features for model registration, versioning, lineage tracking, and governance. This allows enterprises to maintain control over their ML assets and ensure model quality and compliance.  It has features like Model Registry and Model Monitoring.\n",
      "    *   **AI Studio:** Lacks comprehensive model management and governance features.\n",
      "\n",
      "*   **Production-Grade Deployment Options:**\n",
      "    *   **Vertex AI:** Supports various deployment options, including online prediction, batch prediction, and custom serving containers. It offers features like traffic splitting, A/B testing, and shadow deployment for safe and controlled rollouts.\n",
      "    *   **AI Studio:** More focused on quick prototyping and exploration, it has limited deployment options compared to Vertex AI.\n",
      "\n",
      "*   **Monitoring and Explainability:**\n",
      "    *   **Vertex AI:** Provides tools for monitoring model performance, detecting anomalies, and explaining model predictions. This is essential for ensuring model accuracy, fairness, and transparency. Tools like Explainable AI (XAI) are integrated.\n",
      "    *   **AI Studio:** Limited monitoring and explainability capabilities.\n",
      "\n",
      "*   **Customization and Extensibility:**\n",
      "    *   **Vertex AI:** Highly customizable and extensible. Supports custom training jobs, custom containers, and the integration of third-party tools and libraries.\n",
      "    *   **AI Studio:** More restrictive in terms of customization and extensibility.\n",
      "\n",
      "*   **Vertex AI Workbench:**\n",
      "    *   **Vertex AI:** Includes Vertex AI Workbench, a fully managed, secure, and enterprise-ready compute environment for data scientists and ML engineers. It provides pre-installed ML libraries and tools and integrates seamlessly with other Vertex AI services. It supports both user-managed and managed notebooks.\n",
      "    *   **AI Studio:** Doesn't have a comparable dedicated Workbench environment.\n",
      "\n",
      "*   **Pre-trained Models and AutoML:**\n",
      "    *   **Vertex AI:** Offers access to a wide range of pre-trained models and AutoML capabilities. AutoML allows users to build custom ML models without writing code.\n",
      "    *   **AI Studio:** While it might leverage some pre-trained models, it's more geared towards prompt engineering and exploring specific Generative AI tasks rather than a broad AutoML platform.\n",
      "\n",
      "**In Summary:**\n",
      "\n",
      "| Feature          | Vertex AI                                          | AI Studio                                                    |\n",
      "|-------------------|----------------------------------------------------|--------------------------------------------------------------|\n",
      "| **Focus**         | End-to-end ML lifecycle (training to deployment)     | Model building and experimentation, especially Generative AI   |\n",
      "| **Scalability**     | High, designed for production                    | Limited, suitable for prototyping                        |\n",
      "| **Collaboration** | Robust, supports team workflows                    | Limited                                                      |\n",
      "| **Integration**    | Deeply integrated with GCP services               | Less integrated                                              |\n",
      "| **Security**      | Enterprise-grade                                   | Basic                                                        |\n",
      "| **Deployment**    | Production-ready deployment options               | Limited deployment options                                  |\n",
      "| **Monitoring**    | Comprehensive monitoring and explainability tools | Limited monitoring                                          |\n",
      "\n",
      "**When to Choose Which:**\n",
      "\n",
      "*   **Choose Vertex AI:** If you're building, deploying, and managing ML models in a production environment and require scalability, reliability, security, collaboration, and comprehensive features for the entire ML lifecycle. It's suitable for building and managing complex enterprise applications.\n",
      "*   **Choose AI Studio:** If you're experimenting with Generative AI models, exploring prompt engineering, and need a quick and easy way to prototype ML solutions. It's a good starting point for exploring Generative AI concepts. However, you should migrate the work to Vertex AI for production workloads.  AI Studio is useful for quick iterations on prompts and evaluating model outputs, while Vertex AI is where those prompts and models become part of a larger, production-ready system.\n",
      "\n",
      "In essence, Vertex AI is a mature and comprehensive ML platform for enterprise applications, while AI Studio is a tool for experimentation and rapid prototyping, especially in the context of Generative AI. Vertex AI is designed to handle the complexities and demands of production ML workloads, offering the scalability, reliability, and security that enterprises require.\n",
      "\n",
      "Finish reason: FinishReason.STOP\n",
      "Usage: 14 + 1441 = 1455 tokens\n",
      "Create time: 2025-06-11 13:53:34.936745+00:00\n",
      "Response ID: XopJaKmWOeT2xN8Po6bk8QM\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GenerateContentResponse(candidates=[Candidate(content=Content(parts=[Part(video_metadata=None, thought=None, inline_data=None, code_execution_result=None, executable_code=None, file_data=None, function_call=None, function_response=None, text=\"While both Vertex AI and AI Studio are Google Cloud Platform (GCP) offerings for machine learning (ML) development, Vertex AI offers significant advantages over AI Studio for enterprise applications. Here's a breakdown of the benefits:\\n\\n**Vertex AI: The Enterprise-Grade ML Platform**\\n\\nVertex AI is a unified platform designed for the entire ML lifecycle, from data preparation to model deployment and monitoring. It provides a comprehensive set of tools and services for building, training, and deploying ML models at scale.\\n\\n**Key Benefits of Vertex AI over AI Studio for Enterprise Applications:**\\n\\n*   **Full ML Lifecycle Management:**\\n    *   **Vertex AI:** Handles the entire ML lifecycle from data ingestion, preparation, and labeling to model training, evaluation, deployment, monitoring, and management.\\n    *   **AI Studio:** Primarily focused on model building and experimentation, especially for prompt engineering with Generative AI models. It lacks robust features for data management, model deployment, and production-level monitoring.  You might use AI Studio to rapidly prototype and then migrate the refined concept to Vertex AI for production.\\n\\n*   **Scalability and Reliability:**\\n    *   **Vertex AI:** Built on Google Cloud's robust infrastructure, it's designed for scalability and reliability.  Easily handles large datasets, complex models, and high-volume predictions.  Supports autoscaling and other features for production environments.\\n    *   **AI Studio:** While suitable for experimentation, it's not designed for the same level of scalability and reliability required for enterprise applications.\\n\\n*   **Collaboration and Version Control:**\\n    *   **Vertex AI:** Offers robust collaboration features, including shared workspaces, version control for models and pipelines, and access control. This is crucial for enterprise teams working on complex ML projects.\\n    *   **AI Studio:** Collaboration features are more limited compared to Vertex AI.  Version control is less integrated into the workflow.\\n\\n*   **Integration with Google Cloud Ecosystem:**\\n    *   **Vertex AI:** Seamlessly integrates with other GCP services like BigQuery, Dataflow, Cloud Storage, and AI Platform Pipelines. This allows for building end-to-end ML solutions that leverage the full power of the Google Cloud ecosystem.\\n    *   **AI Studio:** While it can access some GCP services, the integration is not as deep or comprehensive as with Vertex AI.\\n\\n*   **Security and Compliance:**\\n    *   **Vertex AI:** Benefits from Google Cloud's robust security infrastructure and compliance certifications (e.g., HIPAA, GDPR). Provides features like encryption, access controls, and audit logging to meet enterprise security requirements.\\n    *   **AI Studio:** Security and compliance features are not as explicitly designed for regulated industries as they are in Vertex AI.\\n\\n*   **Model Management and Governance:**\\n    *   **Vertex AI:** Offers features for model registration, versioning, lineage tracking, and governance. This allows enterprises to maintain control over their ML assets and ensure model quality and compliance.  It has features like Model Registry and Model Monitoring.\\n    *   **AI Studio:** Lacks comprehensive model management and governance features.\\n\\n*   **Production-Grade Deployment Options:**\\n    *   **Vertex AI:** Supports various deployment options, including online prediction, batch prediction, and custom serving containers. It offers features like traffic splitting, A/B testing, and shadow deployment for safe and controlled rollouts.\\n    *   **AI Studio:** More focused on quick prototyping and exploration, it has limited deployment options compared to Vertex AI.\\n\\n*   **Monitoring and Explainability:**\\n    *   **Vertex AI:** Provides tools for monitoring model performance, detecting anomalies, and explaining model predictions. This is essential for ensuring model accuracy, fairness, and transparency. Tools like Explainable AI (XAI) are integrated.\\n    *   **AI Studio:** Limited monitoring and explainability capabilities.\\n\\n*   **Customization and Extensibility:**\\n    *   **Vertex AI:** Highly customizable and extensible. Supports custom training jobs, custom containers, and the integration of third-party tools and libraries.\\n    *   **AI Studio:** More restrictive in terms of customization and extensibility.\\n\\n*   **Vertex AI Workbench:**\\n    *   **Vertex AI:** Includes Vertex AI Workbench, a fully managed, secure, and enterprise-ready compute environment for data scientists and ML engineers. It provides pre-installed ML libraries and tools and integrates seamlessly with other Vertex AI services. It supports both user-managed and managed notebooks.\\n    *   **AI Studio:** Doesn't have a comparable dedicated Workbench environment.\\n\\n*   **Pre-trained Models and AutoML:**\\n    *   **Vertex AI:** Offers access to a wide range of pre-trained models and AutoML capabilities. AutoML allows users to build custom ML models without writing code.\\n    *   **AI Studio:** While it might leverage some pre-trained models, it's more geared towards prompt engineering and exploring specific Generative AI tasks rather than a broad AutoML platform.\\n\\n**In Summary:**\\n\\n| Feature          | Vertex AI                                          | AI Studio                                                    |\\n|-------------------|----------------------------------------------------|--------------------------------------------------------------|\\n| **Focus**         | End-to-end ML lifecycle (training to deployment)     | Model building and experimentation, especially Generative AI   |\\n| **Scalability**     | High, designed for production                    | Limited, suitable for prototyping                        |\\n| **Collaboration** | Robust, supports team workflows                    | Limited                                                      |\\n| **Integration**    | Deeply integrated with GCP services               | Less integrated                                              |\\n| **Security**      | Enterprise-grade                                   | Basic                                                        |\\n| **Deployment**    | Production-ready deployment options               | Limited deployment options                                  |\\n| **Monitoring**    | Comprehensive monitoring and explainability tools | Limited monitoring                                          |\\n\\n**When to Choose Which:**\\n\\n*   **Choose Vertex AI:** If you're building, deploying, and managing ML models in a production environment and require scalability, reliability, security, collaboration, and comprehensive features for the entire ML lifecycle. It's suitable for building and managing complex enterprise applications.\\n*   **Choose AI Studio:** If you're experimenting with Generative AI models, exploring prompt engineering, and need a quick and easy way to prototype ML solutions. It's a good starting point for exploring Generative AI concepts. However, you should migrate the work to Vertex AI for production workloads.  AI Studio is useful for quick iterations on prompts and evaluating model outputs, while Vertex AI is where those prompts and models become part of a larger, production-ready system.\\n\\nIn essence, Vertex AI is a mature and comprehensive ML platform for enterprise applications, while AI Studio is a tool for experimentation and rapid prototyping, especially in the context of Generative AI. Vertex AI is designed to handle the complexities and demands of production ML workloads, offering the scalability, reliability, and security that enterprises require.\\n\")], role='model'), citation_metadata=None, finish_message=None, token_count=None, finish_reason=<FinishReason.STOP: 'STOP'>, url_context_metadata=None, avg_logprobs=-0.485853312993364, grounding_metadata=None, index=None, logprobs_result=None, safety_ratings=None)], create_time=datetime.datetime(2025, 6, 11, 13, 53, 34, 936745, tzinfo=TzInfo(UTC)), response_id='XopJaKmWOeT2xN8Po6bk8QM', model_version='gemini-2.0-flash-001', prompt_feedback=None, usage_metadata=GenerateContentResponseUsageMetadata(cache_tokens_details=None, cached_content_token_count=None, candidates_token_count=1441, candidates_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=1441)], prompt_token_count=14, prompt_tokens_details=[ModalityTokenCount(modality=<MediaModality.TEXT: 'TEXT'>, token_count=14)], thoughts_token_count=None, tool_use_prompt_token_count=None, tool_use_prompt_tokens_details=None, total_token_count=1455, traffic_type=<TrafficType.ON_DEMAND: 'ON_DEMAND'>), automatic_function_calling_history=[], parsed=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize Vertex AI client\n",
    "project_id = os.environ.get(\"GCP_PROJECT_ID\")\n",
    "location = os.environ.get(\"GCP_LOCATION\", \"us-central1\")\n",
    "\n",
    "print(f\"Vertex AI Configuration:\")\n",
    "print(f\"  Project: {project_id}\")\n",
    "print(f\"  Location: {location}\")\n",
    "\n",
    "vertex_client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=project_id,\n",
    "    location=location\n",
    ")\n",
    "\n",
    "# Basic content generation on Vertex AI\n",
    "vertex_response = vertex_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='Explain the benefits of using Vertex AI over AI Studio for enterprise applications.'\n",
    ")\n",
    "\n",
    "print(\"\\n=== Vertex AI Response ===\")\n",
    "print(f\"Model: {vertex_response.model_version}\")\n",
    "\n",
    "if vertex_response.candidates and len(vertex_response.candidates) > 0:\n",
    "    candidate = vertex_response.candidates[0]\n",
    "    print(f\"Content: {candidate.content.parts[0].text}\")\n",
    "    print(f\"Finish reason: {candidate.finish_reason}\")\n",
    "\n",
    "print(f\"Usage: {vertex_response.usage_metadata.prompt_token_count} + {vertex_response.usage_metadata.candidates_token_count} = {vertex_response.usage_metadata.total_token_count} tokens\")\n",
    "\n",
    "# Additional Vertex AI metadata\n",
    "if hasattr(vertex_response, 'create_time'):\n",
    "    print(f\"Create time: {vertex_response.create_time}\")\n",
    "if hasattr(vertex_response, 'response_id'):\n",
    "    print(f\"Response ID: {vertex_response.response_id}\")\n",
    "\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "vertex_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Function Calling\n",
    "\n",
    "Function calling works identically on Vertex AI as with AI Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vertex AI Function Call ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Function: calculator\n",
      "Operation: 88 divide 11\n",
      "Arguments: {'a': 88, 'operation': 'divide', 'b': 11}\n",
      "\n",
      "âœ… Function calling works identically on Vertex AI!\n"
     ]
    }
   ],
   "source": [
    "# Function calling works the same on Vertex AI\n",
    "vertex_tools = types.Tool(function_declarations=[calculator_function])\n",
    "vertex_config = types.GenerateContentConfig(tools=[vertex_tools])\n",
    "\n",
    "vertex_function_response = vertex_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"Calculate 88 divided by 11\")]\n",
    "        )\n",
    "    ],\n",
    "    config=vertex_config\n",
    ")\n",
    "\n",
    "print(\"=== Vertex AI Function Call ===\")\n",
    "print(f\"Model: {vertex_function_response.model_version}\")\n",
    "\n",
    "if vertex_function_response.candidates:\n",
    "    candidate = vertex_function_response.candidates[0]\n",
    "    \n",
    "    for part in candidate.content.parts:\n",
    "        if part.text:\n",
    "            print(f\"Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"Function: {part.function_call.name}\")\n",
    "            args = dict(part.function_call.args)\n",
    "            print(f\"Operation: {args['a']} {args['operation']} {args['b']}\")\n",
    "            print(f\"Arguments: {args}\")\n",
    "\n",
    "print(f\"\\nâœ… Function calling works identically on Vertex AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: AI Studio vs Vertex AI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison: AI Studio vs Vertex AI ===\n",
      "\n",
      "ðŸŸ¡ AI Studio:\n",
      "Model: gemini-2.0-flash-001\n",
      "Response: Artificial intelligence is the ability of a computer or machine to mimic human cognitive functions, such as learning, problem-solving, and decision-making. It involves developing algorithms and models that enable machines to perform tasks that typically require human intelligence.\n",
      "\n",
      "Tokens: 9 + 48\n",
      "\n",
      "ðŸ”µ Vertex AI:\n",
      "Model: gemini-2.0-flash-001\n",
      "Response: Artificial intelligence is the simulation of human intelligence processes by computer systems. These processes include learning, reasoning, and problem-solving, allowing machines to perform tasks that typically require human intelligence.\n",
      "\n",
      "Tokens: 9 + 37\n",
      "\n",
      "ðŸ’¡ Both use identical APIs and response formats!\n",
      "ðŸ’¡ Key differences: authentication, enterprise features, and regional deployment\n"
     ]
    }
   ],
   "source": [
    "test_content = \"Explain artificial intelligence in exactly 2 sentences.\"\n",
    "\n",
    "print(\"=== Comparison: AI Studio vs Vertex AI ===\")\n",
    "\n",
    "# AI Studio\n",
    "studio_response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=test_content\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ¡ AI Studio:\")\n",
    "print(f\"Model: {studio_response.model_version}\")\n",
    "if studio_response.candidates:\n",
    "    print(f\"Response: {studio_response.candidates[0].content.parts[0].text}\")\n",
    "print(f\"Tokens: {studio_response.usage_metadata.prompt_token_count} + {studio_response.usage_metadata.candidates_token_count}\")\n",
    "\n",
    "# Vertex AI\n",
    "vertex_response = vertex_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=test_content\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Vertex AI:\")\n",
    "print(f\"Model: {vertex_response.model_version}\")\n",
    "if vertex_response.candidates:\n",
    "    print(f\"Response: {vertex_response.candidates[0].content.parts[0].text}\")\n",
    "print(f\"Tokens: {vertex_response.usage_metadata.prompt_token_count} + {vertex_response.usage_metadata.candidates_token_count}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Key differences: authentication, enterprise features, and regional deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
