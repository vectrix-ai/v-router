{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Provider Guide ðŸŒŸ\n",
    "\n",
    "This notebook demonstrates how to use Google's Gemini models through different hosting options:\n",
    "- **AI Studio**: Google's direct API service for developers\n",
    "- **Vertex AI**: Enterprise-grade platform through Google Cloud\n",
    "\n",
    "## Overview\n",
    "\n",
    "Google offers Gemini models through multiple platforms, each designed for different use cases:\n",
    "\n",
    "### AI Studio (Direct Google Hosting)\n",
    "- **Easy setup** with simple API key authentication\n",
    "- **Latest models** available immediately upon release\n",
    "- **Developer-friendly** with minimal configuration\n",
    "- **Free tier** available for experimentation\n",
    "- **Global access** without regional restrictions\n",
    "\n",
    "### Vertex AI (Google Cloud Hosting)\n",
    "- **Enterprise features** including VPC integration\n",
    "- **Regional deployment** for compliance and latency\n",
    "- **Advanced monitoring** and logging capabilities\n",
    "- **Custom model fine-tuning** options\n",
    "- **IAM integration** for access control\n",
    "- **Batch processing** and high-throughput scenarios\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI Studio (Direct Google Hosting)\n",
    "\n",
    "### Authentication\n",
    "AI Studio uses the `GEMINI_API_KEY` environment variable for authentication. You can get your API key from [Google AI Studio](https://aistudio.google.com/).\n",
    "\n",
    "### Available Models\n",
    "- `gemini-2.0-flash-001` - Latest and fastest Gemini model\n",
    "- `gemini-1.5-pro-001` - Most capable model for complex tasks\n",
    "- `gemini-1.5-flash-001` - Balanced performance and speed\n",
    "- `gemini-1.0-pro` - Previous generation model\n",
    "\n",
    "### Key Features\n",
    "- **Multimodal capabilities** (text, images, audio, video)\n",
    "- **Function calling** for tool integration\n",
    "- **Large context windows** (up to 2M tokens)\n",
    "- **Code generation** and execution\n",
    "- **Reasoning capabilities** with chain-of-thought\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://github.com/googleapis/python-genai)\n",
    "- [AI Studio](https://aistudio.google.com/)\n",
    "- [Model Documentation](https://ai.google.dev/models/gemini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AI Studio Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: Okay, imagine you're trying to find the best route through a maze.\n",
      "\n",
      "**Classic Computer (Like a Normal Light Switch):**\n",
      "\n",
      "*   Think of a classic computer as having a single light switch. It can be either **ON (1)** or **OFF (0)**.\n",
      "*   To find the best route in the maze, a classic computer tries each path **one at a time**. It starts at one path, checks if it leads to the end. If not, it goes back and tries another path. It's like flipping the light switch to ON (checking a path) and then back to OFF before checking the next. This takes a long time, especially for a complicated maze.\n",
      "\n",
      "**Quantum Computer (Like a Magical Compass Rose):**\n",
      "\n",
      "*   Now imagine you have a **magical compass rose**. Instead of pointing in just one direction, it can point **in multiple directions at the same time**! It's not just pointing North, or South, or East, or West. It's pointing in **all possible directions simultaneously**. This is like the quantum concept of **superposition**.\n",
      "*   A quantum computer takes advantage of this ability to explore all possible paths in the maze **at the same time**. It's like your compass rose trying all the directions at once.\n",
      "*   Then, it uses another quantum trick called **interference** to amplify the paths that lead to the end and cancel out the paths that don't. It's like the compass rose magically highlighting the correct direction and dimming all the wrong ones.\n",
      "*   Finally, when you *look* at the compass rose (measure the result), it points you towards the **most likely best path** through the maze.\n",
      "\n",
      "**In short:**\n",
      "\n",
      "*   **Classic Computer:** Tries each option one by one.\n",
      "*   **Quantum Computer:** Tries all options simultaneously and then amplifies the correct one.\n",
      "\n",
      "**Key Concepts:**\n",
      "\n",
      "*   **Superposition:** Being in multiple states at once (like the compass rose pointing in all directions).\n",
      "*   **Interference:** Amplifying the right answers and canceling out the wrong ones (like the compass rose highlighting the correct direction).\n",
      "*   **Qubit:** The basic unit of information in a quantum computer. You can think of it like the point on the compass rose: a normal computer has a bit which can be 0 or 1, whereas a qubit can be 0, or 1, or both at the same time.\n",
      "\n",
      "**Why is this useful?**\n",
      "\n",
      "For certain types of problems, a quantum computer can be exponentially faster than a classic computer. This could revolutionize fields like:\n",
      "\n",
      "*   **Drug Discovery:** Designing new molecules and simulating their interactions.\n",
      "*   **Materials Science:** Creating new materials with specific properties.\n",
      "*   **Cryptography:** Breaking existing encryption methods and developing new, quantum-resistant ones.\n",
      "*   **Optimization:** Solving complex optimization problems in logistics, finance, and AI.\n",
      "\n",
      "**Important Note:**\n",
      "\n",
      "Quantum computers are still in their early stages of development. They are not going to replace classic computers entirely. Instead, they will be used for specific types of problems where their unique abilities can provide a significant advantage. The maze analogy doesn't capture all the intricacies of quantum computing, but it provides a simplified way to understand the core concepts.\n",
      "\n",
      "Finish reason: FinishReason.STOP\n",
      "Usage: 11 + 687 = 698 tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candidates': [{'content': {'parts': [{'video_metadata': None,\n",
       "      'thought': None,\n",
       "      'inline_data': None,\n",
       "      'code_execution_result': None,\n",
       "      'executable_code': None,\n",
       "      'file_data': None,\n",
       "      'function_call': None,\n",
       "      'function_response': None,\n",
       "      'text': \"Okay, imagine you're trying to find the best route through a maze.\\n\\n**Classic Computer (Like a Normal Light Switch):**\\n\\n*   Think of a classic computer as having a single light switch. It can be either **ON (1)** or **OFF (0)**.\\n*   To find the best route in the maze, a classic computer tries each path **one at a time**. It starts at one path, checks if it leads to the end. If not, it goes back and tries another path. It's like flipping the light switch to ON (checking a path) and then back to OFF before checking the next. This takes a long time, especially for a complicated maze.\\n\\n**Quantum Computer (Like a Magical Compass Rose):**\\n\\n*   Now imagine you have a **magical compass rose**. Instead of pointing in just one direction, it can point **in multiple directions at the same time**! It's not just pointing North, or South, or East, or West. It's pointing in **all possible directions simultaneously**. This is like the quantum concept of **superposition**.\\n*   A quantum computer takes advantage of this ability to explore all possible paths in the maze **at the same time**. It's like your compass rose trying all the directions at once.\\n*   Then, it uses another quantum trick called **interference** to amplify the paths that lead to the end and cancel out the paths that don't. It's like the compass rose magically highlighting the correct direction and dimming all the wrong ones.\\n*   Finally, when you *look* at the compass rose (measure the result), it points you towards the **most likely best path** through the maze.\\n\\n**In short:**\\n\\n*   **Classic Computer:** Tries each option one by one.\\n*   **Quantum Computer:** Tries all options simultaneously and then amplifies the correct one.\\n\\n**Key Concepts:**\\n\\n*   **Superposition:** Being in multiple states at once (like the compass rose pointing in all directions).\\n*   **Interference:** Amplifying the right answers and canceling out the wrong ones (like the compass rose highlighting the correct direction).\\n*   **Qubit:** The basic unit of information in a quantum computer. You can think of it like the point on the compass rose: a normal computer has a bit which can be 0 or 1, whereas a qubit can be 0, or 1, or both at the same time.\\n\\n**Why is this useful?**\\n\\nFor certain types of problems, a quantum computer can be exponentially faster than a classic computer. This could revolutionize fields like:\\n\\n*   **Drug Discovery:** Designing new molecules and simulating their interactions.\\n*   **Materials Science:** Creating new materials with specific properties.\\n*   **Cryptography:** Breaking existing encryption methods and developing new, quantum-resistant ones.\\n*   **Optimization:** Solving complex optimization problems in logistics, finance, and AI.\\n\\n**Important Note:**\\n\\nQuantum computers are still in their early stages of development. They are not going to replace classic computers entirely. Instead, they will be used for specific types of problems where their unique abilities can provide a significant advantage. The maze analogy doesn't capture all the intricacies of quantum computing, but it provides a simplified way to understand the core concepts.\\n\"}],\n",
       "    'role': 'model'},\n",
       "   'citation_metadata': {'citations': [{'end_index': 2720,\n",
       "      'license': None,\n",
       "      'publication_date': None,\n",
       "      'start_index': 2577,\n",
       "      'title': None,\n",
       "      'uri': 'https://www.loveonn.com/web-oracle/what-is-quantum-computing%3F'}]},\n",
       "   'finish_message': None,\n",
       "   'token_count': None,\n",
       "   'finish_reason': <FinishReason.STOP: 'STOP'>,\n",
       "   'url_context_metadata': None,\n",
       "   'avg_logprobs': -0.4437113710539483,\n",
       "   'grounding_metadata': None,\n",
       "   'index': None,\n",
       "   'logprobs_result': None,\n",
       "   'safety_ratings': None}],\n",
       " 'create_time': None,\n",
       " 'response_id': None,\n",
       " 'model_version': 'gemini-2.0-flash-001',\n",
       " 'prompt_feedback': None,\n",
       " 'usage_metadata': {'cache_tokens_details': None,\n",
       "  'cached_content_token_count': None,\n",
       "  'candidates_token_count': 687,\n",
       "  'candidates_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 687}],\n",
       "  'prompt_token_count': 11,\n",
       "  'prompt_tokens_details': [{'modality': <MediaModality.TEXT: 'TEXT'>,\n",
       "    'token_count': 11}],\n",
       "  'thoughts_token_count': None,\n",
       "  'tool_use_prompt_token_count': None,\n",
       "  'tool_use_prompt_tokens_details': None,\n",
       "  'total_token_count': 698,\n",
       "  'traffic_type': None},\n",
       " 'automatic_function_calling_history': [],\n",
       " 'parsed': None}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google import genai\n",
    "\n",
    "# Initialize AI Studio client\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "# Basic content generation\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='Explain quantum computing in simple terms, using an analogy.'\n",
    ")\n",
    "\n",
    "print(\"=== AI Studio Response ===\")\n",
    "print(f\"Model: {response.model_version}\")\n",
    "\n",
    "if response.candidates and len(response.candidates) > 0:\n",
    "    candidate = response.candidates[0]\n",
    "    print(f\"Content: {candidate.content.parts[0].text}\")\n",
    "    print(f\"Finish reason: {candidate.finish_reason}\")\n",
    "    \n",
    "    # Safety ratings (if available)\n",
    "    if candidate.safety_ratings:\n",
    "        print(f\"Safety ratings: {len(candidate.safety_ratings)} categories checked\")\n",
    "\n",
    "print(f\"Usage: {response.usage_metadata.prompt_token_count} + {response.usage_metadata.candidates_token_count} = {response.usage_metadata.total_token_count} tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Configuration with Messages\n",
    "\n",
    "For more complex interactions, you can use the message format with system instructions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Advanced Configuration Response ===\n",
      "Model: gemini-2.0-flash-001\n",
      "Content: Rain, the color of bruised plums, slicked the neon canyons of Neo-Kyoto, each drop a tiny explosion of light on the polished chrome streets. Detective Kaito Ishikawa, his trench coat shimmering like an oil slick, watched a hovercar drift silently past, its windows black as a shark's eyes. Inside, someone was dead. Or, more accurately, someone had been rendered *more* dead than the average citizen of this city, where life was already a precarious negotiation with cybernetics and synthetic enhancements. This was a death that screamed of something beyond the usual corporate espionage or gang warfare, a death that tasted of forbidden tech and whispered of secrets buried deep in the city's glittering, digitized heart.\n",
      "\n",
      "Average log probability: -0.4678\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "# Create a conversation with system instructions\n",
    "system_instruction = \"You are a helpful AI assistant specializing in creative writing. Always provide vivid, engaging responses.\"\n",
    "\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"Write a compelling opening paragraph for a mystery novel set in a futuristic city.\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Configuration for more control\n",
    "config = types.GenerateContentConfig(\n",
    "    system_instruction=system_instruction,\n",
    "    temperature=0.8,  # More creative responses\n",
    "    top_p=0.95,\n",
    "    max_output_tokens=200,\n",
    "    candidate_count=1\n",
    ")\n",
    "\n",
    "advanced_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=messages,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Advanced Configuration Response ===\")\n",
    "print(f\"Model: {advanced_response.model_version}\")\n",
    "if advanced_response.candidates:\n",
    "    print(f\"Content: {advanced_response.candidates[0].content.parts[0].text}\")\n",
    "    print(f\"Average log probability: {advanced_response.candidates[0].avg_logprobs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "Google's Gemini models support function calling for interacting with external tools and APIs.\n",
    "\n",
    "### Key Features:\n",
    "- **Function declarations** using JSON schemas\n",
    "- **Parallel function calls** for efficiency\n",
    "- **Automatic function calling** mode\n",
    "- **Complex parameter schemas** with nested objects\n",
    "\n",
    "### Force function calling\n",
    "The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the function_calling_config.\n",
    "\n",
    "- AUTO (Default): The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. This is the most flexible mode and recommended for most scenarios.\n",
    "- ANY: The model is constrained to always predict a function call and guarantee function schema adherence. If allowed_function_names is not specified, the model can choose from any of the provided function declarations. If allowed_function_names is provided as a list, the model can only choose from the functions in that list. Use this mode when you require a function call in response to every prompt (if applicable).\n",
    "- NONE: The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. Use this to temporarily disable function calling without removing your tool definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Function Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Call Response ===\n",
      "Model: gemini-2.0-flash\n",
      "\n",
      "Part 1: Part\n",
      "  Function: get_weather\n",
      "  Arguments: {'location': 'Tokyo, Japan'}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather function\n",
    "weather_function = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get current temperature for a given location.\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"City and country e.g. Tokyo, Japan\"\n",
    "            },\n",
    "            \"units\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"Temperature units\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create function tool\n",
    "tools = types.Tool(function_declarations=[weather_function])\n",
    "\n",
    "# Create the config\n",
    "tool_config = types.ToolConfig(\n",
    "    function_calling_config=types.FunctionCallingConfig(\n",
    "        mode=\"ANY\", allowed_function_names=[\"get_weather\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "config = types.GenerateContentConfig(tools=[tools], tool_config=types.ToolConfig)\n",
    "\n",
    "# Initial request with function\n",
    "messages = [\n",
    "    types.Content(\n",
    "        role=\"user\",\n",
    "        parts=[types.Part(text=\"What is the weather like in Tokyo today?\")]\n",
    "    )\n",
    "]\n",
    "\n",
    "function_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=messages,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"=== Function Call Response ===\")\n",
    "print(f\"Model: {function_response.model_version}\")\n",
    "\n",
    "if function_response.candidates and len(function_response.candidates) > 0:\n",
    "    candidate = function_response.candidates[0]\n",
    "    \n",
    "    for i, part in enumerate(candidate.content.parts):\n",
    "        print(f\"\\nPart {i+1}: {part.__class__.__name__}\")\n",
    "        \n",
    "        if part.text:\n",
    "            print(f\"  Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"  Function: {part.function_call.name}\")\n",
    "            print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append(candidate.content)\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate function execution\n",
    "def execute_weather_function(location, units=\"celsius\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"tokyo\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"22Â°C, partly cloudy with light breeze\"\n",
    "        else:\n",
    "            return \"72Â°F, partly cloudy with light breeze\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Find function call in the response\n",
    "function_call = None\n",
    "if function_response.candidates:\n",
    "    for part in function_response.candidates[0].content.parts:\n",
    "        if part.function_call:\n",
    "            function_call = part.function_call\n",
    "            break\n",
    "\n",
    "if function_call:\n",
    "    # Execute the function\n",
    "    location = function_call.args.get(\"location\")\n",
    "    units = function_call.args.get(\"units\", \"celsius\")\n",
    "    weather_result = execute_weather_function(location, units)\n",
    "    \n",
    "    print(f\"=== Function Execution ===\")\n",
    "    print(f\"Function: {function_call.name}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Units: {units}\")\n",
    "    print(f\"Result: {weather_result}\")\n",
    "    \n",
    "    # Create function response\n",
    "    function_response_part = types.Part.from_function_response(\n",
    "        name=function_call.name,\n",
    "        response={\"result\": weather_result}\n",
    "    )\n",
    "    \n",
    "    messages.append(\n",
    "        types.Content(role=\"user\", parts=[function_response_part])\n",
    "    )\n",
    "    \n",
    "    # Get final response with function results\n",
    "    final_response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash\",\n",
    "        contents=messages,\n",
    "        config=config\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    if final_response.candidates:\n",
    "        print(f\"Gemini's response: {final_response.candidates[0].content.parts[0].text}\")\n",
    "else:\n",
    "    print(\"No function calls found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple functions\n",
    "calculator_function = {\n",
    "    \"name\": \"calculator\",\n",
    "    \"description\": \"Perform basic mathematical operations\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"description\": \"The mathematical operation\"\n",
    "            },\n",
    "            \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "            \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "        },\n",
    "        \"required\": [\"operation\", \"a\", \"b\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "time_function = {\n",
    "    \"name\": \"get_current_time\",\n",
    "    \"description\": \"Get the current time in a specified timezone\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"timezone\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Timezone (e.g., UTC, America/New_York, Asia/Tokyo)\",\n",
    "                \"default\": \"UTC\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create tools with multiple functions\n",
    "multi_tools = types.Tool(function_declarations=[\n",
    "    weather_function,\n",
    "    calculator_function,\n",
    "    time_function\n",
    "])\n",
    "multi_config = types.GenerateContentConfig(tools=[multi_tools])\n",
    "\n",
    "# Request that might use multiple functions\n",
    "multi_function_response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"Calculate 35 * 12, and also tell me what time it is in Tokyo\")]\n",
    "        )\n",
    "    ],\n",
    "    config=multi_config\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Functions Response ===\")\n",
    "print(f\"Model: {multi_function_response.model_version}\")\n",
    "\n",
    "if multi_function_response.candidates:\n",
    "    candidate = multi_function_response.candidates[0]\n",
    "    \n",
    "    print(f\"\\nContent parts: {len(candidate.content.parts)}\")\n",
    "    function_calls = []\n",
    "    \n",
    "    for i, part in enumerate(candidate.content.parts):\n",
    "        print(f\"\\nPart {i+1}: {part.__class__.__name__}\")\n",
    "        \n",
    "        if part.text:\n",
    "            print(f\"  Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"  Function: {part.function_call.name}\")\n",
    "            print(f\"  Arguments: {dict(part.function_call.args)}\")\n",
    "            function_calls.append(part.function_call)\n",
    "    \n",
    "    print(f\"\\nTotal function calls: {len(function_calls)}\")\n",
    "    print(f\"Usage: {multi_function_response.usage_metadata.prompt_token_count} + {multi_function_response.usage_metadata.candidates_token_count} = {multi_function_response.usage_metadata.total_token_count} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and PDF-files\n",
    "\n",
    "### Images\n",
    "First let's try sending an image from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "image1_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "\n",
    "# Download image from URL instead of trying to open it as a local file\n",
    "response = requests.get(image1_url)\n",
    "image_bytes = response.content\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash',\n",
    "    contents=[\n",
    "        types.Part.from_bytes(\n",
    "            data=image_bytes,\n",
    "            mime_type='image/jpeg',\n",
    "        ),\n",
    "        \"What's this image about? \"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just pass a URL if it's a public image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"https://goo.gle/instrument-img\"\n",
    "image_bytes = requests.get(image_path).content\n",
    "image = types.Part.from_bytes(\n",
    "  data=image_bytes, mime_type=\"image/jpeg\"\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-exp\",\n",
    "    contents=[\"What is this image?\", image],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF-Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Summarize this document\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.0-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=open(\"./assets/gameboy_color.pdf\", \"rb\").read(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile = client.files.upload(file=\"./assets/gameboy_color.pdf\")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=[\"Describe this PDF in 2 sentences\", myfile]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Hosting\n",
    "\n",
    "### Setup and Authentication\n",
    "Vertex AI requires Google Cloud project configuration and uses service account authentication or Application Default Credentials (ADC).\n",
    "\n",
    "### Key Differences:\n",
    "- **Project ID**: Must specify your Google Cloud project\n",
    "- **Location**: Choose deployment region for compliance/latency\n",
    "- **Enterprise Features**: VPC integration, audit logging, and advanced monitoring\n",
    "- **IAM Integration**: Fine-grained access control\n",
    "- **Custom Models**: Support for fine-tuned and custom models\n",
    "\n",
    "### Available Regions:\n",
    "- `us-central1` - United States (primary)\n",
    "- `us-east4` - United States (secondary)\n",
    "- `europe-west1` - Europe (primary)\n",
    "- `asia-northeast1` - Asia Pacific (Tokyo)\n",
    "- See [Vertex AI Locations](https://cloud.google.com/vertex-ai/docs/general/locations) for complete list\n",
    "\n",
    "### Resources:\n",
    "- [Vertex AI Documentation](https://cloud.google.com/vertex-ai/docs)\n",
    "- [Gemini on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/model-reference/gemini)\n",
    "- [Google Cloud Console](https://console.cloud.google.com/vertex-ai)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vertex AI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI client\n",
    "project_id = os.environ.get(\"GCP_PROJECT_ID\")\n",
    "location = os.environ.get(\"GCP_LOCATION\", \"us-central1\")\n",
    "\n",
    "print(f\"Vertex AI Configuration:\")\n",
    "print(f\"  Project: {project_id}\")\n",
    "print(f\"  Location: {location}\")\n",
    "\n",
    "vertex_client = genai.Client(\n",
    "    vertexai=True,\n",
    "    project=project_id,\n",
    "    location=location\n",
    ")\n",
    "\n",
    "# Basic content generation on Vertex AI\n",
    "vertex_response = vertex_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents='Explain the benefits of using Vertex AI over AI Studio for enterprise applications.'\n",
    ")\n",
    "\n",
    "print(\"\\n=== Vertex AI Response ===\")\n",
    "print(f\"Model: {vertex_response.model_version}\")\n",
    "\n",
    "if vertex_response.candidates and len(vertex_response.candidates) > 0:\n",
    "    candidate = vertex_response.candidates[0]\n",
    "    print(f\"Content: {candidate.content.parts[0].text}\")\n",
    "    print(f\"Finish reason: {candidate.finish_reason}\")\n",
    "\n",
    "print(f\"Usage: {vertex_response.usage_metadata.prompt_token_count} + {vertex_response.usage_metadata.candidates_token_count} = {vertex_response.usage_metadata.total_token_count} tokens\")\n",
    "\n",
    "# Additional Vertex AI metadata\n",
    "if hasattr(vertex_response, 'create_time'):\n",
    "    print(f\"Create time: {vertex_response.create_time}\")\n",
    "if hasattr(vertex_response, 'response_id'):\n",
    "    print(f\"Response ID: {vertex_response.response_id}\")\n",
    "\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "vertex_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Function Calling\n",
    "\n",
    "Function calling works identically on Vertex AI as with AI Studio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling works the same on Vertex AI\n",
    "vertex_tools = types.Tool(function_declarations=[calculator_function])\n",
    "vertex_config = types.GenerateContentConfig(tools=[vertex_tools])\n",
    "\n",
    "vertex_function_response = vertex_client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    contents=[\n",
    "        types.Content(\n",
    "            role=\"user\",\n",
    "            parts=[types.Part(text=\"Calculate 88 divided by 11\")]\n",
    "        )\n",
    "    ],\n",
    "    config=vertex_config\n",
    ")\n",
    "\n",
    "print(\"=== Vertex AI Function Call ===\")\n",
    "print(f\"Model: {vertex_function_response.model_version}\")\n",
    "\n",
    "if vertex_function_response.candidates:\n",
    "    candidate = vertex_function_response.candidates[0]\n",
    "    \n",
    "    for part in candidate.content.parts:\n",
    "        if part.text:\n",
    "            print(f\"Text: {part.text}\")\n",
    "        elif part.function_call:\n",
    "            print(f\"Function: {part.function_call.name}\")\n",
    "            args = dict(part.function_call.args)\n",
    "            print(f\"Operation: {args['a']} {args['operation']} {args['b']}\")\n",
    "            print(f\"Arguments: {args}\")\n",
    "\n",
    "print(f\"\\nâœ… Function calling works identically on Vertex AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: AI Studio vs Vertex AI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_content = \"Explain artificial intelligence in exactly 2 sentences.\"\n",
    "\n",
    "print(\"=== Comparison: AI Studio vs Vertex AI ===\")\n",
    "\n",
    "# AI Studio\n",
    "studio_response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=test_content\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ¡ AI Studio:\")\n",
    "print(f\"Model: {studio_response.model_version}\")\n",
    "if studio_response.candidates:\n",
    "    print(f\"Response: {studio_response.candidates[0].content.parts[0].text}\")\n",
    "print(f\"Tokens: {studio_response.usage_metadata.prompt_token_count} + {studio_response.usage_metadata.candidates_token_count}\")\n",
    "\n",
    "# Vertex AI\n",
    "vertex_response = vertex_client.models.generate_content(\n",
    "    model='gemini-2.0-flash-001',\n",
    "    contents=test_content\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Vertex AI:\")\n",
    "print(f\"Model: {vertex_response.model_version}\")\n",
    "if vertex_response.candidates:\n",
    "    print(f\"Response: {vertex_response.candidates[0].content.parts[0].text}\")\n",
    "print(f\"Tokens: {vertex_response.usage_metadata.prompt_token_count} + {vertex_response.usage_metadata.candidates_token_count}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Key differences: authentication, enterprise features, and regional deployment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
