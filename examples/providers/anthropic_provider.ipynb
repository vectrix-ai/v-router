{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anthropic Provider Guide ðŸ¤–\n",
    "\n",
    "This notebook demonstrates how to use Anthropic's Claude models through different hosting options:\n",
    "- **Direct Anthropic**: Using Anthropic's native API\n",
    "- **Vertex AI**: Using Claude through Google Cloud's Vertex AI platform\n",
    "\n",
    "## Overview\n",
    "\n",
    "Anthropic offers Claude models through multiple hosting options, each with their own advantages:\n",
    "\n",
    "### Direct Anthropic Hosting\n",
    "- **Direct API access** to Anthropic's servers\n",
    "- **Latest models** available first\n",
    "- **Simple authentication** with API key\n",
    "- **Global availability** with multiple regions\n",
    "\n",
    "### Vertex AI Hosting\n",
    "- **Enterprise features** through Google Cloud\n",
    "- **VPC integration** for secure deployments\n",
    "- **Regional compliance** options\n",
    "- **Unified billing** with other Google Cloud services\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Anthropic Hosting\n",
    "\n",
    "### Authentication\n",
    "The Anthropic client automatically uses the `ANTHROPIC_API_KEY` environment variable for authentication.\n",
    "\n",
    "### Available Models\n",
    "- `claude-sonnet-4-20250514` - Latest Sonnet model\n",
    "- `claude-opus-4-20250514` - Most capable model for complex tasks\n",
    "- `claude-haiku-4-20250115` - Fastest model for simple tasks\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://github.com/anthropics/anthropic-sdk-python)\n",
    "- [API Reference](https://docs.anthropic.com/en/api/messages)\n",
    "- [Model Comparison](https://docs.anthropic.com/en/docs/about-claude/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Direct Anthropic Response ===\n",
      "Model: claude-sonnet-4-20250514\n",
      "Content: I'm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest. I enjoy engaging in conversations on a wide range of topics and helping with various tasks like analysis, writing, math, coding, and creative projects.\n",
      "Usage: 28 input + 54 output = 82 total tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_017LYki5Rop3NfJrkNYvWR9D',\n",
       " 'content': [{'citations': None,\n",
       "   'text': \"I'm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest. I enjoy engaging in conversations on a wide range of topics and helping with various tasks like analysis, writing, math, coding, and creative projects.\",\n",
       "   'type': 'text'}],\n",
       " 'model': 'claude-sonnet-4-20250514',\n",
       " 'role': 'assistant',\n",
       " 'stop_reason': 'end_turn',\n",
       " 'stop_sequence': None,\n",
       " 'type': 'message',\n",
       " 'usage': {'cache_creation_input_tokens': 0,\n",
       "  'cache_read_input_tokens': 0,\n",
       "  'input_tokens': 28,\n",
       "  'output_tokens': 54,\n",
       "  'server_tool_use': None,\n",
       "  'service_tier': 'standard'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import anthropic\n",
    "\n",
    "# Initialize client (uses ANTHROPIC_API_KEY from environment)\n",
    "client = anthropic.Anthropic()\n",
    "\n",
    "# Basic message creation\n",
    "message = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    system=\"You are a helpful AI assistant.\",  # System message as top-level parameter\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello, Claude! Tell me about yourself in 2 sentences.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Direct Anthropic Response ===\")\n",
    "print(f\"Model: {message.model}\")\n",
    "print(f\"Content: {message.content[0].text}\")\n",
    "print(f\"Usage: {message.usage.input_tokens} input + {message.usage.output_tokens} output = {message.usage.input_tokens + message.usage.output_tokens} total tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "message.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Configuration\n",
    "\n",
    "Claude supports various parameters for fine-tuning responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Creative Writing Response ===\n",
      "Opening line: The last human memory was uploaded at 3:47 AM on a Tuesday, and by Wednesday, the machines had already begun to forget what it felt like to dream.\n",
      "Stop reason: end_turn\n"
     ]
    }
   ],
   "source": [
    "# Advanced configuration example\n",
    "response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,  # More creative responses\n",
    "    top_p=0.9,        # Nucleus sampling\n",
    "    system=\"You are a helpful AI assistant specializing in creative writing.\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Write a creative opening line for a science fiction story.\"\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"=== Creative Writing Response ===\")\n",
    "print(f\"Opening line: {response.content[0].text}\")\n",
    "print(f\"Stop reason: {response.stop_reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "Claude supports function calling for interacting with external tools and APIs. Here's how to implement single and multiple tool calls:\n",
    "\n",
    "### Key Features:\n",
    "- **Multiple tool calls**: Claude can call multiple tools in a single response\n",
    "- **Structured inputs**: Tools use JSON schemas for parameter validation\n",
    "- **Conversation flow**: Tool results can be fed back for continued interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Tool Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tool Call Response ===\n",
      "Content blocks: 2\n",
      "Block 0: text\n",
      "  Text: I'll check the current weather in San Francisco for you.\n",
      "Block 1: tool_use\n",
      "  Tool: get_weather\n",
      "  ID: toolu_01Sf5WQ1drYBeFyPLcUssAXv\n",
      "  Input: {'location': 'San Francisco, CA'}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather tool\n",
    "weather_tool = {\n",
    "    \"name\": \"get_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "            },\n",
    "            \"units\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                \"description\": \"Temperature units\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "# Initial request with tool\n",
    "messages = [{\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}]\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-opus-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    tools=[weather_tool],\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(\"=== Tool Call Response ===\")\n",
    "print(f\"Content blocks: {len(response.content)}\")\n",
    "for i, content in enumerate(response.content):\n",
    "    print(f\"Block {i}: {content.type}\")\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text}\")\n",
    "    elif content.type == \"tool_use\":\n",
    "        print(f\"  Tool: {content.name}\")\n",
    "        print(f\"  ID: {content.id}\")\n",
    "        print(f\"  Input: {content.input}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": response.content\n",
    "})\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tool Execution ===\n",
      "Tool: get_weather\n",
      "Location: San Francisco, CA\n",
      "Result: 65Â°F, partly cloudy with light fog\n",
      "\n",
      "=== Final Response ===\n",
      "Claude's response: The current weather in San Francisco is 65Â°F (about 18Â°C), partly cloudy with light fog. This is typical San Francisco weather - mild temperatures with some coastal fog.\n"
     ]
    }
   ],
   "source": [
    "# Simulate tool execution\n",
    "def execute_weather_tool(location, units=\"fahrenheit\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"san francisco\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"18Â°C, partly cloudy with light fog\"\n",
    "        else:\n",
    "            return \"65Â°F, partly cloudy with light fog\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Find tool use in the response\n",
    "tool_use = None\n",
    "for content in response.content:\n",
    "    if content.type == \"tool_use\":\n",
    "        tool_use = content\n",
    "        break\n",
    "\n",
    "if tool_use:\n",
    "    # Execute the tool\n",
    "    location = tool_use.input.get(\"location\")\n",
    "    units = tool_use.input.get(\"units\", \"fahrenheit\")\n",
    "    weather_result = execute_weather_tool(location, units)\n",
    "    \n",
    "    print(f\"=== Tool Execution ===\")\n",
    "    print(f\"Tool: {tool_use.name}\")\n",
    "    print(f\"Location: {location}\")\n",
    "    print(f\"Result: {weather_result}\")\n",
    "    \n",
    "    # Send tool result back to Claude\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"tool_result\",\n",
    "                \"tool_use_id\": tool_use.id,\n",
    "                \"content\": weather_result\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Get final response\n",
    "    final_response = client.messages.create(\n",
    "        model=\"claude-opus-4-20250514\",\n",
    "        max_tokens=1024,\n",
    "        tools=[weather_tool],\n",
    "        messages=messages\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    print(f\"Claude's response: {final_response.content[0].text}\")\n",
    "else:\n",
    "    print(\"No tool use found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Tools Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multiple Tools Response ===\n",
      "Total content blocks: 3\n",
      "\n",
      "Block 1: text\n",
      "  Text: I'll help you with both requests - calculating 15 * 23 and searching for information about Anthropic Claude models.\n",
      "\n",
      "Block 2: tool_use\n",
      "  Tool: calculator\n",
      "  ID: toolu_014PUoqdUjmh2Vfqd4uzjX9H\n",
      "  Input: {'operation': 'multiply', 'a': 15, 'b': 23}\n",
      "\n",
      "Block 3: tool_use\n",
      "  Tool: web_search\n",
      "  ID: toolu_01EDhWuJGae4Bk67Ei2arCAg\n",
      "  Input: {'query': 'anthropic claude models'}\n",
      "\n",
      "Usage: 646 + 152 = 798 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define multiple tools\n",
    "calculator_tool = {\n",
    "    \"name\": \"calculator\",\n",
    "    \"description\": \"Perform basic mathematical operations\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"description\": \"The mathematical operation to perform\"\n",
    "            },\n",
    "            \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "            \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "        },\n",
    "        \"required\": [\"operation\", \"a\", \"b\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "search_tool = {\n",
    "    \"name\": \"web_search\",\n",
    "    \"description\": \"Search the web for information\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query\"\n",
    "            },\n",
    "            \"max_results\": {\n",
    "                \"type\": \"integer\",\n",
    "                \"description\": \"Maximum number of results\",\n",
    "                \"default\": 5\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Request that might use multiple tools\n",
    "multi_tool_response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=1024,\n",
    "    tools=[weather_tool, calculator_tool, search_tool],\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What's 15 * 23? Also, search for information about 'anthropic claude models'\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Tools Response ===\")\n",
    "print(f\"Total content blocks: {len(multi_tool_response.content)}\")\n",
    "\n",
    "for i, content in enumerate(multi_tool_response.content):\n",
    "    print(f\"\\nBlock {i+1}: {content.type}\")\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text}\")\n",
    "    elif content.type == \"tool_use\":\n",
    "        print(f\"  Tool: {content.name}\")\n",
    "        print(f\"  ID: {content.id}\")\n",
    "        print(f\"  Input: {content.input}\")\n",
    "\n",
    "print(f\"\\nUsage: {multi_tool_response.usage.input_tokens} + {multi_tool_response.usage.output_tokens} = {multi_tool_response.usage.input_tokens + multi_tool_response.usage.output_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vertex AI Hosting\n",
    "\n",
    "### Setup and Authentication\n",
    "Vertex AI requires Google Cloud project configuration and regional settings.\n",
    "\n",
    "### Key Differences:\n",
    "- **Project ID**: Must specify your Google Cloud project\n",
    "- **Region**: Choose deployment region for compliance/latency\n",
    "- **Model Names**: Slightly different naming convention with `@` version suffix\n",
    "- **Enterprise Features**: VPC integration, audit logging, and enterprise security\n",
    "\n",
    "### Available Regions:\n",
    "- `us-central1` - United States (primary)\n",
    "- `us-east4` - United States (secondary)\n",
    "- `europe-west1` - Europe (primary)\n",
    "- See [Vertex AI Locations](https://cloud.google.com/vertex-ai/docs/general/locations) for complete list\n",
    "\n",
    "### Resources:\n",
    "- [Anthropic on Vertex AI Documentation](https://docs.anthropic.com/en/api/claude-on-vertex-ai)\n",
    "- [Google Cloud Vertex AI](https://cloud.google.com/vertex-ai)\n",
    "- [Model Garden](https://console.cloud.google.com/vertex-ai/model-garden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Vertex AI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Vertex AI:\n",
      "  Project: vectrix-401014\n",
      "  Region: europe-west1\n",
      "\n",
      "=== Vertex AI Response ===\n",
      "Model: claude-sonnet-4-20250514\n",
      "Content: Here are the key differences between using Vertex AI and direct Anthropic hosting for Claude:\n",
      "\n",
      "## **Vertex AI (Google Cloud)**\n",
      "\n",
      "**Pros:**\n",
      "- **Enterprise integration**: Seamlessly integrates with other Google Cloud services (BigQuery, Cloud Storage, etc.)\n",
      "- **Unified billing**: Single invoice for all Google Cloud services\n",
      "- **Enterprise controls**: Advanced IAM, logging, monitoring through Google Cloud Console\n",
      "- **Compliance**: Leverages Google\n",
      "Usage: 24 + 100 = 124 tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'msg_vrtx_01Ei4SdeYxzbZg3S1qgeqKZm',\n",
       " 'content': [{'citations': None,\n",
       "   'text': 'Here are the key differences between using Vertex AI and direct Anthropic hosting for Claude:\\n\\n## **Vertex AI (Google Cloud)**\\n\\n**Pros:**\\n- **Enterprise integration**: Seamlessly integrates with other Google Cloud services (BigQuery, Cloud Storage, etc.)\\n- **Unified billing**: Single invoice for all Google Cloud services\\n- **Enterprise controls**: Advanced IAM, logging, monitoring through Google Cloud Console\\n- **Compliance**: Leverages Google',\n",
       "   'type': 'text'}],\n",
       " 'model': 'claude-sonnet-4-20250514',\n",
       " 'role': 'assistant',\n",
       " 'stop_reason': 'max_tokens',\n",
       " 'stop_sequence': None,\n",
       " 'type': 'message',\n",
       " 'usage': {'cache_creation_input_tokens': 0,\n",
       "  'cache_read_input_tokens': 0,\n",
       "  'input_tokens': 24,\n",
       "  'output_tokens': 100,\n",
       "  'server_tool_use': None,\n",
       "  'service_tier': None}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from anthropic import AnthropicVertex\n",
    "\n",
    "# Initialize Vertex AI client\n",
    "project_id = os.getenv(\"GCP_PROJECT_ID\")\n",
    "region = os.getenv(\"GCP_LOCATION\", \"us-central1\")  # Default to us-central1\n",
    "\n",
    "print(f\"Connecting to Vertex AI:\")\n",
    "print(f\"  Project: {project_id}\")\n",
    "print(f\"  Region: {region}\")\n",
    "\n",
    "vertex_client = AnthropicVertex(project_id=project_id, region=region)\n",
    "\n",
    "# Note: Vertex AI uses @ notation for model versions\n",
    "vertex_response = vertex_client.messages.create(\n",
    "    model=\"claude-sonnet-4@20250514\",\n",
    "    max_tokens=100,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hello! Explain the difference between Vertex AI and direct Anthropic hosting.\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"\\n=== Vertex AI Response ===\")\n",
    "print(f\"Model: {vertex_response.model}\")\n",
    "print(f\"Content: {vertex_response.content[0].text}\")\n",
    "print(f\"Usage: {vertex_response.usage.input_tokens} + {vertex_response.usage.output_tokens} = {vertex_response.usage.input_tokens + vertex_response.usage.output_tokens} tokens\")\n",
    "\n",
    "# Full response structure (same as direct Anthropic)\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "vertex_response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vertex AI Function Calling\n",
    "\n",
    "Function calling works identically on Vertex AI as with direct Anthropic hosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Vertex AI Tool Call ===\n",
      "Model: claude-sonnet-4-20250514\n",
      "Text: I'll calculate 42 divided by 7 for you.\n",
      "Tool: calculator\n",
      "Operation: divide\n",
      "Numbers: 42 divide 7\n",
      "\n",
      "âœ… Tool calling works identically on Vertex AI!\n"
     ]
    }
   ],
   "source": [
    "# Function calling works the same on Vertex AI\n",
    "vertex_tool_response = vertex_client.messages.create(\n",
    "    model=\"claude-sonnet-4@20250514\",\n",
    "    max_tokens=1024,\n",
    "    tools=[calculator_tool],  # Same tool definition as before\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Calculate 42 divided by 7\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Vertex AI Tool Call ===\")\n",
    "print(f\"Model: {vertex_tool_response.model}\")\n",
    "\n",
    "for content in vertex_tool_response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"Text: {content.text}\")\n",
    "    elif content.type == \"tool_use\":\n",
    "        print(f\"Tool: {content.name}\")\n",
    "        print(f\"Operation: {content.input['operation']}\")\n",
    "        print(f\"Numbers: {content.input['a']} {content.input['operation']} {content.input['b']}\")\n",
    "\n",
    "print(f\"\\nâœ… Tool calling works identically on Vertex AI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Direct vs Vertex AI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison: Direct vs Vertex AI ===\n",
      "\n",
      "ðŸ”µ Direct Anthropic:\n",
      "Model: claude-sonnet-4-20250514\n",
      "Response: Quantum computing harnesses quantum mechanics principles like superposition and entanglement to process information. Unlike classical bits (0 or 1), quantum bits (qubits) exist in multiple states simultaneously, enabling parallel computations. This allows quantum computers to potentially solve certain complex problems exponentially faster than classical computers, revolutionizing cryptography, optimization, and scientific simulation.\n",
      "Tokens: 18 + 79\n",
      "\n",
      "ðŸŸ¢ Vertex AI:\n",
      "Model: claude-sonnet-4-20250514\n",
      "Response: Quantum computing harnesses quantum mechanics principles like superposition and entanglement to process information. Unlike classical bits (0 or 1), quantum bits (qubits) exist in multiple states simultaneously, enabling parallel calculations. This allows quantum computers to solve certain complex problems exponentially faster than classical computers, particularly in cryptography, optimization, and simulation.\n",
      "Tokens: 18 + 75\n",
      "\n",
      "ðŸ’¡ Both use identical APIs and response formats!\n",
      "ðŸ’¡ Only differences: authentication method and model naming convention\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing in exactly 50 words.\"}\n",
    "]\n",
    "\n",
    "print(\"=== Comparison: Direct vs Vertex AI ===\")\n",
    "\n",
    "# Direct Anthropic\n",
    "direct_response = client.messages.create(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    max_tokens=100,\n",
    "    messages=test_messages\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Direct Anthropic:\")\n",
    "print(f\"Model: {direct_response.model}\")\n",
    "print(f\"Response: {direct_response.content[0].text}\")\n",
    "print(f\"Tokens: {direct_response.usage.input_tokens} + {direct_response.usage.output_tokens}\")\n",
    "\n",
    "# Vertex AI\n",
    "vertex_response = vertex_client.messages.create(\n",
    "    model=\"claude-sonnet-4@20250514\",\n",
    "    max_tokens=100,\n",
    "    messages=test_messages\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ¢ Vertex AI:\")\n",
    "print(f\"Model: {vertex_response.model}\")\n",
    "print(f\"Response: {vertex_response.content[0].text}\")\n",
    "print(f\"Tokens: {vertex_response.usage.input_tokens} + {vertex_response.usage.output_tokens}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Only differences: authentication method and model naming convention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### When to Use Direct Anthropic:\n",
    "- **Rapid prototyping** and development\n",
    "- **Latest model access** and features\n",
    "- **Simple deployment** requirements\n",
    "- **Global applications** without specific regional needs\n",
    "\n",
    "### When to Use Vertex AI:\n",
    "- **Enterprise deployments** with strict security requirements\n",
    "- **VPC integration** for network isolation\n",
    "- **Regional compliance** needs (data residency)\n",
    "- **Unified Google Cloud billing** and management\n",
    "- **Advanced monitoring** and audit logging requirements\n",
    "\n",
    "### Error Handling:\n",
    "```python\n",
    "try:\n",
    "    response = client.messages.create(...)  # or vertex_client\n",
    "except anthropic.APIError as e:\n",
    "    print(f\"API Error: {e}\")\n",
    "except anthropic.RateLimitError as e:\n",
    "    print(f\"Rate limit exceeded: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "```\n",
    "\n",
    "### Performance Tips:\n",
    "- **Choose appropriate max_tokens** based on your use case\n",
    "- **Use streaming** for long responses: `stream=True`\n",
    "- **Implement retry logic** for production applications\n",
    "- **Monitor token usage** to optimize costs\n",
    "\n",
    "## Summary\n",
    "\n",
    "âœ… **Identical APIs**: Both hosting options use the same Anthropic Python SDK  \n",
    "âœ… **Same Features**: Function calling, streaming, and all model capabilities work identically  \n",
    "âœ… **Flexible Deployment**: Choose based on your security, compliance, and infrastructure needs  \n",
    "âœ… **Enterprise Ready**: Vertex AI provides additional enterprise features when needed  \n",
    "\n",
    "Both direct Anthropic and Vertex AI hosting provide access to the same powerful Claude models with identical APIs, giving you flexibility in how you deploy and scale your applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
