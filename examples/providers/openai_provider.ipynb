{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Provider Guide ðŸš€\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's models through different hosting options:\n",
    "- **Direct OpenAI**: Using OpenAI's native API\n",
    "- **Azure OpenAI**: Using OpenAI models through Microsoft Azure\n",
    "\n",
    "## Overview\n",
    "\n",
    "OpenAI offers their models through multiple hosting platforms, each with distinct advantages:\n",
    "\n",
    "### Direct OpenAI Hosting\n",
    "- **Latest models** available immediately upon release\n",
    "- **Global availability** with worldwide access\n",
    "- **Simple authentication** with API key\n",
    "- **Full feature set** including all model capabilities\n",
    "- **Competitive pricing** with usage-based billing\n",
    "\n",
    "### Azure OpenAI Hosting\n",
    "- **Enterprise security** and compliance features\n",
    "- **Regional deployment** for data residency requirements\n",
    "- **VNet integration** for private network access\n",
    "- **Microsoft ecosystem** integration\n",
    "- **SLA guarantees** for production workloads\n",
    "- **Content filtering** and safety features\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct OpenAI Hosting\n",
    "\n",
    "### Authentication\n",
    "The OpenAI client automatically uses the `OPENAI_API_KEY` environment variable for authentication.\n",
    "\n",
    "### Available Models (as of 2025)\n",
    "- `gpt-4.1` - Latest and most capable model\n",
    "- `gpt-4o` - Optimized for speed and efficiency\n",
    "- `gpt-4-turbo` - Previous generation flagship model\n",
    "- `gpt-3.5-turbo` - Fast and cost-effective option\n",
    "\n",
    "### Key Features\n",
    "- **Function calling** for tool integration\n",
    "- **Structured outputs** with JSON mode\n",
    "- **Vision capabilities** for image understanding\n",
    "- **Code generation** and analysis\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://platform.openai.com/docs/libraries?language=python)\n",
    "- [API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Model Documentation](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example\n",
    "\n",
    "**Note**: OpenAI recently updated their API with a new `responses` endpoint that provides enhanced capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Direct OpenAI Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Status: completed\n",
      "Content: With a swish of her tail and a mischievous wink, Whiskers the tabby leapt through the laundry basket and landed in ancient Egypt, where she was immediately mistaken for a goddess and offered a golden pyramid of tuna.\n",
      "Role: assistant\n",
      "Usage: 22 input + 49 output = 71 total tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_683b61ac753081a0805500beedcd190f089e89ddbfb758aa',\n",
       " 'created_at': 1748722092.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'msg_683b61ae2c4481a098c5d3a1e044a965089e89ddbfb758aa',\n",
       "   'content': [{'annotations': [],\n",
       "     'text': 'With a swish of her tail and a mischievous wink, Whiskers the tabby leapt through the laundry basket and landed in ancient Egypt, where she was immediately mistaken for a goddess and offered a golden pyramid of tuna.',\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None, 'summary': None},\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 22,\n",
       "  'input_tokens_details': {'cached_tokens': 0},\n",
       "  'output_tokens': 49,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 71},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client (uses OPENAI_API_KEY from environment)\n",
    "client = OpenAI()\n",
    "\n",
    "# Using the new responses API for enhanced capabilities\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a creative one-sentence story about a time-traveling cat.\"\n",
    ")\n",
    "\n",
    "print(\"=== Direct OpenAI Response ===\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Status: {response.status}\")\n",
    "\n",
    "# Extract the message content\n",
    "if response.output and len(response.output) > 0:\n",
    "    message = response.output[0]\n",
    "    print(f\"Content: {message.content[0].text}\")\n",
    "    print(f\"Role: {message.role}\")\n",
    "\n",
    "print(f\"Usage: {response.usage.input_tokens} input + {response.usage.output_tokens} output = {response.usage.total_tokens} total tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API\n",
    "\n",
    "For traditional chat-style interactions, you can also use the chat completions endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Completions Response ===\n",
      "Model: gpt-4o-2024-08-06\n",
      "Content: The last thing Detective Marlowe expected to find in the abandoned lighthouse was a meticulously set dinner table for two.\n",
      "Finish reason: stop\n",
      "Usage: 31 + 23 = 54 tokens\n"
     ]
    }
   ],
   "source": [
    "# Traditional chat completions API\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in creative writing.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write an opening line for a mystery novel.\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Chat Completions Response ===\")\n",
    "print(f\"Model: {chat_response.model}\")\n",
    "print(f\"Content: {chat_response.choices[0].message.content}\")\n",
    "print(f\"Finish reason: {chat_response.choices[0].finish_reason}\")\n",
    "print(f\"Usage: {chat_response.usage.prompt_tokens} + {chat_response.usage.completion_tokens} = {chat_response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "OpenAI's function calling allows models to interact with external tools and APIs. Here's how to implement it:\n",
    "\n",
    "### Key Features:\n",
    "- **Parallel function calls**: Multiple tools can be called simultaneously\n",
    "- **Structured schemas**: Tools use JSON schemas for parameter validation\n",
    "- **Tool choice control**: Force specific tools or let the model choose\n",
    "- **Conversation continuity**: Results can be fed back for extended interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Function Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Call Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: None\n",
      "\n",
      "Function calls: 1\n",
      "  Call 1:\n",
      "    ID: call_pOJkoy8lSK57rlYYod9WVlhk\n",
      "    Function: get_weather\n",
      "    Arguments: {\"location\":\"Paris, France\",\"units\":\"celsius\"}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather function\n",
    "weather_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. New York, USA\"\n",
    "                },\n",
    "                \"units\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature units\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initial request with function\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}]\n",
    "\n",
    "function_response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=messages,\n",
    "    tools=[weather_function],\n",
    "    tool_choice=\"auto\"  # Let the model decide when to use tools\n",
    ")\n",
    "\n",
    "print(\"=== Function Call Response ===\")\n",
    "print(f\"Model: {function_response.model}\")\n",
    "\n",
    "message = function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nFunction calls: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls):\n",
    "        print(f\"  Call {i+1}:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Function: {tool_call.function.name}\")\n",
    "        print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append(message)\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Execution ===\n",
      "Function: get_weather\n",
      "Location: Paris, France\n",
      "Units: celsius\n",
      "Result: 20Â°C, sunny with light clouds\n",
      "\n",
      "=== Final Response ===\n",
      "GPT's response: The weather in Paris today is 20Â°C, sunny with light clouds. Itâ€™s a pleasant day to be outside!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Simulate function execution\n",
    "def execute_weather_function(location, units=\"fahrenheit\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"paris\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"20Â°C, sunny with light clouds\"\n",
    "        else:\n",
    "            return \"68Â°F, sunny with light clouds\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Execute function calls if any\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        if tool_call.function.name == \"get_weather\":\n",
    "            # Parse function arguments\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            location = args.get(\"location\")\n",
    "            units = args.get(\"units\", \"fahrenheit\")\n",
    "            \n",
    "            # Execute the function\n",
    "            weather_result = execute_weather_function(location, units)\n",
    "            \n",
    "            print(f\"=== Function Execution ===\")\n",
    "            print(f\"Function: {tool_call.function.name}\")\n",
    "            print(f\"Location: {location}\")\n",
    "            print(f\"Units: {units}\")\n",
    "            print(f\"Result: {weather_result}\")\n",
    "            \n",
    "            # Add function result to conversation\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": weather_result,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    \n",
    "    # Get final response with function results\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        tools=[weather_function]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    print(f\"GPT's response: {final_response.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"No function calls found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multiple Functions Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: None\n",
      "\n",
      "Function calls: 2\n",
      "  Call 1:\n",
      "    ID: call_VKjGDrigCRuPxh8INMbFKBXW\n",
      "    Function: calculator\n",
      "    Arguments: {\"operation\": \"multiply\", \"a\": 25, \"b\": 16}\n",
      "  Call 2:\n",
      "    ID: call_bUwqQXSXQQFUu6k7qg0J31Rm\n",
      "    Function: web_search\n",
      "    Arguments: {\"query\": \"OpenAI GPT-4 capabilities\"}\n",
      "\n",
      "Usage: 180 + 57 = 237 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define multiple functions\n",
    "calculator_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Perform basic mathematical operations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"operation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                    \"description\": \"The mathematical operation\"\n",
    "                },\n",
    "                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "                \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "            },\n",
    "            \"required\": [\"operation\", \"a\", \"b\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"Search the web for information\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Request that might use multiple functions\n",
    "multi_function_response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Calculate 25 * 16, and also search for 'OpenAI GPT-4 capabilities'\"\n",
    "        }\n",
    "    ],\n",
    "    tools=[weather_function, calculator_function, search_function],\n",
    "    parallel_tool_calls=True  # Enable parallel function calls\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Functions Response ===\")\n",
    "print(f\"Model: {multi_function_response.model}\")\n",
    "\n",
    "message = multi_function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nFunction calls: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls):\n",
    "        print(f\"  Call {i+1}:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Function: {tool_call.function.name}\")\n",
    "        print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "        \n",
    "print(f\"\\nUsage: {multi_function_response.usage.prompt_tokens} + {multi_function_response.usage.completion_tokens} = {multi_function_response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and PDF-files\n",
    "\n",
    "### Images\n",
    "First let's try sending an image using a base64 encoded string (local file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a close-up (macro) photograph of an ant. The image captures a side view of the ant, highlighting its segmented body, long legs, antennae, and detailed hairs on its body. The ant appears to be in a defensive or alert posture, possibly lifting its upper body and antennae. The background is blurred, focusing attention on the ant as the central subject.\n"
     ]
    }
   ],
   "source": [
    "# For base64-encoded images\n",
    "image1_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "image1_media_type = \"image/jpeg\"\n",
    "image1_data = base64.standard_b64encode(httpx.get(image1_url).content).decode(\"utf-8\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{image1_data}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just pass a URL if it's a public image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image shows a peaceful landscape with a wooden boardwalk stretching straight ahead through a grassy field. The sky is partly cloudy with a lot of blue visible, suggesting a pleasant day. On both sides of the boardwalk, there is tall green grass, and in the distance, there are some trees and shrubs. The scene evokes a sense of calm and natural beauty.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF-Files\n",
    "Use a base64 encoded PDF-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is an addendum to the model card for Anthropicâ€™s Claude 3.5 Sonnet and introduces both an upgraded version of Claude 3.5 Sonnet and the new Claude 3.5 Haiku, highlighting their enhanced capabilities in reasoning, coding, visual processing, and especially computer use (interacting with GUIs from screenshots). It provides detailed performance evaluations, safety considerations, benchmark results, and safety procedures to inform users and the AI community about these models' advancements and responsible deployment.\n"
     ]
    }
   ],
   "source": [
    "# First, load and encode the PDF \n",
    "pdf_url = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\n",
    "pdf_data = base64.standard_b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"filename\": \"Claude-3-Model-Card-October-Addendum.pdf\",\n",
    "                    \"file_data\": f\"data:application/pdf;base64,{pdf_data}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Explain what this document is about in 2 sentences. \",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly pass a PDF-file to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'file-Cnt6BvgXHajtnod2Da7Nsm',\n",
       " 'bytes': 1042289,\n",
       " 'created_at': 1748723552,\n",
       " 'filename': 'gameboy_color.pdf',\n",
       " 'object': 'file',\n",
       " 'purpose': 'user_data',\n",
       " 'status': 'processed',\n",
       " 'expires_at': None,\n",
       " 'status_details': None}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"./assets/gameboy_color.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "file.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This PDF is the instruction booklet for the Nintendo Game Boy Color, providing setup, usage, troubleshooting information, and warranty details. It includes diagrams, safety instructions, and guidelines for playing and maintaining the Game Boy Color system.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Explain what this PDF is about in two sentences.\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_683b6770b5c8819c9f8ada170d6e976c0d259d9aa9e7bfa8',\n",
       " 'created_at': 1748723571.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'msg_683b6774e298819cb00742da4f70e2900d259d9aa9e7bfa8',\n",
       "   'content': [{'annotations': [],\n",
       "     'text': 'This PDF is the instruction booklet for the Nintendo Game Boy Color handheld video game system. It provides information on setup, usage, troubleshooting, and warranty details for the device and its accessories.',\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None, 'summary': None},\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 1621,\n",
       "  'input_tokens_details': {'cached_tokens': 0},\n",
       "  'output_tokens': 38,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 1659},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI\n",
    "\n",
    "### Setup and Authentication\n",
    "Azure OpenAI requires specific configuration including endpoint, API version, and deployment names.\n",
    "\n",
    "### Key Differences:\n",
    "- **Custom endpoint**: Your Azure OpenAI resource endpoint\n",
    "- **API version**: Specific API version for compatibility\n",
    "- **Deployment names**: Models are deployed with custom names\n",
    "- **Regional availability**: Not all models available in all regions\n",
    "- **Content filtering**: Additional safety and content filtering layers\n",
    "\n",
    "### Available Regions:\n",
    "- `eastus2` - United States East (primary for US models)\n",
    "- `swedencentral` - Sweden Central (primary for EU models)\n",
    "- `australiaeast` - Australia East\n",
    "- See [Azure OpenAI Regions](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) for complete list\n",
    "\n",
    "### Important Notes:\n",
    "- **Model deployment required**: You must deploy models through Azure AI Studio\n",
    "- **Custom names**: Deployment names can be different from model names\n",
    "- **Quota management**: Each region has specific quotas and limits\n",
    "\n",
    "### Resources:\n",
    "- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/)\n",
    "- [Azure AI Studio](https://ai.azure.com/)\n",
    "- [Model Availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Azure OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure OpenAI Configuration:\n",
      "  Endpoint: https://vectrix-ai.openai.azure.com/\n",
      "  API Version: 2025-01-01-preview\n",
      "\n",
      "=== Azure OpenAI Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: Certainly! **Azure OpenAI** and **OpenAI (direct)** both provide access to powerful language models like GPT-4, but using Azure OpenAI through Microsoftâ€™s Azure cloud platform offers several distinct benefits, especially for enterprise and business use cases. Hereâ€™s a breakdown:\n",
      "\n",
      "---\n",
      "\n",
      "## **1. Enterprise-Grade Security & Compliance**\n",
      "\n",
      "- **Data Privacy & Residency:** Azure allows you to keep your data within specific regions or countries, complying with local regulations (e.g., GDPR, HIPAA).\n",
      "- **Advanced Security:** Integration with Azure security features like Azure Active Directory, role-based access control, private networking (VNET), and auditing.\n",
      "- **Certifications:** Azure OpenAI is certified for many industry standards (ISO, SOC, etc.), which is often required for regulated industries.\n",
      "\n",
      "---\n",
      "\n",
      "## **2. Integration with Azure Ecosystem**\n",
      "\n",
      "- **Easier Integration:** Seamlessly connect models with other Azure services (e.g., Logic Apps, Power Platform, Azure Functions, Cognitive Search).\n",
      "-\n",
      "Usage: 20 + 200 = 220 tokens\n",
      "System fingerprint: fp_3dfb47c1f3\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-BctcDCmWidPuvkQRBh52t4Lu6PIRU',\n",
       " 'choices': [{'finish_reason': 'length',\n",
       "   'index': 0,\n",
       "   'logprobs': None,\n",
       "   'message': {'content': 'Certainly! **Azure OpenAI** and **OpenAI (direct)** both provide access to powerful language models like GPT-4, but using Azure OpenAI through Microsoftâ€™s Azure cloud platform offers several distinct benefits, especially for enterprise and business use cases. Hereâ€™s a breakdown:\\n\\n---\\n\\n## **1. Enterprise-Grade Security & Compliance**\\n\\n- **Data Privacy & Residency:** Azure allows you to keep your data within specific regions or countries, complying with local regulations (e.g., GDPR, HIPAA).\\n- **Advanced Security:** Integration with Azure security features like Azure Active Directory, role-based access control, private networking (VNET), and auditing.\\n- **Certifications:** Azure OpenAI is certified for many industry standards (ISO, SOC, etc.), which is often required for regulated industries.\\n\\n---\\n\\n## **2. Integration with Azure Ecosystem**\\n\\n- **Easier Integration:** Seamlessly connect models with other Azure services (e.g., Logic Apps, Power Platform, Azure Functions, Cognitive Search).\\n-',\n",
       "    'refusal': None,\n",
       "    'role': 'assistant',\n",
       "    'annotations': [],\n",
       "    'audio': None,\n",
       "    'function_call': None,\n",
       "    'tool_calls': None},\n",
       "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
       "    'protected_material_code': {'filtered': False, 'detected': False},\n",
       "    'protected_material_text': {'filtered': False, 'detected': False},\n",
       "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
       "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
       "    'violence': {'filtered': False, 'severity': 'safe'}}}],\n",
       " 'created': 1748609949,\n",
       " 'model': 'gpt-4.1-2025-04-14',\n",
       " 'object': 'chat.completion',\n",
       " 'service_tier': None,\n",
       " 'system_fingerprint': 'fp_3dfb47c1f3',\n",
       " 'usage': {'completion_tokens': 200,\n",
       "  'prompt_tokens': 20,\n",
       "  'total_tokens': 220,\n",
       "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "   'audio_tokens': 0,\n",
       "   'reasoning_tokens': 0,\n",
       "   'rejected_prediction_tokens': 0},\n",
       "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       " 'prompt_filter_results': [{'prompt_index': 0,\n",
       "   'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'},\n",
       "    'jailbreak': {'filtered': False, 'detected': False},\n",
       "    'self_harm': {'filtered': False, 'severity': 'safe'},\n",
       "    'sexual': {'filtered': False, 'severity': 'safe'},\n",
       "    'violence': {'filtered': False, 'severity': 'safe'}}}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-01-01-preview\",  # Use latest API version\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "print(f\"Azure OpenAI Configuration:\")\n",
    "print(f\"  Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"  API Version: 2025-01-01-preview\")\n",
    "\n",
    "# Note: Model name should match your deployment name in Azure\n",
    "azure_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # This should match your deployment name\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the benefits of using Azure OpenAI over direct OpenAI.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\n=== Azure OpenAI Response ===\")\n",
    "print(f\"Model: {azure_response.model}\")\n",
    "print(f\"Content: {azure_response.choices[0].message.content}\")\n",
    "print(f\"Usage: {azure_response.usage.prompt_tokens} + {azure_response.usage.completion_tokens} = {azure_response.usage.total_tokens} tokens\")\n",
    "\n",
    "# System fingerprint for version tracking\n",
    "if hasattr(azure_response, 'system_fingerprint'):\n",
    "    print(f\"System fingerprint: {azure_response.system_fingerprint}\")\n",
    "\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "azure_response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI Function Calling\n",
    "\n",
    "Function calling works identically on Azure OpenAI as with direct OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Azure OpenAI Function Call ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: None\n",
      "\n",
      "Function call:\n",
      "  Function: calculator\n",
      "  Arguments: {\"operation\":\"divide\",\"a\":144,\"b\":12}\n",
      "  Operation: 144 divide 12\n",
      "\n",
      "âœ… Function calling works identically on Azure OpenAI!\n"
     ]
    }
   ],
   "source": [
    "# Function calling works the same on Azure OpenAI\n",
    "azure_function_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # Your Azure deployment name\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Calculate 144 divided by 12\"}\n",
    "    ],\n",
    "    tools=[calculator_function],  # Same function definition as before\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"=== Azure OpenAI Function Call ===\")\n",
    "print(f\"Model: {azure_function_response.model}\")\n",
    "\n",
    "message = azure_function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        print(f\"\\nFunction call:\")\n",
    "        print(f\"  Function: {tool_call.function.name}\")\n",
    "        print(f\"  Arguments: {tool_call.function.arguments}\")\n",
    "        \n",
    "        # Parse and display the calculation\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        print(f\"  Operation: {args['a']} {args['operation']} {args['b']}\")\n",
    "\n",
    "print(f\"\\nâœ… Function calling works identically on Azure OpenAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Direct OpenAI vs Azure OpenAI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Comparison: Direct OpenAI vs Azure OpenAI ===\n",
      "\n",
      "ðŸ”µ Direct OpenAI:\n",
      "Model: gpt-4o-2024-08-06\n",
      "Response: Machine learning is a branch of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed. It involves the use of algorithms and statistical models to analyze and draw inferences from patterns within data. This process allows machines to autonomously make predictions or decisions based on new, unseen data.\n",
      "Tokens: 16 + 60\n",
      "\n",
      "ðŸŸ  Azure OpenAI:\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Response: Machine learning is a branch of artificial intelligence that enables computers to learn from data without being explicitly programmed. It involves creating algorithms that identify patterns and make predictions or decisions based on new data. This process improves over time as the system is exposed to more data and feedback.\n",
      "Tokens: 16 + 54\n",
      "\n",
      "ðŸ’¡ Both use identical APIs and response formats!\n",
      "ðŸ’¡ Key differences: authentication, endpoints, and enterprise features\n"
     ]
    }
   ],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain machine learning in exactly 3 sentences.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Comparison: Direct OpenAI vs Azure OpenAI ===\")\n",
    "\n",
    "# Direct OpenAI\n",
    "direct_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=test_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Direct OpenAI:\")\n",
    "print(f\"Model: {direct_response.model}\")\n",
    "print(f\"Response: {direct_response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {direct_response.usage.prompt_tokens} + {direct_response.usage.completion_tokens}\")\n",
    "\n",
    "# Azure OpenAI\n",
    "azure_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # Your deployment name\n",
    "    messages=test_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ  Azure OpenAI:\")\n",
    "print(f\"Model: {azure_response.model}\")\n",
    "print(f\"Response: {azure_response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {azure_response.usage.prompt_tokens} + {azure_response.usage.completion_tokens}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Key differences: authentication, endpoints, and enterprise features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
