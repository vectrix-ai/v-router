{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Provider Guide ðŸš€\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's models through different hosting options:\n",
    "- **Direct OpenAI**: Using OpenAI's native API\n",
    "- **Azure OpenAI**: Using OpenAI models through Microsoft Azure\n",
    "\n",
    "## Overview\n",
    "\n",
    "OpenAI offers their models through multiple hosting platforms, each with distinct advantages:\n",
    "\n",
    "### Direct OpenAI Hosting\n",
    "- **Latest models** available immediately upon release\n",
    "- **Global availability** with worldwide access\n",
    "- **Simple authentication** with API key\n",
    "- **Full feature set** including all model capabilities\n",
    "- **Competitive pricing** with usage-based billing\n",
    "\n",
    "### Azure OpenAI Hosting\n",
    "- **Enterprise security** and compliance features\n",
    "- **Regional deployment** for data residency requirements\n",
    "- **VNet integration** for private network access\n",
    "- **Microsoft ecosystem** integration\n",
    "- **SLA guarantees** for production workloads\n",
    "- **Content filtering** and safety features\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct OpenAI Hosting\n",
    "\n",
    "### Authentication\n",
    "The OpenAI client automatically uses the `OPENAI_API_KEY` environment variable for authentication.\n",
    "\n",
    "### Available Models (as of 2025)\n",
    "- `gpt-4.1` - Latest and most capable model\n",
    "- `gpt-4o` - Optimized for speed and efficiency\n",
    "- `gpt-4-turbo` - Previous generation flagship model\n",
    "- `gpt-3.5-turbo` - Fast and cost-effective option\n",
    "\n",
    "### Key Features\n",
    "- **Function calling** for tool integration\n",
    "- **Structured outputs** with JSON mode\n",
    "- **Vision capabilities** for image understanding\n",
    "- **Code generation** and analysis\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://platform.openai.com/docs/libraries?language=python)\n",
    "- [API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Model Documentation](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example\n",
    "\n",
    "**Note**: OpenAI recently updated their API with a new `responses` endpoint that provides enhanced capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Direct OpenAI Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Status: completed\n",
      "Content: With a flick of her tail and a spark from the ancient clock, Miso the cat leaped through centuries, pawing at pharaohs in Egypt and curling up on Shakespeareâ€™s quill in search of the worldâ€™s coziest sunbeam.\n",
      "Role: assistant\n",
      "Usage: 22 input + 52 output = 74 total tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_6846073f9318819cafaaf8a96d2e94a50c0654e45455f799',\n",
       " 'created_at': 1749419839.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'msg_6846074050f8819cb97005b790f62a8e0c0654e45455f799',\n",
       "   'content': [{'annotations': [],\n",
       "     'text': 'With a flick of her tail and a spark from the ancient clock, Miso the cat leaped through centuries, pawing at pharaohs in Egypt and curling up on Shakespeareâ€™s quill in search of the worldâ€™s coziest sunbeam.',\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None, 'summary': None},\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 22,\n",
       "  'input_tokens_details': {'cached_tokens': 0},\n",
       "  'output_tokens': 52,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 74},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client (uses OPENAI_API_KEY from environment)\n",
    "client = OpenAI()\n",
    "\n",
    "# Using the new responses API for enhanced capabilities\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a creative one-sentence story about a time-traveling cat.\"\n",
    ")\n",
    "\n",
    "print(\"=== Direct OpenAI Response ===\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Status: {response.status}\")\n",
    "\n",
    "# Extract the message content\n",
    "if response.output and len(response.output) > 0:\n",
    "    message = response.output[0]\n",
    "    print(f\"Content: {message.content[0].text}\")\n",
    "    print(f\"Role: {message.role}\")\n",
    "\n",
    "print(f\"Usage: {response.usage.input_tokens} input + {response.usage.output_tokens} output = {response.usage.total_tokens} total tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API\n",
    "\n",
    "For traditional chat-style interactions, you can also use the chat completions endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Completions Response ===\n",
      "Model: gpt-4o-2024-08-06\n",
      "Content: The old grandfather clock struck midnight just as the phone rang, and Detective Harper knew that no good news ever came with the twelfth chime.\n",
      "Finish reason: stop\n",
      "Usage: 31 + 29 = 60 tokens\n"
     ]
    }
   ],
   "source": [
    "# Traditional chat completions API\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in creative writing.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write an opening line for a mystery novel.\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Chat Completions Response ===\")\n",
    "print(f\"Model: {chat_response.model}\")\n",
    "print(f\"Content: {chat_response.choices[0].message.content}\")\n",
    "print(f\"Finish reason: {chat_response.choices[0].finish_reason}\")\n",
    "print(f\"Usage: {chat_response.usage.prompt_tokens} + {chat_response.usage.completion_tokens} = {chat_response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "OpenAI's function calling allows models to interact with external tools and APIs. Here's how to implement it:\n",
    "\n",
    "### Key Features:\n",
    "- **Parallel function calls**: Multiple tools can be called simultaneously\n",
    "- **Structured schemas**: Tools use JSON schemas for parameter validation\n",
    "- **Tool choice control**: Force specific tools or let the model choose\n",
    "- **Conversation continuity**: Results can be fed back for extended interactions\n",
    "\n",
    "\n",
    "### Force tool use\n",
    "By default the model will determine when and how many tools to use. You can force specific behavior with the tool_choice parameter.\n",
    "- **Auto**: (Default) Call zero, one, or multiple functions. tool_choice: \"auto\"\n",
    "- **Required**: Call one or more functions. tool_choice: \"required\"\n",
    "- **Forced Function**: Call exactly one specific function: ```tool_choice: {\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Function Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Call Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: None\n",
      "\n",
      "Function calls: 1\n",
      "  Call 1:\n",
      "    ID: call_5978HUJ3uCZZEUg8AmMt8Om8\n",
      "    Function: get_weather\n",
      "    Arguments: {\"location\":\"Paris, France\"}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather function\n",
    "weather_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. New York, USA\"\n",
    "                },\n",
    "                \"units\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature units\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initial request with function\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}]\n",
    "\n",
    "function_response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=messages,\n",
    "    tools=[weather_function],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\"\n",
    "        }\n",
    "    },  # Force this specific function to be called\n",
    ")\n",
    "\n",
    "print(\"=== Function Call Response ===\")\n",
    "print(f\"Model: {function_response.model}\")\n",
    "\n",
    "message = function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nFunction calls: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls):\n",
    "        print(f\"  Call {i+1}:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Function: {tool_call.function.name}\")\n",
    "        print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append(message)\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Simulate function execution\n",
    "def execute_weather_function(location, units=\"fahrenheit\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"paris\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"20Â°C, sunny with light clouds\"\n",
    "        else:\n",
    "            return \"68Â°F, sunny with light clouds\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Execute function calls if any\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        if tool_call.function.name == \"get_weather\":\n",
    "            # Parse function arguments\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            location = args.get(\"location\")\n",
    "            units = args.get(\"units\", \"fahrenheit\")\n",
    "            \n",
    "            # Execute the function\n",
    "            weather_result = execute_weather_function(location, units)\n",
    "            \n",
    "            print(f\"=== Function Execution ===\")\n",
    "            print(f\"Function: {tool_call.function.name}\")\n",
    "            print(f\"Location: {location}\")\n",
    "            print(f\"Units: {units}\")\n",
    "            print(f\"Result: {weather_result}\")\n",
    "            \n",
    "            # Add function result to conversation\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": weather_result,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    \n",
    "    # Get final response with function results\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        tools=[weather_function]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    print(f\"GPT's response: {final_response.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"No function calls found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define multiple functions\n",
    "calculator_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Perform basic mathematical operations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"operation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                    \"description\": \"The mathematical operation\"\n",
    "                },\n",
    "                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "                \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "            },\n",
    "            \"required\": [\"operation\", \"a\", \"b\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"Search the web for information\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Request that might use multiple functions\n",
    "multi_function_response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Calculate 25 * 16, and also search for 'OpenAI GPT-4 capabilities'\"\n",
    "        }\n",
    "    ],\n",
    "    tools=[weather_function, calculator_function, search_function],\n",
    "    parallel_tool_calls=True  # Enable parallel function calls\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Functions Response ===\")\n",
    "print(f\"Model: {multi_function_response.model}\")\n",
    "\n",
    "message = multi_function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nFunction calls: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls):\n",
    "        print(f\"  Call {i+1}:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Function: {tool_call.function.name}\")\n",
    "        print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "        \n",
    "print(f\"\\nUsage: {multi_function_response.usage.prompt_tokens} + {multi_function_response.usage.completion_tokens} = {multi_function_response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and PDF-files\n",
    "\n",
    "### Images\n",
    "First let's try sending an image using a base64 encoded string (local file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For base64-encoded images\n",
    "image1_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "image1_media_type = \"image/jpeg\"\n",
    "image1_data = base64.standard_b64encode(httpx.get(image1_url).content).decode(\"utf-8\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{image1_data}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just pass a URL if it's a public image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF-Files\n",
    "Use a base64 encoded PDF-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, load and encode the PDF \n",
    "pdf_url = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\n",
    "pdf_data = base64.standard_b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"filename\": \"Claude-3-Model-Card-October-Addendum.pdf\",\n",
    "                    \"file_data\": f\"data:application/pdf;base64,{pdf_data}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Explain what this document is about in 2 sentences. \",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly pass a PDF-file to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"./assets/gameboy_color.pdf\", \"rb\"),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "file.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Explain what this PDF is about in two sentences.\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI\n",
    "\n",
    "### Setup and Authentication\n",
    "Azure OpenAI requires specific configuration including endpoint, API version, and deployment names.\n",
    "\n",
    "### Key Differences:\n",
    "- **Custom endpoint**: Your Azure OpenAI resource endpoint\n",
    "- **API version**: Specific API version for compatibility\n",
    "- **Deployment names**: Models are deployed with custom names\n",
    "- **Regional availability**: Not all models available in all regions\n",
    "- **Content filtering**: Additional safety and content filtering layers\n",
    "\n",
    "### Available Regions:\n",
    "- `eastus2` - United States East (primary for US models)\n",
    "- `swedencentral` - Sweden Central (primary for EU models)\n",
    "- `australiaeast` - Australia East\n",
    "- See [Azure OpenAI Regions](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) for complete list\n",
    "\n",
    "### Important Notes:\n",
    "- **Model deployment required**: You must deploy models through Azure AI Studio\n",
    "- **Custom names**: Deployment names can be different from model names\n",
    "- **Quota management**: Each region has specific quotas and limits\n",
    "\n",
    "### Resources:\n",
    "- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/)\n",
    "- [Azure AI Studio](https://ai.azure.com/)\n",
    "- [Model Availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Azure OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-01-01-preview\",  # Use latest API version\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "print(f\"Azure OpenAI Configuration:\")\n",
    "print(f\"  Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"  API Version: 2025-01-01-preview\")\n",
    "\n",
    "# Note: Model name should match your deployment name in Azure\n",
    "azure_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # This should match your deployment name\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the benefits of using Azure OpenAI over direct OpenAI.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\n=== Azure OpenAI Response ===\")\n",
    "print(f\"Model: {azure_response.model}\")\n",
    "print(f\"Content: {azure_response.choices[0].message.content}\")\n",
    "print(f\"Usage: {azure_response.usage.prompt_tokens} + {azure_response.usage.completion_tokens} = {azure_response.usage.total_tokens} tokens\")\n",
    "\n",
    "# System fingerprint for version tracking\n",
    "if hasattr(azure_response, 'system_fingerprint'):\n",
    "    print(f\"System fingerprint: {azure_response.system_fingerprint}\")\n",
    "\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "azure_response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI Function Calling\n",
    "\n",
    "Function calling works identically on Azure OpenAI as with direct OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling works the same on Azure OpenAI\n",
    "azure_function_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # Your Azure deployment name\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Calculate 144 divided by 12\"}\n",
    "    ],\n",
    "    tools=[calculator_function],  # Same function definition as before\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"=== Azure OpenAI Function Call ===\")\n",
    "print(f\"Model: {azure_function_response.model}\")\n",
    "\n",
    "message = azure_function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        print(f\"\\nFunction call:\")\n",
    "        print(f\"  Function: {tool_call.function.name}\")\n",
    "        print(f\"  Arguments: {tool_call.function.arguments}\")\n",
    "        \n",
    "        # Parse and display the calculation\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        print(f\"  Operation: {args['a']} {args['operation']} {args['b']}\")\n",
    "\n",
    "print(f\"\\nâœ… Function calling works identically on Azure OpenAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Direct OpenAI vs Azure OpenAI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain machine learning in exactly 3 sentences.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Comparison: Direct OpenAI vs Azure OpenAI ===\")\n",
    "\n",
    "# Direct OpenAI\n",
    "direct_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=test_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Direct OpenAI:\")\n",
    "print(f\"Model: {direct_response.model}\")\n",
    "print(f\"Response: {direct_response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {direct_response.usage.prompt_tokens} + {direct_response.usage.completion_tokens}\")\n",
    "\n",
    "# Azure OpenAI\n",
    "azure_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # Your deployment name\n",
    "    messages=test_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ  Azure OpenAI:\")\n",
    "print(f\"Model: {azure_response.model}\")\n",
    "print(f\"Response: {azure_response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {azure_response.usage.prompt_tokens} + {azure_response.usage.completion_tokens}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Key differences: authentication, endpoints, and enterprise features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
