{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Provider Guide ðŸš€\n",
    "\n",
    "This notebook demonstrates how to use OpenAI's models through different hosting options:\n",
    "- **Direct OpenAI**: Using OpenAI's native API\n",
    "- **Azure OpenAI**: Using OpenAI models through Microsoft Azure\n",
    "\n",
    "## Overview\n",
    "\n",
    "OpenAI offers their models through multiple hosting platforms, each with distinct advantages:\n",
    "\n",
    "### Direct OpenAI Hosting\n",
    "- **Latest models** available immediately upon release\n",
    "- **Global availability** with worldwide access\n",
    "- **Simple authentication** with API key\n",
    "- **Full feature set** including all model capabilities\n",
    "- **Competitive pricing** with usage-based billing\n",
    "\n",
    "### Azure OpenAI Hosting\n",
    "- **Enterprise security** and compliance features\n",
    "- **Regional deployment** for data residency requirements\n",
    "- **VNet integration** for private network access\n",
    "- **Microsoft ecosystem** integration\n",
    "- **SLA guarantees** for production workloads\n",
    "- **Content filtering** and safety features\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load environment variables for authentication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import base64\n",
    "import httpx\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct OpenAI Hosting\n",
    "\n",
    "### Authentication\n",
    "The OpenAI client automatically uses the `OPENAI_API_KEY` environment variable for authentication.\n",
    "\n",
    "### Available Models (as of 2025)\n",
    "- `gpt-4.1` - Latest and most capable model\n",
    "- `gpt-4o` - Optimized for speed and efficiency\n",
    "- `gpt-4-turbo` - Previous generation flagship model\n",
    "- `gpt-3.5-turbo` - Fast and cost-effective option\n",
    "\n",
    "### Key Features\n",
    "- **Function calling** for tool integration\n",
    "- **Structured outputs** with JSON mode\n",
    "- **Vision capabilities** for image understanding\n",
    "- **Code generation** and analysis\n",
    "\n",
    "### Resources\n",
    "- [Python SDK Documentation](https://platform.openai.com/docs/libraries?language=python)\n",
    "- [API Reference](https://platform.openai.com/docs/api-reference)\n",
    "- [Model Documentation](https://platform.openai.com/docs/models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Message Example\n",
    "\n",
    "**Note**: OpenAI recently updated their API with a new `responses` endpoint that provides enhanced capabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Direct OpenAI Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Status: completed\n",
      "Content: In a blur of silver whiskers and ancient clockwork, Whiskers the cat leapt onto the Grandfather Clock, vanishing into 17th-century Paris with nothing but a purr and a stolen string of pearls from Cleopatraâ€™s nightstand.\n",
      "Role: assistant\n",
      "Usage: 22 input + 52 output = 74 total tokens\n",
      "\n",
      "=== Full Response Structure ===\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'resp_684ad5209c50819f90937cb3da27868206ad33c427ed9407',\n",
       " 'created_at': 1749734688.0,\n",
       " 'error': None,\n",
       " 'incomplete_details': None,\n",
       " 'instructions': None,\n",
       " 'metadata': {},\n",
       " 'model': 'gpt-4.1-2025-04-14',\n",
       " 'object': 'response',\n",
       " 'output': [{'id': 'msg_684ad5212724819fa48fdbad2203965e06ad33c427ed9407',\n",
       "   'content': [{'annotations': [],\n",
       "     'text': 'In a blur of silver whiskers and ancient clockwork, Whiskers the cat leapt onto the Grandfather Clock, vanishing into 17th-century Paris with nothing but a purr and a stolen string of pearls from Cleopatraâ€™s nightstand.',\n",
       "     'type': 'output_text'}],\n",
       "   'role': 'assistant',\n",
       "   'status': 'completed',\n",
       "   'type': 'message'}],\n",
       " 'parallel_tool_calls': True,\n",
       " 'temperature': 1.0,\n",
       " 'tool_choice': 'auto',\n",
       " 'tools': [],\n",
       " 'top_p': 1.0,\n",
       " 'background': False,\n",
       " 'max_output_tokens': None,\n",
       " 'previous_response_id': None,\n",
       " 'reasoning': {'effort': None, 'generate_summary': None, 'summary': None},\n",
       " 'service_tier': 'default',\n",
       " 'status': 'completed',\n",
       " 'text': {'format': {'type': 'text'}},\n",
       " 'truncation': 'disabled',\n",
       " 'usage': {'input_tokens': 22,\n",
       "  'input_tokens_details': {'cached_tokens': 0},\n",
       "  'output_tokens': 52,\n",
       "  'output_tokens_details': {'reasoning_tokens': 0},\n",
       "  'total_tokens': 74},\n",
       " 'user': None,\n",
       " 'store': True}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize client (uses OPENAI_API_KEY from environment)\n",
    "client = OpenAI()\n",
    "\n",
    "# Using the new responses API for enhanced capabilities\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=\"Write a creative one-sentence story about a time-traveling cat.\"\n",
    ")\n",
    "\n",
    "print(\"=== Direct OpenAI Response ===\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Status: {response.status}\")\n",
    "\n",
    "# Extract the message content\n",
    "if response.output and len(response.output) > 0:\n",
    "    message = response.output[0]\n",
    "    print(f\"Content: {message.content[0].text}\")\n",
    "    print(f\"Role: {message.role}\")\n",
    "\n",
    "print(f\"Usage: {response.usage.input_tokens} input + {response.usage.output_tokens} output = {response.usage.total_tokens} total tokens\")\n",
    "\n",
    "# Full response structure\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Completions API\n",
    "\n",
    "For traditional chat-style interactions, you can also use the chat completions endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chat Completions Response ===\n",
      "Model: gpt-4o-2024-08-06\n",
      "Content: The fog clung to the cobblestones like a secret, and as Inspector Hayes stepped into the alley, he couldn't shake the feeling that someone was watching from the shadows.\n",
      "Finish reason: stop\n",
      "Usage: 31 + 35 = 66 tokens\n"
     ]
    }
   ],
   "source": [
    "# Traditional chat completions API\n",
    "chat_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful AI assistant specializing in creative writing.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write an opening line for a mystery novel.\"}\n",
    "    ],\n",
    "    max_tokens=100,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"=== Chat Completions Response ===\")\n",
    "print(f\"Model: {chat_response.model}\")\n",
    "print(f\"Content: {chat_response.choices[0].message.content}\")\n",
    "print(f\"Finish reason: {chat_response.choices[0].finish_reason}\")\n",
    "print(f\"Usage: {chat_response.usage.prompt_tokens} + {chat_response.usage.completion_tokens} = {chat_response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling (Tool Use)\n",
    "\n",
    "OpenAI's function calling allows models to interact with external tools and APIs. Here's how to implement it:\n",
    "\n",
    "### Key Features:\n",
    "- **Parallel function calls**: Multiple tools can be called simultaneously\n",
    "- **Structured schemas**: Tools use JSON schemas for parameter validation\n",
    "- **Tool choice control**: Force specific tools or let the model choose\n",
    "- **Conversation continuity**: Results can be fed back for extended interactions\n",
    "\n",
    "\n",
    "### Force tool use\n",
    "By default the model will determine when and how many tools to use. You can force specific behavior with the tool_choice parameter.\n",
    "- **Auto**: (Default) Call zero, one, or multiple functions. tool_choice: \"auto\"\n",
    "- **Required**: Call one or more functions. tool_choice: \"required\"\n",
    "- **Forced Function**: Call exactly one specific function: ```tool_choice: {\"type\": \"function\", \"function\": {\"name\": \"get_weather\"}}```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Function Call Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Call Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: None\n",
      "\n",
      "Function calls: 1\n",
      "  Call 1:\n",
      "    ID: call_2zazoxDmKUbJBUvejWjRj5zD\n",
      "    Function: get_weather\n",
      "    Arguments: {\"location\":\"Paris, France\"}\n",
      "\n",
      "Messages so far: 2\n"
     ]
    }
   ],
   "source": [
    "# Define a weather function\n",
    "weather_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country e.g. New York, USA\"\n",
    "                },\n",
    "                \"units\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                    \"description\": \"Temperature units\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initial request with function\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the weather like in Paris today?\"}]\n",
    "\n",
    "function_response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=messages,\n",
    "    tools=[weather_function],\n",
    "    tool_choice={\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\"\n",
    "        }\n",
    "    },  # Force this specific function to be called\n",
    ")\n",
    "\n",
    "print(\"=== Function Call Response ===\")\n",
    "print(f\"Model: {function_response.model}\")\n",
    "\n",
    "message = function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nFunction calls: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls):\n",
    "        print(f\"  Call {i+1}:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Function: {tool_call.function.name}\")\n",
    "        print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "\n",
    "# Add assistant's response to conversation\n",
    "messages.append(message)\n",
    "\n",
    "print(f\"\\nMessages so far: {len(messages)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function Execution and Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Execution ===\n",
      "Function: get_weather\n",
      "Location: Paris, France\n",
      "Units: fahrenheit\n",
      "Result: 68Â°F, sunny with light clouds\n",
      "\n",
      "=== Final Response ===\n",
      "GPT's response: The weather in Paris today is 68Â°F, sunny with light clouds.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Simulate function execution\n",
    "def execute_weather_function(location, units=\"fahrenheit\"):\n",
    "    \"\"\"Simulate getting weather data\"\"\"\n",
    "    if \"paris\" in location.lower():\n",
    "        if units == \"celsius\":\n",
    "            return \"20Â°C, sunny with light clouds\"\n",
    "        else:\n",
    "            return \"68Â°F, sunny with light clouds\"\n",
    "    else:\n",
    "        return f\"Weather data not available for {location}\"\n",
    "\n",
    "# Execute function calls if any\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        if tool_call.function.name == \"get_weather\":\n",
    "            # Parse function arguments\n",
    "            args = json.loads(tool_call.function.arguments)\n",
    "            location = args.get(\"location\")\n",
    "            units = args.get(\"units\", \"fahrenheit\")\n",
    "            \n",
    "            # Execute the function\n",
    "            weather_result = execute_weather_function(location, units)\n",
    "            \n",
    "            print(f\"=== Function Execution ===\")\n",
    "            print(f\"Function: {tool_call.function.name}\")\n",
    "            print(f\"Location: {location}\")\n",
    "            print(f\"Units: {units}\")\n",
    "            print(f\"Result: {weather_result}\")\n",
    "            \n",
    "            # Add function result to conversation\n",
    "            messages.append({\n",
    "                \"role\": \"tool\",\n",
    "                \"content\": weather_result,\n",
    "                \"tool_call_id\": tool_call.id\n",
    "            })\n",
    "    \n",
    "    # Get final response with function results\n",
    "    final_response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",\n",
    "        messages=messages,\n",
    "        tools=[weather_function]\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== Final Response ===\")\n",
    "    print(f\"GPT's response: {final_response.choices[0].message.content}\")\n",
    "else:\n",
    "    print(\"No function calls found in response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Functions Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Multiple Functions Response ===\n",
      "Model: gpt-4.1-2025-04-14\n",
      "Content: None\n",
      "\n",
      "Function calls: 2\n",
      "  Call 1:\n",
      "    ID: call_1lvUuzkWelutuBicfBFVFuQV\n",
      "    Function: calculator\n",
      "    Arguments: {\"operation\": \"multiply\", \"a\": 25, \"b\": 16}\n",
      "  Call 2:\n",
      "    ID: call_CAbefRMUYdArvImLDDQYzHZ8\n",
      "    Function: web_search\n",
      "    Arguments: {\"query\": \"OpenAI GPT-4 capabilities\"}\n",
      "\n",
      "Usage: 180 + 57 = 237 tokens\n"
     ]
    }
   ],
   "source": [
    "# Define multiple functions\n",
    "calculator_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"calculator\",\n",
    "        \"description\": \"Perform basic mathematical operations\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"operation\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                    \"description\": \"The mathematical operation\"\n",
    "                },\n",
    "                \"a\": {\"type\": \"number\", \"description\": \"First number\"},\n",
    "                \"b\": {\"type\": \"number\", \"description\": \"Second number\"}\n",
    "            },\n",
    "            \"required\": [\"operation\", \"a\", \"b\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_function = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"Search the web for information\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query\"\n",
    "                },\n",
    "                \"max_results\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"description\": \"Maximum number of results\",\n",
    "                    \"default\": 5\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Request that might use multiple functions\n",
    "multi_function_response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Calculate 25 * 16, and also search for 'OpenAI GPT-4 capabilities'\"\n",
    "        }\n",
    "    ],\n",
    "    tools=[weather_function, calculator_function, search_function],\n",
    "    parallel_tool_calls=True  # Enable parallel function calls\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Functions Response ===\")\n",
    "print(f\"Model: {multi_function_response.model}\")\n",
    "\n",
    "message = multi_function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    print(f\"\\nFunction calls: {len(message.tool_calls)}\")\n",
    "    for i, tool_call in enumerate(message.tool_calls):\n",
    "        print(f\"  Call {i+1}:\")\n",
    "        print(f\"    ID: {tool_call.id}\")\n",
    "        print(f\"    Function: {tool_call.function.name}\")\n",
    "        print(f\"    Arguments: {tool_call.function.arguments}\")\n",
    "        \n",
    "print(f\"\\nUsage: {multi_function_response.usage.prompt_tokens} + {multi_function_response.usage.completion_tokens} = {multi_function_response.usage.total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images and PDF-files\n",
    "\n",
    "### Images\n",
    "First let's try sending an image using a base64 encoded string (local file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a close-up photograph of an ant. The image appears to show the ant in a defensive or alert posture, with its body raised and antennae extended. The photo focuses on the details of the antâ€™s body, legs, and antennae, giving a clear view of its segments and tiny hairs. The background is blurred, drawing attention to the ant itself.\n"
     ]
    }
   ],
   "source": [
    "# For base64-encoded images\n",
    "image1_url = \"https://upload.wikimedia.org/wikipedia/commons/a/a7/Camponotus_flavomarginatus_ant.jpg\"\n",
    "image1_media_type = \"image/jpeg\"\n",
    "image1_data = base64.standard_b64encode(httpx.get(image1_url).content).decode(\"utf-8\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{image1_data}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also just pass a URL if it's a public image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image shows a wooden pathway or boardwalk stretching through a field of tall green grass. There are bushes and trees in the distance. The sky is mostly clear with light, wispy clouds and a bright blue color, suggesting it is either morning or late afternoon. The overall scene appears to be peaceful and natural.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    input=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": \"what's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"input_image\",\n",
    "                \"image_url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "            },\n",
    "        ],\n",
    "    }],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDF-Files\n",
    "Use a base64 encoded PDF-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This document is an addendum to the Claude 3 model card, detailing the capabilities, performance, and safety evaluations of two new AI models: the upgraded Claude 3.5 Sonnet and the newly released Claude 3.5 Haiku. It discusses their advancements in reasoning, coding, visual processing, computer use from screenshots, and the rigorous safety measures implemented during their development and testing.\n"
     ]
    }
   ],
   "source": [
    "# First, load and encode the PDF \n",
    "pdf_url = \"https://assets.anthropic.com/m/1cd9d098ac3e6467/original/Claude-3-Model-Card-October-Addendum.pdf\"\n",
    "pdf_data = base64.standard_b64encode(httpx.get(pdf_url).content).decode(\"utf-8\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"filename\": \"Claude-3-Model-Card-October-Addendum.pdf\",\n",
    "                    \"file_data\": f\"data:application/pdf;base64,{pdf_data}\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Explain what this document is about in 2 sentences. \",\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly pass a PDF-file to OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'file-F73zJnGLmkuRTjQV3Eyrea',\n",
       " 'bytes': 1042289,\n",
       " 'created_at': 1749648775,\n",
       " 'filename': 'my_custom_name.pdf',\n",
       " 'object': 'file',\n",
       " 'purpose': 'user_data',\n",
       " 'status': 'processed',\n",
       " 'expires_at': None,\n",
       " 'status_details': None}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=(\"my_custom_name.pdf\", open(\"./assets/gameboy_color.pdf\", \"rb\")),\n",
    "    purpose=\"user_data\"\n",
    ")\n",
    "\n",
    "file.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The filename of this PDF is not visible in the images you provided. The images show scanned pages from a Game Boy Color instruction booklet, but they do not display or mention the actual filename of the PDF. If you downloaded this from Manualslib.com, you can check the downloaded file on your device or view the filename in your browser's downloads section.\n"
     ]
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_file\",\n",
    "                    \"file_id\": file.id,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"What's the filename of this pdf?\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Documents (.docx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_684ad567f8708192839de1b96a897e250b8bc6211e66ab55', created_at=1749734759.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4.1-2025-04-14', object='response', output=[ResponseOutputMessage(id='msg_684ad568563081928aad3d81e3823cbc0b8bc6211e66ab55', content=[ResponseOutputText(annotations=[], text='This file is a purchase order from Redline Auto Center to a vendor for automotive parts, specifying items, quantities, unit prices, discounts, taxes, additional costs, and a total amount due of $2,694.30, with payment terms of 30 days upon receipt.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=2589, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=57, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=2646), user=None, store=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mammoth\n",
    "\n",
    "# Convert with default options\n",
    "with open(\"./assets/order.docx\", \"rb\") as docx_file:\n",
    "    result = mammoth.convert_to_html(docx_file)\n",
    "    html = result.value  # The HTML\n",
    "    messages = result.messages  # Any warnings\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": html,\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"input_text\",\n",
    "                    \"text\": \"Summerize this file in one sentence\",\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure OpenAI\n",
    "\n",
    "### Setup and Authentication\n",
    "Azure OpenAI requires specific configuration including endpoint, API version, and deployment names.\n",
    "\n",
    "### Key Differences:\n",
    "- **Custom endpoint**: Your Azure OpenAI resource endpoint\n",
    "- **API version**: Specific API version for compatibility\n",
    "- **Deployment names**: Models are deployed with custom names\n",
    "- **Regional availability**: Not all models available in all regions\n",
    "- **Content filtering**: Additional safety and content filtering layers\n",
    "\n",
    "### Available Regions:\n",
    "- `eastus2` - United States East (primary for US models)\n",
    "- `swedencentral` - Sweden Central (primary for EU models)\n",
    "- `australiaeast` - Australia East\n",
    "- See [Azure OpenAI Regions](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models) for complete list\n",
    "\n",
    "### Important Notes:\n",
    "- **Model deployment required**: You must deploy models through Azure AI Studio\n",
    "- **Custom names**: Deployment names can be different from model names\n",
    "- **Quota management**: Each region has specific quotas and limits\n",
    "\n",
    "### Resources:\n",
    "- [Azure OpenAI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/openai/)\n",
    "- [Azure AI Studio](https://ai.azure.com/)\n",
    "- [Model Availability](https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#model-summary-table-and-region-availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Azure OpenAI Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Initialize Azure OpenAI client\n",
    "azure_client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=\"2025-01-01-preview\",  # Use latest API version\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "print(f\"Azure OpenAI Configuration:\")\n",
    "print(f\"  Endpoint: {os.getenv('AZURE_OPENAI_ENDPOINT')}\")\n",
    "print(f\"  API Version: 2025-01-01-preview\")\n",
    "\n",
    "# Note: Model name should match your deployment name in Azure\n",
    "azure_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # This should match your deployment name\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the benefits of using Azure OpenAI over direct OpenAI.\",\n",
    "        },\n",
    "    ],\n",
    "    max_tokens=200,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"\\n=== Azure OpenAI Response ===\")\n",
    "print(f\"Model: {azure_response.model}\")\n",
    "print(f\"Content: {azure_response.choices[0].message.content}\")\n",
    "print(f\"Usage: {azure_response.usage.prompt_tokens} + {azure_response.usage.completion_tokens} = {azure_response.usage.total_tokens} tokens\")\n",
    "\n",
    "# System fingerprint for version tracking\n",
    "if hasattr(azure_response, 'system_fingerprint'):\n",
    "    print(f\"System fingerprint: {azure_response.system_fingerprint}\")\n",
    "\n",
    "print(\"\\n=== Full Response Structure ===\")\n",
    "azure_response.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI Function Calling\n",
    "\n",
    "Function calling works identically on Azure OpenAI as with direct OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function calling works the same on Azure OpenAI\n",
    "azure_function_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # Your Azure deployment name\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Calculate 144 divided by 12\"}\n",
    "    ],\n",
    "    tools=[calculator_function],  # Same function definition as before\n",
    "    tool_choice=\"auto\"\n",
    ")\n",
    "\n",
    "print(\"=== Azure OpenAI Function Call ===\")\n",
    "print(f\"Model: {azure_function_response.model}\")\n",
    "\n",
    "message = azure_function_response.choices[0].message\n",
    "print(f\"Content: {message.content}\")\n",
    "\n",
    "if message.tool_calls:\n",
    "    for tool_call in message.tool_calls:\n",
    "        print(f\"\\nFunction call:\")\n",
    "        print(f\"  Function: {tool_call.function.name}\")\n",
    "        print(f\"  Arguments: {tool_call.function.arguments}\")\n",
    "        \n",
    "        # Parse and display the calculation\n",
    "        args = json.loads(tool_call.function.arguments)\n",
    "        print(f\"  Operation: {args['a']} {args['operation']} {args['b']}\")\n",
    "\n",
    "print(f\"\\nâœ… Function calling works identically on Azure OpenAI!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Direct OpenAI vs Azure OpenAI\n",
    "\n",
    "Let's compare the same request across both hosting options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Explain machine learning in exactly 3 sentences.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== Comparison: Direct OpenAI vs Azure OpenAI ===\")\n",
    "\n",
    "# Direct OpenAI\n",
    "direct_response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=test_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\nðŸ”µ Direct OpenAI:\")\n",
    "print(f\"Model: {direct_response.model}\")\n",
    "print(f\"Response: {direct_response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {direct_response.usage.prompt_tokens} + {direct_response.usage.completion_tokens}\")\n",
    "\n",
    "# Azure OpenAI\n",
    "azure_response = azure_client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # Your deployment name\n",
    "    messages=test_messages,\n",
    "    max_tokens=150\n",
    ")\n",
    "\n",
    "print(\"\\nðŸŸ  Azure OpenAI:\")\n",
    "print(f\"Model: {azure_response.model}\")\n",
    "print(f\"Response: {azure_response.choices[0].message.content}\")\n",
    "print(f\"Tokens: {azure_response.usage.prompt_tokens} + {azure_response.usage.completion_tokens}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Both use identical APIs and response formats!\")\n",
    "print(\"ðŸ’¡ Key differences: authentication, endpoints, and enterprise features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
