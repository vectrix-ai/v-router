{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LangSmith Tracing with v-router\n\nThis notebook demonstrates how to enable LangSmith tracing for multiple providers in v-router.\n\n## Prerequisites\n\n1. Install v-router with tracing support:\n```bash\npip install 'v-router[tracing]'\n```\n\n2. Set up your LangSmith account and get your API key from [LangSmith](https://smith.langchain.com/)",
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")  # Your LangSmith API key\n",
    "\n",
    "# Optional: Set a project name for better organization in LangSmith\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"v-router-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Basic Usage with Tracing - OpenAI Example",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from v_router import Client, LLM\n\n# Create a client with OpenAI provider\nclient = Client(\n    llm_config=LLM(\n        model_name=\"gpt-4o\",\n        provider=\"openai\",\n        max_tokens=1000,\n        temperature=0.7\n    )\n)\n\n# Make a simple request - this will be traced in LangSmith\nresponse = await client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n    ]\n)\n\nprint(response.content[0].text)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Using Traceable Decorator with v-router\n\nYou can also use LangSmith's `@traceable` decorator to trace your own functions that use v-router:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "from langsmith import traceable\n\n@traceable(name=\"Question Answering Pipeline\")\nasync def answer_question(question: str) -> str:\n    \"\"\"Answer a question using v-router with OpenAI.\"\"\"\n    \n    # This inner function call will also be traced\n    response = await client.messages.create(\n        messages=[\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Answer concisely.\"},\n            {\"role\": \"user\", \"content\": question}\n        ]\n    )\n    \n    return response.content[0].text\n\n# Call the traced function\nanswer = await answer_question(\"What are the main ingredients in a margherita pizza?\")\nprint(answer)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Example with Tool Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from v_router.classes.tools import Tool, Tools\n",
    "\n",
    "@traceable(run_type=\"tool\", name=\"Weather Lookup\")\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Mock weather function for demonstration.\"\"\"\n",
    "    return f\"The weather in {location} is sunny and 72Â°F.\"\n",
    "\n",
    "@traceable(name=\"Weather Assistant\")\n",
    "async def weather_assistant(user_query: str):\n",
    "    \"\"\"Process weather queries with tool use.\"\"\"\n",
    "    \n",
    "    # Define the weather tool\n",
    "    weather_tool = Tool(\n",
    "        name=\"get_weather\",\n",
    "        description=\"Get the current weather for a location\",\n",
    "        input_schema={\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tools = Tools(tools=[weather_tool])\n",
    "    \n",
    "    # First call to determine if we need to use a tool\n",
    "    response = await client.messages.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": user_query}\n",
    "        ],\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Check if the model wants to use a tool\n",
    "    if response.tool_use:\n",
    "        tool_results = []\n",
    "        for tool_use in response.tool_use:\n",
    "            if tool_use.name == \"get_weather\":\n",
    "                # Execute the tool (this is also traced)\n",
    "                result = get_weather(**tool_use.arguments)\n",
    "                tool_results.append({\n",
    "                    \"tool_use_id\": tool_use.id,\n",
    "                    \"content\": result\n",
    "                })\n",
    "        \n",
    "        # Send tool results back to the model\n",
    "        final_response = await client.messages.create(\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": user_query},\n",
    "                {\"role\": \"assistant\", \"content\": response.content},\n",
    "                {\"role\": \"user\", \"content\": tool_results}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return final_response.content[0].text\n",
    "    else:\n",
    "        return response.content[0].text\n",
    "\n",
    "# Run the weather assistant\n",
    "result = await weather_assistant(\"What's the weather like in San Francisco?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Traces in LangSmith\n",
    "\n",
    "After running the above examples, you can view your traces in the LangSmith UI:\n",
    "\n",
    "1. Go to [LangSmith](https://smith.langchain.com/)\n",
    "2. Navigate to your project (default is \"default\" unless you set LANGCHAIN_PROJECT)\n",
    "3. You'll see all your traced calls with:\n",
    "   - Input/output data\n",
    "   - Latency metrics\n",
    "   - Token usage\n",
    "   - Error tracking\n",
    "   - Nested trace structure\n",
    "\n",
    "## Disabling Tracing\n",
    "\n",
    "To disable tracing, simply set the environment variable to false or remove it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"false\"\n",
    "\n",
    "# Or remove the environment variable entirely\n",
    "# del os.environ[\"LANGCHAIN_TRACING_V2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Example with Multiple Providers\n\nLangSmith tracing works with all supported providers:",
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Re-enable tracing\nos.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n\n# Example with Azure OpenAI\nazure_client = Client(\n    llm_config=LLM(\n        model_name=\"gpt-4o\",  # This should be your deployment name in Azure\n        provider=\"azure\",\n        max_tokens=1000\n    )\n)\n\n# This will also be traced\nresponse = await azure_client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Write a haiku about cloud computing.\"}\n    ]\n)\n\nprint(response.content[0].text)\n\n# Example with Anthropic (also traced)\nanthropic_client = Client(\n    llm_config=LLM(\n        model_name=\"claude-3-5-sonnet-20241022\",\n        provider=\"anthropic\",\n        max_tokens=1000\n    )\n)\n\nresponse = await anthropic_client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence.\"}\n    ]\n)\n\nprint(response.content[0].text)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}