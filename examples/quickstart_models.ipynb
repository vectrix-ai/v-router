{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Models and Providers\n",
    "\n",
    "This notebook demonstrates how to use v-router for basic model interactions, fallback strategies, and cross-provider switching.\n",
    "\n",
    "## What is v-router?\n",
    "\n",
    "v-router is a unified LLM interface that provides:\n",
    "- **Automatic fallback** between different models and providers\n",
    "- **Unified response format** across all providers (Anthropic, OpenAI, Google, Azure)\n",
    "- **Seamless provider switching** with the same API\n",
    "- **Intelligent routing** based on model availability and configuration\n",
    "\n",
    "## Core Components\n",
    "\n",
    "### Request Models\n",
    "- **`LLM`**: Configuration for a language model including provider, model name, and parameters\n",
    "- **`BackupModel`**: Fallback model configuration with priority ordering\n",
    "- **`Client`**: Main interface for sending requests to models\n",
    "\n",
    "### Response Models  \n",
    "- **`Response`**: Unified response format with content, usage, model info, and raw provider response\n",
    "- **`Content`**: Text content blocks from the model response\n",
    "- **`Usage`**: Token usage information (input/output tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Example\n",
    "\n",
    "Let's start with a simple example to see how easy it is to use different providers with the same interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 14:46:12,866 - v_router.router - INFO - Trying primary model: claude-sonnet-4 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello, it's nice to meet you!\n",
      "Model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n"
     ]
    }
   ],
   "source": [
    "from v_router import Client, LLM, BackupModel\n",
    "\n",
    "# Create an LLM configuration\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4\",\n",
    "    provider=\"anthropic\",\n",
    "    max_tokens=100,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Create a client with the LLM configuration\n",
    "client = Client(llm_config)\n",
    "\n",
    "# Send a message using the unified API\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Access the unified response format\n",
    "print(f\"Response: {response.content[0].text}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Response Format\n",
    "\n",
    "v-router provides a unified response format across all providers. Let's examine the response structure in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 14:46:14,336 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Unified Response Structure:\n",
      "‚îú‚îÄ‚îÄ response.content: list of 1 items\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ content[0].type: 'text'\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ content[0].role: 'assistant'\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ content[0].text: 'Python is a high-level, interpreted programming language known for its simple, readable syntax and versatility across applications like web development, data science, artificial intelligence, and automation.'\n",
      "‚îú‚îÄ‚îÄ response.tool_use: list of 0 items\n",
      "‚îú‚îÄ‚îÄ response.usage:\n",
      "‚îÇ   ‚îú‚îÄ‚îÄ input_tokens: 16\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ output_tokens: 38\n",
      "‚îú‚îÄ‚îÄ response.model: 'claude-sonnet-4-20250514'\n",
      "‚îú‚îÄ‚îÄ response.provider: 'anthropic'\n",
      "‚îî‚îÄ‚îÄ response.raw_response: dict\n",
      "\n",
      "‚úÖ This same structure works for ALL providers!\n"
     ]
    }
   ],
   "source": [
    "# Let's examine the unified Response structure\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is Python? Answer in one sentence.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# The unified Response structure provides:\n",
    "print(\"üîç Unified Response Structure:\")\n",
    "print(f\"‚îú‚îÄ‚îÄ response.content: {type(response.content).__name__} of {len(response.content)} items\")\n",
    "print(f\"‚îÇ   ‚îî‚îÄ‚îÄ content[0].type: '{response.content[0].type}'\")\n",
    "print(f\"‚îÇ   ‚îî‚îÄ‚îÄ content[0].role: '{response.content[0].role}'\")\n",
    "print(f\"‚îÇ   ‚îî‚îÄ‚îÄ content[0].text: '{response.content[0].text}'\")\n",
    "print(f\"‚îú‚îÄ‚îÄ response.tool_use: {type(response.tool_use).__name__} of {len(response.tool_use)} items\")\n",
    "print(f\"‚îú‚îÄ‚îÄ response.usage:\")\n",
    "print(f\"‚îÇ   ‚îú‚îÄ‚îÄ input_tokens: {response.usage.input_tokens}\")\n",
    "print(f\"‚îÇ   ‚îî‚îÄ‚îÄ output_tokens: {response.usage.output_tokens}\")\n",
    "print(f\"‚îú‚îÄ‚îÄ response.model: '{response.model}'\")\n",
    "print(f\"‚îú‚îÄ‚îÄ response.provider: '{response.provider}'\")\n",
    "print(f\"‚îî‚îÄ‚îÄ response.raw_response: {type(response.raw_response).__name__}\")\n",
    "\n",
    "print(\"\\n‚úÖ This same structure works for ALL providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback Example\n",
    "\n",
    "One of v-router's key features is automatic fallback. If the primary model fails, it will try backup models in priority order.\n",
    "\n",
    "### How Fallback Works:\n",
    "1. **Primary Model**: Attempts the main model first\n",
    "2. **Backup Models**: If primary fails, tries backup models by priority (1, 2, 3...)\n",
    "3. **Tool Inheritance**: Backup models automatically inherit tools from the primary model\n",
    "4. **Same Interface**: No changes needed in your code - v-router handles it transparently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 14:46:16,594 - v_router.router - INFO - Trying primary model: claude-6 on anthropic\u001b[0m\n",
      "\u001b[33m2025-05-30 14:46:16,975 - v_router.router - WARNING - Primary model failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-6'}}\u001b[0m\n",
      "\u001b[32m2025-05-30 14:46:16,976 - v_router.router - INFO - Trying backup model: gpt-4o on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 2 + 2 equals 4.\n",
      "Model: gpt-4o-2024-08-06\n",
      "Provider: openai\n",
      "\n",
      "üí° Notice: The fallback model was used seamlessly!\n"
     ]
    }
   ],
   "source": [
    "# Configure fallback models with different providers\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-6\",  # Primary model (intentionally non-existent)\n",
    "    provider=\"anthropic\",\n",
    "    max_tokens=100,\n",
    "    backup_models=[\n",
    "        BackupModel(\n",
    "            model=LLM(\n",
    "                model_name=\"gpt-4o\",\n",
    "                provider=\"openai\"\n",
    "            ),\n",
    "            priority=1  # First fallback\n",
    "        ),\n",
    "        BackupModel(\n",
    "            model=LLM(\n",
    "                model_name=\"gemini-1.5-pro\",\n",
    "                provider=\"google\"\n",
    "            ),\n",
    "            priority=2  # Second fallback\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "# This will try claude-6 first (fail), then gpt-4o, then gemini-1.5-pro if needed\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What's 2+2?\"}\n",
    "    ]\n",
    ")\n",
    "    \n",
    "print(f\"Response: {response.content[0].text}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"\\nüí° Notice: The fallback model was used seamlessly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Provider Switch\n",
    "\n",
    "You can enable cross-provider fallback by setting `try_other_providers=True`. If a call fails on one provider, the system will try another provider with the same model.\n",
    "\n",
    "### How Cross-Provider Switching Works:\n",
    "1. **Primary Provider**: Tries the specified provider first\n",
    "2. **Model Mapping**: Uses models.yml to find the same model on other providers\n",
    "3. **Automatic Retry**: Seamlessly switches to alternative providers\n",
    "4. **Provider-Specific Formatting**: Handles different API formats automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 15:02:12,859 - v_router.router - INFO - Trying primary model: claude-opus-4 on vertexai\u001b[0m\n",
      "\u001b[33m2025-05-30 15:02:15,058 - v_router.router - WARNING - Primary model failed: Error code: 429 - {'error': {'code': 429, 'message': 'Quota exceeded for aiplatform.googleapis.com/online_prediction_input_tokens_per_minute_per_base_model with base model: anthropic-claude-opus-4. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.', 'status': 'RESOURCE_EXHAUSTED'}}\u001b[0m\n",
      "\u001b[32m2025-05-30 15:02:15,060 - v_router.router - INFO - Trying alternative provider: claude-opus-4 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "Model: claude-opus-4-20250514\n",
      "Provider: anthropic\n",
      "\n",
      "üí° If Vertex AI failed, it automatically tried Anthropic!\n"
     ]
    }
   ],
   "source": [
    "llm_config = LLM(\n",
    "    model_name=\"claude-opus-4\",\n",
    "    provider=\"vertexai\",  # Try Vertex AI first (may not be configured)\n",
    "    max_tokens=100,\n",
    "    try_other_providers=True  # Enable cross-provider fallback\n",
    ")\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\":\"You are a friendly assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a short joke.\"}\n",
    "    ]\n",
    ")\n",
    "    \n",
    "print(f\"Response: {response.content[0].text}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"\\nüí° If Vertex AI failed, it automatically tried Anthropic!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Different Providers\n",
    "\n",
    "Let's test the same request across different providers to show the unified interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 14:46:22,558 - v_router.router - INFO - Trying primary model: claude-sonnet-4 on anthropic\u001b[0m\n",
      "\u001b[32m2025-05-30 14:46:24,481 - v_router.router - INFO - Trying primary model: gpt-4 on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Anthropic Claude ===\n",
      "Model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n",
      "Response: Machine learning is a method of teaching computers to recognize patterns and make predictions from data without being explicitly programmed for each specific task.\n",
      "Tokens: 15 in, 29 out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 14:46:26,207 - v_router.router - INFO - Trying primary model: gemini-1.5-pro on google\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== OpenAI GPT ===\n",
      "Model: gpt-4-0613\n",
      "Provider: openai\n",
      "Response: Machine learning is a subset of artificial intelligence that enables computers to learn and make decisions from data without being explicitly programmed.\n",
      "Tokens: 15 in, 23 out\n",
      "\n",
      "=== Google Gemini ===\n",
      "Model: gemini-1.5-pro\n",
      "Provider: google\n",
      "Response: Machine learning is the process of enabling computers to learn from data without explicit programming.\n",
      "\n",
      "Tokens: 7 in, 17 out\n",
      "\n",
      "‚úÖ Notice: Same API, same response format, different providers!\n"
     ]
    }
   ],
   "source": [
    "# Test message for all providers\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain machine learning in one sentence.\"}\n",
    "]\n",
    "\n",
    "# Anthropic Claude\n",
    "anthropic_llm = LLM(\n",
    "    model_name=\"claude-sonnet-4\",\n",
    "    provider=\"anthropic\",\n",
    "    max_tokens=100\n",
    ")\n",
    "anthropic_client = Client(anthropic_llm)\n",
    "anthropic_response = await anthropic_client.messages.create(messages=test_messages)\n",
    "\n",
    "print(\"=== Anthropic Claude ===\")\n",
    "print(f\"Model: {anthropic_response.model}\")\n",
    "print(f\"Provider: {anthropic_response.provider}\")\n",
    "print(f\"Response: {anthropic_response.content[0].text}\")\n",
    "print(f\"Tokens: {anthropic_response.usage.input_tokens} in, {anthropic_response.usage.output_tokens} out\")\n",
    "\n",
    "# OpenAI GPT\n",
    "openai_llm = LLM(\n",
    "    model_name=\"gpt-4\",\n",
    "    provider=\"openai\",\n",
    "    max_tokens=100\n",
    ")\n",
    "openai_client = Client(openai_llm)\n",
    "openai_response = await openai_client.messages.create(messages=test_messages)\n",
    "\n",
    "print(\"\\n=== OpenAI GPT ===\")\n",
    "print(f\"Model: {openai_response.model}\")\n",
    "print(f\"Provider: {openai_response.provider}\")\n",
    "print(f\"Response: {openai_response.content[0].text}\")\n",
    "print(f\"Tokens: {openai_response.usage.input_tokens} in, {openai_response.usage.output_tokens} out\")\n",
    "\n",
    "# Google Gemini\n",
    "google_llm = LLM(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    provider=\"google\",\n",
    "    max_tokens=100\n",
    ")\n",
    "google_client = Client(google_llm)\n",
    "google_response = await google_client.messages.create(messages=test_messages)\n",
    "\n",
    "print(\"\\n=== Google Gemini ===\")\n",
    "print(f\"Model: {google_response.model}\")\n",
    "print(f\"Provider: {google_response.provider}\")\n",
    "print(f\"Response: {google_response.content[0].text}\")\n",
    "print(f\"Tokens: {google_response.usage.input_tokens} in, {google_response.usage.output_tokens} out\")\n",
    "\n",
    "print(\"\\n‚úÖ Notice: Same API, same response format, different providers!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "You can configure various parameters for fine-tuned control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-30 14:46:26,801 - v_router.router - INFO - Trying primary model: claude-sonnet-4 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creative Response: The day humanity received its eviction notice from Earth, it arrived not as a dramatic alien invasion, but as a polite holographic memo that materialized in every living room at exactly 3:47 PM, universal time.\n",
      "Model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n",
      "Usage: 26 + 50 = 76 tokens\n"
     ]
    }
   ],
   "source": [
    "# Advanced LLM configuration\n",
    "advanced_llm = LLM(\n",
    "    model_name=\"claude-sonnet-4\",\n",
    "    provider=\"anthropic\",\n",
    "    max_tokens=200,\n",
    "    temperature=0.7,  # More creative responses\n",
    "    top_p=0.9,        # Nucleus sampling\n",
    "    try_other_providers=True,\n",
    "    backup_models=[\n",
    "        BackupModel(\n",
    "            model=LLM(\n",
    "                model_name=\"gpt-4o\",\n",
    "                provider=\"openai\",\n",
    "                temperature=0.7  # Same temperature for consistency\n",
    "            ),\n",
    "            priority=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "client = Client(advanced_llm)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a creative writing assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a creative opening line for a sci-fi story.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Creative Response: {response.content[0].text}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Usage: {response.usage.input_tokens} + {response.usage.output_tokens} = {response.usage.input_tokens + response.usage.output_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "\n",
    "‚úÖ **Unified Interface**: Same API works across Anthropic, OpenAI, Google, and Azure  \n",
    "‚úÖ **Automatic Fallback**: Seamless switching between models when primary fails  \n",
    "‚úÖ **Cross-Provider Support**: Try the same model on different providers automatically  \n",
    "‚úÖ **Unified Response Format**: Consistent response structure regardless of provider  \n",
    "‚úÖ **Flexible Configuration**: Control temperature, tokens, and other parameters  \n",
    "\n",
    "### Request Models:\n",
    "- **`LLM`**: Primary configuration (model, provider, parameters)\n",
    "- **`BackupModel`**: Fallback configuration with priority\n",
    "- **`Client`**: Main interface for sending requests\n",
    "\n",
    "### Response Models:\n",
    "- **`Response`**: Unified response with content, usage, model info\n",
    "- **`Content`**: Text content blocks from the model\n",
    "- **`Usage`**: Token usage information\n",
    "\n",
    "### Next Steps:\n",
    "- Check out `quickstart_tool_calling.ipynb` to learn about function calling across providers\n",
    "- Explore the `models.yml` configuration for advanced model mapping\n",
    "- See the full documentation for more advanced features\n",
    "\n",
    "v-router provides a truly unified interface for working with LLMs across all major providers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
