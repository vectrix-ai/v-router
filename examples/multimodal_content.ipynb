{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook uses `await` syntax. Run it in a Jupyter environment with IPython kernel, or use `asyncio.run()` wrapper for regular Python scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Content with v-router üñºÔ∏è\n",
    "\n",
    "This notebook demonstrates how to use v-router with multimodal content (images and PDFs). The v-router library provides a unified interface for sending images and documents to different LLM providers.\n",
    "\n",
    "## Overview\n",
    "\n",
    "v-router supports:\n",
    "- **Images**: JPEG, PNG, GIF, WebP formats\n",
    "- **Documents**: PDF files (provider support varies)\n",
    "- **Automatic conversion**: File paths are automatically converted to base64\n",
    "- **Unified interface**: Same API across all providers\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Using Local Files\n",
    "\n",
    "The easiest way to send images and PDFs is by passing file paths directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:14,113 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Summary:\n",
      "This is an instruction manual/booklet for the Nintendo Game Boy Color handheld video game system. The document covers various aspects of the device including:\n",
      "\n",
      "1. Introduction and basic features\n",
      "2. Component descriptions and diagrams\n",
      "3. Battery installation instructions\n",
      "4. Usage instructions and setup\n",
      "5. Information about compatible Game Paks (game cartridges)\n",
      "6. Two-player gaming setup using Game Link cables\n",
      "7. Troubleshooting guide\n",
      "8. Warranty information\n",
      "9. Parts list and order form\n",
      "\n",
      "The manual provides detailed technical specifications, safety warnings, and operating instructions for users of the Game Boy Color, which was a color screen upgrade to Nintendo's original Game Boy handheld gaming system. It includes illustrations showing the device's components and proper setup procedures, as well as information about battery usage, game compatibility, and how to contact Nintendo for support.\n"
     ]
    }
   ],
   "source": [
    "# Example using the PDF file included in this repository\n",
    "from v_router import Client, LLM\n",
    "\n",
    "# Create a client\n",
    "client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send a PDF file by just passing its path\n",
    "pdf_path = \"providers/assets/gameboy_color.pdf\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": pdf_path  # v-router automatically detects and converts the PDF\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is this document about? Give me a brief summary.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=200)\n",
    "print(\"PDF Summary:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import httpx\n",
    "from pathlib import Path\n",
    "\n",
    "# Import content types if you need to create multimodal messages manually\n",
    "from v_router.classes.message import TextContent, ImageContent, DocumentContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Images\n",
    "\n",
    "### Method 1: Base64 Encoded Images\n",
    "\n",
    "You can send images by providing base64-encoded data directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:20,849 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Response:\n",
      "This image shows a carpenter ant (genus Camponotus) in striking detail. The ant is captured in a dynamic pose, appearing to be rearing up on its hind legs, which is a common defensive or aggressive posture. The ant's body shows the classic characteristics of carpenter ants: a segmented body with a pronounced thorax, a large head, and long, jointed legs. The ant's antennae are clearly visible, extending forward from its head. The image has a shallow depth of field, creating a beautiful bokeh effect in the background while keeping the ant in sharp focus. The lighting gives the ant's exoskeleton a subtle sheen, and you can make out fine details of its body structure. The overall color palette is warm, with browns and reddish tones dominating the composition.\n"
     ]
    }
   ],
   "source": [
    "# Download a sample image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/320px-Camponotus_flavomarginatus_ant.jpg\"\n",
    "response = httpx.get(image_url)\n",
    "image_data = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "\n",
    "# Create a client with Anthropic\n",
    "client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send multimodal message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What animal is in this image? Describe it in detail.\"),\n",
    "            ImageContent(data=image_data, media_type=\"image/jpeg\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=200)\n",
    "print(\"Anthropic Response:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: File Path (Automatic Conversion)\n",
    "\n",
    "v-router can automatically convert local image files to base64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:25,324 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from file path:\n",
      "This is a detailed macro photograph of an ant, showing its distinctive features like its segmented body, long antennae, and slender legs. The ant appears to be in a rearing or defensive posture, with its front legs raised off the ground. The image has a shallow depth of field, creating a soft, blurred background while keeping the ant in sharp focus. The lighting gives the photograph a warm, brownish tone, and the detail captured allows you to see the ant's\n"
     ]
    }
   ],
   "source": [
    "# Save the image locally\n",
    "image_path = Path(\"/tmp/test_ant.jpg\")\n",
    "# Use the httpx response from earlier, not the v-router response\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(httpx.get(image_url).content)\n",
    "\n",
    "# Send using file path - v-router will automatically convert to base64\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": str(image_path)  # Just pass the file path as a string\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=100)\n",
    "print(\"Response from file path:\")\n",
    "print(response.content[0].text)\n",
    "\n",
    "# Clean up\n",
    "image_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Provider Compatibility\n",
    "\n",
    "The same multimodal content works across different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:28,256 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANTHROPIC (claude-3-5-sonnet-20241022):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:30,752 - v_router.router - INFO - Trying primary model: gemini-1.5-flash on google\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì This is a detailed macro photograph of an ant, showing its distinctive features like its segmented b...\n",
      "\n",
      "GOOGLE (gemini-1.5-flash):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:32,007 - v_router.router - INFO - Trying primary model: gpt-4o on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì That's a close-up image of a carpenter ant (genus *Camponotus*) carrying something.  Specifically, i...\n",
      "\n",
      "OPENAI (gpt-4o):\n",
      "‚úì This image shows a close-up of an ant. The ant is standing on a surface, and its body is clearly vis...\n"
     ]
    }
   ],
   "source": [
    "# Prepare the same multimodal message\n",
    "multimodal_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What do you see in this image?\"),\n",
    "            ImageContent(data=image_data, media_type=\"image/jpeg\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with different providers\n",
    "providers = [\n",
    "    (\"anthropic\", \"claude-3-5-sonnet-20241022\"),\n",
    "    (\"google\", \"gemini-1.5-flash\"),\n",
    "    (\"openai\", \"gpt-4o\")\n",
    "]\n",
    "\n",
    "for provider, model in providers:\n",
    "    print(f\"\\n{provider.upper()} ({model}):\")\n",
    "    try:\n",
    "        client = Client(\n",
    "            llm_config=LLM(\n",
    "                model_name=model,\n",
    "                provider=provider\n",
    "            )\n",
    "        )\n",
    "        response = await client.messages.create(messages=multimodal_messages, max_tokens=50)\n",
    "        print(f\"‚úì {response.content[0].text[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Documents\n",
    "\n",
    "Some providers (like Anthropic and Google) support PDF documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:34,661 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Analysis:\n",
      "The PDF contains only the text \"Dummy PDF file\" at the top of an otherwise blank page.\n"
     ]
    }
   ],
   "source": [
    "# Download a sample PDF\n",
    "pdf_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n",
    "try:\n",
    "    pdf_response = httpx.get(pdf_url, timeout=10.0)\n",
    "    pdf_data = base64.b64encode(pdf_response.content).decode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download PDF: {e}\")\n",
    "    # Use the local PDF as fallback\n",
    "    with open(\"providers/assets/gameboy_color.pdf\", \"rb\") as f:\n",
    "        pdf_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Send PDF to Anthropic\n",
    "anthropic_client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pdf_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What is the content of this PDF?\"),\n",
    "            DocumentContent(data=pdf_data, media_type=\"application/pdf\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await anthropic_client.messages.create(messages=pdf_messages, max_tokens=200)\n",
    "print(\"PDF Analysis:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Multimodal Conversations\n",
    "\n",
    "You can combine multiple images and text in a single message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-31 23:36:36,433 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Response:\n",
      "First image: This is a detailed macro photograph of an ant, showing its distinctive segmented body, thin legs, and antennae. The ant appears to be in a rearing or alert posture, with its front legs raised. The image has a shallow depth of field, creating a blurred background that makes the ant stand out in sharp detail.\n",
      "\n",
      "Second image: This is a close-up portrait of a ginger/orange cat with striking amber-colored eyes. The cat has a cream-colored face with orange markings, pointed ears, and appears to be looking directly at the camera. The image shows great detail in the cat's fur texture and facial features. The background appears to have some red stripes but is mostly out of focus, putting emphasis on the cat's face.\n"
     ]
    }
   ],
   "source": [
    "# Use the ant image from earlier and download a new one\n",
    "image1_data = image_data  # Reuse the ant image from earlier\n",
    "\n",
    "# Download a different image\n",
    "image2_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/320px-Cat03.jpg\"\n",
    "try:\n",
    "    image2_response = httpx.get(image2_url)\n",
    "    image2_data = base64.b64encode(image2_response.content).decode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download second image: {e}\")\n",
    "    # Use a simple fallback example\n",
    "    image2_data = image1_data\n",
    "\n",
    "# Create a complex multimodal message\n",
    "complex_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"I'm going to show you two images.\"),\n",
    "            TextContent(text=\"First image:\"),\n",
    "            ImageContent(data=image1_data, media_type=\"image/jpeg\"),\n",
    "            TextContent(text=\"Second image:\"),\n",
    "            ImageContent(data=image2_data, media_type=\"image/jpeg\"),\n",
    "            TextContent(text=\"Can you describe what you see in each image?\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = await anthropic_client.messages.create(messages=complex_messages, max_tokens=300)\n",
    "    print(\"Comparison Response:\")\n",
    "    print(response.content[0].text)\n",
    "except Exception as e:\n",
    "    print(f\"Error with complex message: {e}\")\n",
    "    print(\"\\nTrying with a simpler message...\")\n",
    "    \n",
    "    # Fallback to a simpler message\n",
    "    simple_messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                TextContent(text=\"What do you see in this image?\"),\n",
    "                ImageContent(data=image1_data, media_type=\"image/jpeg\")\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    response = await anthropic_client.messages.create(messages=simple_messages, max_tokens=100)\n",
    "    print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider-Specific Considerations\n",
    "\n",
    "### Anthropic\n",
    "- Supports: Images (JPEG, PNG, GIF, WebP) and PDFs\n",
    "- Max image size: 5MB per image\n",
    "- Multiple images per message: Yes\n",
    "\n",
    "### Google (Gemini)\n",
    "- Supports: Images and PDFs\n",
    "- Processes images through `inline_data`\n",
    "- Multiple images per message: Yes\n",
    "\n",
    "### OpenAI\n",
    "- Supports: Images only (no native PDF support)\n",
    "- Uses data URI format for images\n",
    "- Multiple images per message: Yes\n",
    "- Note: PDFs will show a placeholder message\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Image Optimization**: Resize large images before sending to reduce latency\n",
    "2. **Error Handling**: Always handle provider-specific limitations\n",
    "3. **Fallback Strategy**: Use v-router's fallback mechanism for providers that don't support certain content types\n",
    "4. **Content Validation**: Ensure your content matches supported MIME types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Building a Visual Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_image(image_path: str, question: str, provider: str = \"anthropic\"):\n",
    "    \"\"\"Analyze an image and answer a question about it.\"\"\"\n",
    "    \n",
    "    # Read and encode the image\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Determine MIME type\n",
    "    import mimetypes\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "    # Create client with fallback\n",
    "    client = Client(\n",
    "        llm_config=LLM(\n",
    "            model_name=\"claude-3-5-sonnet-20241022\" if provider == \"anthropic\" else \"gpt-4o\",\n",
    "            provider=provider,\n",
    "            try_other_providers=True  # Enable cross-provider fallback\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Send the question with the image\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                TextContent(text=question),\n",
    "                ImageContent(data=image_data, media_type=mime_type or \"image/jpeg\")\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = await client.messages.create(messages=messages, max_tokens=300)\n",
    "    return response.content[0].text\n",
    "\n",
    "# Example usage (you would need to provide an actual image path)\n",
    "# result = await analyze_image(\"/path/to/image.jpg\", \"What objects can you identify in this image?\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "v-router makes it easy to work with multimodal content across different LLM providers:\n",
    "\n",
    "- **Unified API**: Same interface for all providers\n",
    "- **Automatic conversion**: File paths are converted to base64 automatically\n",
    "- **Provider abstraction**: Handle provider differences transparently\n",
    "- **Fallback support**: Automatically try other providers if one fails\n",
    "\n",
    "This enables you to build robust multimodal applications without worrying about provider-specific implementation details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
