{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This notebook uses `await` syntax. Run it in a Jupyter environment with IPython kernel, or use `asyncio.run()` wrapper for regular Python scripts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multimodal Content with v-router üñºÔ∏è\n",
    "\n",
    "This notebook demonstrates how to use v-router with multimodal content (images, PDFs, and Word documents). The v-router library provides a unified interface for sending images and documents to different LLM providers.\n",
    "\n",
    "## Overview\n",
    "\n",
    "v-router supports:\n",
    "- **Images**: JPEG, PNG, GIF, WebP formats\n",
    "- **Documents**: PDF files and Word documents (.docx)\n",
    "- **Automatic conversion**: File paths are automatically converted to base64\n",
    "- **Unified interface**: Same API across all providers\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's set up our environment and imports:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: Using Local Files\n",
    "\n",
    "The easiest way to send images and PDFs is by passing file paths directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:03:26,965 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Summary:\n",
      "This is an instruction manual/booklet for the Nintendo Game Boy Color handheld video game system. The document covers various aspects of the device including:\n",
      "\n",
      "1. Introduction and basic features\n",
      "2. Component descriptions and diagrams\n",
      "3. Battery installation instructions\n",
      "4. Usage instructions and setup\n",
      "5. Warnings about battery packs\n",
      "6. Information about compatible Game Paks (game cartridges)\n",
      "7. Two-player gaming setup using Game Link cables\n",
      "8. Screen color settings\n",
      "9. Troubleshooting guide\n",
      "10. Warranty information\n",
      "11. Parts list and order form\n",
      "\n",
      "The manual provides detailed technical specifications, safety information, and operating instructions for users of the Game Boy Color, which was a color screen upgrade to Nintendo's original Game Boy handheld gaming system. It includes clear diagrams and step-by-step instructions for various features and functions of the device.\n"
     ]
    }
   ],
   "source": [
    "# Example using the PDF file included in this repository\n",
    "from v_router import Client, LLM\n",
    "\n",
    "# Create a client\n",
    "client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send a PDF file by just passing its path\n",
    "pdf_path = \"providers/assets/gameboy_color.pdf\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": pdf_path  # v-router automatically detects and converts the PDF\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is this document about? Give me a brief summary.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=200)\n",
    "print(\"PDF Summary:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import httpx\n",
    "from pathlib import Path\n",
    "\n",
    "# Import content types if you need to create multimodal messages manually\n",
    "from v_router.classes.message import TextContent, ImageContent, DocumentContent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending Images\n",
    "\n",
    "### Method 1: Base64 Encoded Images\n",
    "\n",
    "You can send images by providing base64-encoded data directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:03:40,919 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anthropic Response:\n",
      "This image shows a carpenter ant (genus Camponotus) in striking detail. The ant is captured in a dynamic pose, appearing to be rearing up on its hind legs, which is a common defensive posture. The ant's characteristic features are clearly visible, including its segmented body, long antennae, and powerful mandibles. The photo has a shallow depth of field, creating a beautifully blurred background that makes the ant stand out in sharp focus. The ant appears to be dark in color, possibly black or dark brown, and its exoskeleton has a slight sheen to it. Carpenter ants are among the larger ant species, and this image really showcases their impressive size and distinctive anatomy.\n"
     ]
    }
   ],
   "source": [
    "# Download a sample image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a7/Camponotus_flavomarginatus_ant.jpg/320px-Camponotus_flavomarginatus_ant.jpg\"\n",
    "response = httpx.get(image_url)\n",
    "image_data = base64.b64encode(response.content).decode(\"utf-8\")\n",
    "\n",
    "# Create a client with Anthropic\n",
    "client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send multimodal message\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What animal is in this image? Describe it in detail.\"),\n",
    "            ImageContent(data=image_data, media_type=\"image/jpeg\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=200)\n",
    "print(\"Anthropic Response:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: File Path (Automatic Conversion)\n",
    "\n",
    "v-router can automatically convert local image files to base64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:03:47,436 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from file path:\n",
      "This is a detailed macro photograph of an ant, showing its distinctive features like its segmented body, long antennae, and slender legs. The ant appears to be in a dynamic pose, with its body raised and antennae extended, suggesting it might be in a defensive or alert posture. The image has a shallow depth of field, creating a soft, blurred background that makes the ant stand out in sharp focus. The lighting and close-up perspective allow us to see the\n"
     ]
    }
   ],
   "source": [
    "# Save the image locally\n",
    "image_path = Path(\"/tmp/test_ant.jpg\")\n",
    "# Use the httpx response from earlier, not the v-router response\n",
    "with open(image_path, \"wb\") as f:\n",
    "    f.write(httpx.get(image_url).content)\n",
    "\n",
    "# Send using file path - v-router will automatically convert to base64\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": str(image_path)  # Just pass the file path as a string\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=100)\n",
    "print(\"Response from file path:\")\n",
    "print(response.content[0].text)\n",
    "\n",
    "# Clean up\n",
    "image_path.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Provider Compatibility\n",
    "\n",
    "The same multimodal content works across different providers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:03:53,910 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANTHROPIC (claude-3-5-sonnet-20241022):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:03:58,319 - v_router.router - INFO - Trying primary model: gemini-1.5-flash on google\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì This is a detailed macro photograph of an ant, showing its distinctive features like its segmented b...\n",
      "\n",
      "GOOGLE (gemini-1.5-flash):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:03:59,745 - v_router.router - INFO - Trying primary model: gpt-4o on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì That's a close-up image of a carpenter ant (genus *Camponotus*) carrying something.  Specifically, i...\n",
      "\n",
      "OPENAI (gpt-4o):\n",
      "‚úì The image shows a close-up of an ant standing on a surface. The ant appears to be in a raised positi...\n"
     ]
    }
   ],
   "source": [
    "# Prepare the same multimodal message\n",
    "multimodal_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What do you see in this image?\"),\n",
    "            ImageContent(data=image_data, media_type=\"image/jpeg\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with different providers\n",
    "providers = [\n",
    "    (\"anthropic\", \"claude-3-5-sonnet-20241022\"),\n",
    "    (\"google\", \"gemini-1.5-flash\"),\n",
    "    (\"openai\", \"gpt-4o\")\n",
    "]\n",
    "\n",
    "for provider, model in providers:\n",
    "    print(f\"\\n{provider.upper()} ({model}):\")\n",
    "    try:\n",
    "        client = Client(\n",
    "            llm_config=LLM(\n",
    "                model_name=model,\n",
    "                provider=provider\n",
    "            )\n",
    "        )\n",
    "        response = await client.messages.create(messages=multimodal_messages, max_tokens=50)\n",
    "        print(f\"‚úì {response.content[0].text[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Documents (.docx)\n",
    "\n",
    "v-router supports Word documents (.docx) by automatically converting them to HTML using the mammoth library. This works across all providers by sending the converted content as text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1: File Path (Automatic Conversion)\n",
    "\n",
    "The easiest way to send Word documents is by passing the file path directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:04:16,122 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Document Summary:\n",
      "This is a Purchase Order (PO) document with PO number 23781 for automotive parts. The customer, John Smith from Redline Auto Center in Lansing, Michigan, is ordering the following items:\n",
      "\n",
      "1. 4 units of Brake Discs, Pads & Calipers at $111.36 each\n",
      "2. 2 units of Control Arm at $60.93 each\n",
      "3. 2 units of Suspension Lift Kit at $399.83 each\n",
      "\n",
      "The subtotal is $1,366.96, with a 10% discount, 12% sales tax, additional costs for shipping & handling ($800) and other costs ($500), bringing the total amount to $2,694.30. The payment terms indicate that payment is due 30 days upon receipt of the items, and shipping terms are Freight on Board via Air & Land shipping method.\n"
     ]
    }
   ],
   "source": [
    "# Example using the Word document included in this repository\n",
    "from v_router import Client, LLM\n",
    "\n",
    "# Create a client\n",
    "client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send a Word document by just passing its path\n",
    "docx_path = \"providers/assets/order.docx\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": docx_path  # v-router automatically detects and converts the Word document\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": \"What is this document about? Give me a brief summary.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=200)\n",
    "print(\"Word Document Summary:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Base64 Encoded Word Documents\n",
    "\n",
    "You can also send Word documents by providing base64-encoded data directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:04:29,247 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Document Analysis:\n",
      "This is a Purchase Order (PO) document. This can be clearly identified by:\n",
      "\n",
      "1. The title \"PURCHASE ORDER\" at the top of the document\n",
      "2. The presence of a PO number (23781)\n",
      "3. The standard PO structure including:\n",
      "   - Vendor information\n",
      "   - Customer information\n",
      "   - Shipping terms and method\n",
      "   - Product details with codes, descriptions, quantities, and prices\n",
      "   - Cost calculations including subtotal, discount, tax, shipping & handling\n",
      "   - Payment terms in the notes section\n",
      "\n",
      "The document is formatted as a formal business transaction between a vendor and customer (Redline Auto Center) for the purchase of automotive parts, with detailed pricing and payment terms.\n"
     ]
    }
   ],
   "source": [
    "# Load and encode a Word document\n",
    "with open(\"providers/assets/order.docx\", \"rb\") as f:\n",
    "    docx_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Create a client with Anthropic\n",
    "client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"claude-3-5-sonnet-20241022\",\n",
    "        provider=\"anthropic\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Send multimodal message with Word document\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"Please analyze this Word document and tell me what type of document it is.\"),\n",
    "            DocumentContent(\n",
    "                data=docx_data, \n",
    "                media_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages, max_tokens=200)\n",
    "print(\"Word Document Analysis:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Provider Word Document Support\n",
    "\n",
    "Word documents work across all providers through automatic HTML conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:04:44,420 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ANTHROPIC (claude-3-5-sonnet-20241022):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:04:49,277 - v_router.router - INFO - Trying primary model: gemini-1.5-flash on google\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì This is a Purchase Order (PO) document. This can be clearly seen in the header of the document where...\n",
      "\n",
      "GOOGLE (gemini-1.5-flash):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:04:50,128 - v_router.router - INFO - Trying primary model: gpt-4o on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì This is a **Purchase Order (PO)**.  It's a commercial document issued by a buyer (Redline Auto Cente...\n",
      "\n",
      "OPENAI (gpt-4o):\n",
      "‚úì The document you provided is a \"Purchase Order.\" A purchase order is a commercial document issued by...\n"
     ]
    }
   ],
   "source": [
    "# Prepare the same Word document message\n",
    "word_doc_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What type of document is this?\"),\n",
    "            DocumentContent(\n",
    "                data=docx_data, \n",
    "                media_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test with different providers\n",
    "providers = [\n",
    "    (\"anthropic\", \"claude-3-5-sonnet-20241022\"),\n",
    "    (\"google\", \"gemini-1.5-flash\"),\n",
    "    (\"openai\", \"gpt-4o\")\n",
    "]\n",
    "\n",
    "for provider, model in providers:\n",
    "    print(f\"\\n{provider.upper()} ({model}):\")\n",
    "    try:\n",
    "        client = Client(\n",
    "            llm_config=LLM(\n",
    "                model_name=model,\n",
    "                provider=provider\n",
    "            )\n",
    "        )\n",
    "        response = await client.messages.create(messages=word_doc_messages, max_tokens=50)\n",
    "        print(f\"‚úì {response.content[0].text[:100]}...\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complex Multimodal Messages with Word Documents\n",
    "\n",
    "You can combine Word documents with images and other content types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:05:29,236 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Content Analysis:\n",
      "Let me describe both documents:\n",
      "\n",
      "1. The Word document appears to be a purchase order template with the following key elements:\n",
      "- Header section for company details\n",
      "- PO Number: 23781\n",
      "- Vendor and customer information sections\n",
      "- Shipping terms: Freight on Board\n",
      "- Shipping method: Air & Land\n",
      "- A list of auto parts being ordered (brake discs, control arm, suspension lift kit)\n",
      "- Pricing calculations including subtotal, discount, tax, shipping and handling\n",
      "- Total amount: $2,694.30\n",
      "- Payment terms of 30 days\n",
      "\n",
      "2. The image shows a close-up macro photograph of an ant in a dramatic pose. The ant appears to be standing upright on its hind legs in what looks like a defensive or alert posture. The photo has a shallow depth of field with a blurred reddish-brown background, creating an artistic composition that highlights the ant's distinctive features.\n"
     ]
    }
   ],
   "source": [
    "# Create a complex multimodal message with multiple content types\n",
    "complex_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"I'm sharing multiple documents with you:\"),\n",
    "            TextContent(text=\"1. A Word document:\"),\n",
    "            DocumentContent(\n",
    "                data=docx_data, \n",
    "                media_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "            ),\n",
    "            TextContent(text=\"2. An image:\"),\n",
    "            ImageContent(data=image_data, media_type=\"image/jpeg\"),\n",
    "            TextContent(text=\"Can you briefly describe what each contains?\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    client = Client(\n",
    "        llm_config=LLM(\n",
    "            model_name=\"claude-3-5-sonnet-20241022\",\n",
    "            provider=\"anthropic\"\n",
    "        )\n",
    "    )\n",
    "    response = await client.messages.create(messages=complex_messages, max_tokens=300)\n",
    "    print(\"Multi-Content Analysis:\")\n",
    "    print(response.content[0].text)\n",
    "except Exception as e:\n",
    "    print(f\"Error with complex message: {e}\")\n",
    "    print(\"\\nTrying with a simpler Word document message...\")\n",
    "    \n",
    "    # Fallback to a simpler message\n",
    "    simple_messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                TextContent(text=\"What type of document is this?\"),\n",
    "                DocumentContent(\n",
    "                    data=docx_data, \n",
    "                    media_type=\"application/vnd.openxmlformats-officedocument.wordprocessingml.document\"\n",
    "                )\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    response = await client.messages.create(messages=simple_messages, max_tokens=100)\n",
    "    print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF Documents\n",
    "\n",
    "Some providers (like Anthropic and Google) support PDF documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:09:02,070 - v_router.router - INFO - Trying primary model: gpt-4o-mini on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF Analysis:\n",
      "I'm sorry, but I can't view or analyze PDF documents directly. However, if you can provide text or specific details from the PDF, I'd be happy to help you understand or summarize the content!\n"
     ]
    }
   ],
   "source": [
    "# Download a sample PDF\n",
    "pdf_url = \"https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf\"\n",
    "try:\n",
    "    pdf_response = httpx.get(pdf_url, timeout=10.0)\n",
    "    pdf_data = base64.b64encode(pdf_response.content).decode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download PDF: {e}\")\n",
    "    # Use the local PDF as fallback\n",
    "    with open(\"providers/assets/gameboy_color.pdf\", \"rb\") as f:\n",
    "        pdf_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "# Send PDF to OpenAI\n",
    "anthropic_client = Client(\n",
    "    llm_config=LLM(\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        provider=\"openai\"\n",
    "    )\n",
    ")\n",
    "\n",
    "pdf_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"What is the content of this PDF?\"),\n",
    "            DocumentContent(data=pdf_data, media_type=\"application/pdf\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "response = await anthropic_client.messages.create(messages=pdf_messages, max_tokens=200)\n",
    "print(\"PDF Analysis:\")\n",
    "print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complex Multimodal Conversations\n",
    "\n",
    "You can combine multiple images and text in a single message:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-12 16:06:21,918 - v_router.router - INFO - Trying primary model: claude-3-5-sonnet-20241022 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison Response:\n",
      "First image: This is a detailed macro photograph of an ant, showing its distinctive segmented body, thin legs, and antennae. The ant appears to be in a rearing or alert posture, with its front legs raised. The image has a shallow depth of field, creating a blurred background that makes the ant stand out in sharp detail.\n",
      "\n",
      "Second image: This is a close-up portrait of a ginger/orange cat with striking amber-colored eyes. The cat has a cream-colored face with orange markings, pointed ears, and appears to be looking directly at the camera. The image shows great detail in the cat's fur texture and facial features. The background appears to have some red stripes but is mostly out of focus, putting emphasis on the cat's face.\n"
     ]
    }
   ],
   "source": [
    "# Use the ant image from earlier and download a new one\n",
    "image1_data = image_data  # Reuse the ant image from earlier\n",
    "\n",
    "# Download a different image\n",
    "image2_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/320px-Cat03.jpg\"\n",
    "try:\n",
    "    image2_response = httpx.get(image2_url)\n",
    "    image2_data = base64.b64encode(image2_response.content).decode(\"utf-8\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to download second image: {e}\")\n",
    "    # Use a simple fallback example\n",
    "    image2_data = image1_data\n",
    "\n",
    "# Create a complex multimodal message\n",
    "complex_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            TextContent(text=\"I'm going to show you two images.\"),\n",
    "            TextContent(text=\"First image:\"),\n",
    "            ImageContent(data=image1_data, media_type=\"image/jpeg\"),\n",
    "            TextContent(text=\"Second image:\"),\n",
    "            ImageContent(data=image2_data, media_type=\"image/jpeg\"),\n",
    "            TextContent(text=\"Can you describe what you see in each image?\")\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = await anthropic_client.messages.create(messages=complex_messages, max_tokens=300)\n",
    "    print(\"Comparison Response:\")\n",
    "    print(response.content[0].text)\n",
    "except Exception as e:\n",
    "    print(f\"Error with complex message: {e}\")\n",
    "    print(\"\\nTrying with a simpler message...\")\n",
    "    \n",
    "    # Fallback to a simpler message\n",
    "    simple_messages = [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": [\n",
    "                TextContent(text=\"What do you see in this image?\"),\n",
    "                ImageContent(data=image1_data, media_type=\"image/jpeg\")\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    response = await anthropic_client.messages.create(messages=simple_messages, max_tokens=100)\n",
    "    print(response.content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provider-Specific Considerations\n",
    "\n",
    "### Anthropic\n",
    "- Supports: Images (JPEG, PNG, GIF, WebP), PDFs, and Word documents (.docx)\n",
    "- Max image size: 5MB per image\n",
    "- Multiple images per message: Yes\n",
    "- Word documents: Converted to HTML automatically\n",
    "\n",
    "### Google (Gemini)\n",
    "- Supports: Images, PDFs, and Word documents (.docx)\n",
    "- Processes images through `inline_data`\n",
    "- Multiple images per message: Yes\n",
    "- Word documents: Converted to HTML automatically\n",
    "\n",
    "### OpenAI\n",
    "- Supports: Images and Word documents (.docx)\n",
    "- Uses data URI format for images\n",
    "- Multiple images per message: Yes\n",
    "- Word documents: Converted to HTML automatically\n",
    "- Note: PDFs will show a placeholder message\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Image Optimization**: Resize large images before sending to reduce latency\n",
    "2. **Error Handling**: Always handle provider-specific limitations\n",
    "3. **Fallback Strategy**: Use v-router's fallback mechanism for providers that don't support certain content types\n",
    "4. **Content Validation**: Ensure your content matches supported MIME types\n",
    "5. **Word Document Format**: Use .docx format (not .doc) for best compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Building a Visual Question Answering System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def analyze_image(image_path: str, question: str, provider: str = \"anthropic\"):\n",
    "    \"\"\"Analyze an image and answer a question about it.\"\"\"\n",
    "    \n",
    "    # Read and encode the image\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image_data = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "    \n",
    "    # Determine MIME type\n",
    "    import mimetypes\n",
    "    mime_type, _ = mimetypes.guess_type(image_path)\n",
    "    \n",
    "    # Create client with fallback\n",
    "    client = Client(\n",
    "        llm_config=LLM(\n",
    "            model_name=\"claude-3-5-sonnet-20241022\" if provider == \"anthropic\" else \"gpt-4o\",\n",
    "            provider=provider,\n",
    "            try_other_providers=True  # Enable cross-provider fallback\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Send the question with the image\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                TextContent(text=question),\n",
    "                ImageContent(data=image_data, media_type=mime_type or \"image/jpeg\")\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    response = await client.messages.create(messages=messages, max_tokens=300)\n",
    "    return response.content[0].text\n",
    "\n",
    "# Example usage (you would need to provide an actual image path)\n",
    "# result = await analyze_image(\"/path/to/image.jpg\", \"What objects can you identify in this image?\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "v-router makes it easy to work with multimodal content across different LLM providers:\n",
    "\n",
    "- **Unified API**: Same interface for all providers\n",
    "- **Automatic conversion**: File paths are converted to base64 automatically\n",
    "- **Word document support**: .docx files are automatically converted to HTML using mammoth\n",
    "- **Provider abstraction**: Handle provider differences transparently\n",
    "- **Fallback support**: Automatically try other providers if one fails\n",
    "\n",
    "This enables you to build robust multimodal applications without worrying about provider-specific implementation details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
