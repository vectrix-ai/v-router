{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "## Basic Example\n",
    "\n",
    "First, let's start with a basic example, to see how easy it is to switch between providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-29 18:46:19,171 - v_router.router - INFO - Trying primary model: claude-sonnet-4 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello, it's nice to meet you!\n",
      "Model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n"
     ]
    }
   ],
   "source": [
    "from v_router import Client, LLM, BackupModel\n",
    "\n",
    "llm_config = LLM(\n",
    "        model_name=\"claude-sonnet-4\",\n",
    "        provider=\"anthropic\",\n",
    "        max_tokens=100,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback Example\n",
    "Here we see what happens when you set a model fallback, if the main model fails VRouter will try the fallback models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-29 18:46:20,845 - v_router.router - INFO - Trying primary model: claude-6 on anthropic\u001b[0m\n",
      "\u001b[33m2025-05-29 18:46:21,067 - v_router.router - WARNING - Primary model failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-6'}}\u001b[0m\n",
      "\u001b[32m2025-05-29 18:46:21,068 - v_router.router - INFO - Trying backup model: gpt-4o on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 2 + 2 equals 4.\n",
      "Model: gpt-4o-2024-08-06\n",
      "Provider: openai\n"
     ]
    }
   ],
   "source": [
    "llm_config = LLM(\n",
    "        model_name=\"claude-6\",  # Primary model\n",
    "        provider=\"anthropic\",\n",
    "        max_tokens=100,\n",
    "        backup_models=[\n",
    "            BackupModel(\n",
    "                model=LLM(\n",
    "                    model_name=\"gpt-4o\",\n",
    "                    provider=\"openai\"\n",
    "                ),\n",
    "                priority=1  # First fallback\n",
    "            ),\n",
    "            BackupModel(\n",
    "                model=LLM(\n",
    "                    model_name=\"gemini-1.5-pro\",\n",
    "                    provider=\"google\"\n",
    "                ),\n",
    "                priority=2  # Second fallback\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "client = Client(llm_config)\n",
    "\n",
    "# This will try claude-3-opus first, then gpt-4, then gemini-1.5-pro\n",
    "response = await client.messages.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's 2+2?\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Provider Switch\n",
    "You can set try other providers = True.\n",
    "If a call fails the system will try to call another provider with the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-29 18:46:22,878 - v_router.router - INFO - Trying primary model: claude-opus-4 on vertexai\u001b[0m\n",
      "\u001b[33m2025-05-29 18:46:23,770 - v_router.router - WARNING - Primary model failed: Reauthentication is needed. Please run `gcloud auth application-default login` to reauthenticate.\u001b[0m\n",
      "\u001b[32m2025-05-29 18:46:23,770 - v_router.router - INFO - Trying alternative provider: claude-opus-4 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "Model: claude-opus-4-20250514\n",
      "Provider: anthropic\n"
     ]
    }
   ],
   "source": [
    "llm_config = LLM(\n",
    "        model_name=\"claude-opus-4\",\n",
    "        provider=\"vertexai\",  # Try Anthropic first\n",
    "        max_tokens=100,\n",
    "        try_other_providers=True  # Enable cross-provider fallback\n",
    "    )\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a short joke.\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "# Function Calling / Tools\n\nV-Router supports function calling across all providers! You can define tools once and they'll work with Anthropic, OpenAI, Google, and Azure models.\n\n## Basic Function Calling\n\nLet's start with a simple weather tool example.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "from pydantic import BaseModel, Field\nfrom v_router.classes.tools import ToolCall, Tools\n\n# Define the tool schema using Pydantic\nclass WeatherQuery(BaseModel):\n    \"\"\"Schema for weather query parameters.\"\"\"\n    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n    units: str = Field(\"fahrenheit\", description=\"Temperature units: fahrenheit or celsius\")\n\n# Create a tool definition\nweather_tool = ToolCall(\n    name=\"get_weather\",\n    description=\"Get the current weather in a given location\",\n    input_schema=WeatherQuery.model_json_schema()\n)\n\n# Create tools collection\ntools = Tools(tools=[weather_tool])\n\n# Create LLM configuration with tools\nllm_config = LLM(\n    model_name=\"claude-sonnet-4-20250514\",\n    provider=\"anthropic\",\n    tools=tools  # Pass the tools to the LLM\n)\n\nclient = Client(llm_config)\n\n# Make a request that should trigger the tool\nresponse = await client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n    ]\n)\n\nprint(f\"Response type: {type(response.content)}\")\nprint(f\"Model: {response.model}\")\nprint(f\"Provider: {response.provider}\")\n\n# Check if the response contains tool calls\nif isinstance(response.content, list):\n    print(\"\\nContent blocks:\")\n    for i, block in enumerate(response.content):\n        print(f\"  Block {i}: {block}\")\n        if hasattr(block, 'type'):\n            print(f\"    Type: {block.type}\")\n            if block.type == 'tool_use':\n                print(f\"    Tool: {block.name}\")\n                print(f\"    Input: {block.input}\")\nelse:\n    print(f\"\\nText response: {response.content}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Multiple Tools Example\n\nYou can define multiple tools for more complex workflows:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Define multiple tool schemas\nclass CalculatorQuery(BaseModel):\n    \"\"\"Schema for calculator operations.\"\"\"\n    operation: str = Field(..., description=\"The mathematical operation: add, subtract, multiply, divide\")\n    a: float = Field(..., description=\"First number\")\n    b: float = Field(..., description=\"Second number\")\n\nclass TimeQuery(BaseModel):\n    \"\"\"Schema for time queries.\"\"\"\n    timezone: str = Field(\"UTC\", description=\"Timezone (e.g., UTC, America/New_York, Europe/London)\")\n\n# Create multiple tools\ncalculator_tool = ToolCall(\n    name=\"calculator\",\n    description=\"Perform basic mathematical operations\",\n    input_schema=CalculatorQuery.model_json_schema()\n)\n\ntime_tool = ToolCall(\n    name=\"get_current_time\",\n    description=\"Get the current time in a specified timezone\",\n    input_schema=TimeQuery.model_json_schema()\n)\n\nweather_tool = ToolCall(\n    name=\"get_weather\",\n    description=\"Get the current weather in a given location\",\n    input_schema=WeatherQuery.model_json_schema()\n)\n\n# Create tools collection with multiple tools\nmulti_tools = Tools(tools=[calculator_tool, time_tool, weather_tool])\n\n# Create LLM configuration with multiple tools\nllm_config = LLM(\n    model_name=\"claude-sonnet-4-20250514\",\n    provider=\"anthropic\",\n    tools=multi_tools\n)\n\nclient = Client(llm_config)\n\n# Test with a query that could use multiple tools\nresponse = await client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What's 15 * 23? Also, what time is it in New York?\"}\n    ]\n)\n\nprint(f\"Response: {response.content}\")\nprint(f\"Raw tool calls: {[content for content in response.raw_response.content if hasattr(content, 'type') and content.type == 'tool_use']}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Cross-Provider Function Calling\n\nThe same tools work across different providers! Let's test with OpenAI:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Same tools, different provider\nllm_config_openai = LLM(\n    model_name=\"gpt-4\",\n    provider=\"openai\",\n    tools=multi_tools  # Same tools work across providers!\n)\n\nclient_openai = Client(llm_config_openai)\n\nresponse_openai = await client_openai.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Calculate 42 divided by 7\"}\n    ]\n)\n\nprint(\"=== OpenAI Provider ===\")\nprint(f\"Response type: {type(response_openai.content)}\")\nprint(f\"Provider: {response_openai.provider}\")\nprint(f\"Model: {response_openai.model}\")\n\n# OpenAI returns a ChatCompletionMessage object when tools are called\nif hasattr(response_openai.content, 'tool_calls') and response_openai.content.tool_calls:\n    print(\"\\nTool calls detected:\")\n    for tool_call in response_openai.content.tool_calls:\n        print(f\"  Tool: {tool_call.function.name}\")\n        print(f\"  Arguments: {tool_call.function.arguments}\")\nelse:\n    print(f\"Text: {response_openai.content}\")\n\n# Let's also try with Google\nllm_config_google = LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\",\n    tools=Tools(tools=[calculator_tool])  # Just the calculator for Google\n)\n\nclient_google = Client(llm_config_google)\n\nresponse_google = await client_google.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is 8 times 9?\"}\n    ]\n)\n\nprint(\"\\n=== Google Provider ===\")\nprint(f\"Response type: {type(response_google.content)}\")\nprint(f\"Provider: {response_google.provider}\")\nprint(f\"Model: {response_google.model}\")\n\n# Google returns a list of Part objects when tools are called\nif isinstance(response_google.content, list):\n    for part in response_google.content:\n        if hasattr(part, 'function_call') and part.function_call:\n            print(\"\\nFunction call detected:\")\n            print(f\"  Function: {part.function_call.name}\")\n            print(f\"  Arguments: {part.function_call.args}\")\nelse:\n    print(f\"Text: {response_google.content}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Function Calling with Fallbacks\n\nTools work seamlessly with the fallback system. If the primary model fails, the backup models will inherit the same tools:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a configuration with a non-existent primary model and fallbacks\nllm_config_with_fallback = LLM(\n    model_name=\"claude-nonexistent\",  # This will fail\n    provider=\"anthropic\",\n    tools=multi_tools,  # Tools will be passed to fallback models too\n    backup_models=[\n        BackupModel(\n            model=LLM(\n                model_name=\"gpt-4\",\n                provider=\"openai\"\n                # Note: No tools specified here - they'll be inherited from primary\n            ),\n            priority=1\n        ),\n        BackupModel(\n            model=LLM(\n                model_name=\"gemini-1.5-pro\", \n                provider=\"google\"\n                # Note: No tools specified here - they'll be inherited from primary\n            ),\n            priority=2\n        )\n    ]\n)\n\nclient_fallback = Client(llm_config_with_fallback)\n\n# This will fail on the primary model but succeed on the fallback with tools intact\nresponse_fallback = await client_fallback.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Please calculate 100 divided by 5\"}\n    ]\n)\n\nprint(f\"Response type: {type(response_fallback.content)}\")\nprint(f\"Provider used: {response_fallback.provider}\")\nprint(f\"Model used: {response_fallback.model}\")\n\n# Check if tools were used\nif hasattr(response_fallback.content, 'tool_calls') and response_fallback.content.tool_calls:\n    print(\"\\n✅ Tools work seamlessly with fallbacks!\")\n    print(\"Tool calls:\")\n    for tool_call in response_fallback.content.tool_calls:\n        print(f\"  Tool: {tool_call.function.name}\")\n        print(f\"  Arguments: {tool_call.function.arguments}\")\nelif isinstance(response_fallback.content, str):\n    print(f\"\\nText response: {response_fallback.content}\")\nelse:\n    print(f\"\\nRaw content: {response_fallback.content}\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Handling Tool Responses\n\nIn a real application, you'd want to execute the tools and provide results back to the model. Here's a complete example:",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nfrom datetime import datetime\n\ndef execute_tool(tool_name: str, tool_input: dict) -> str:\n    \"\"\"Simulate executing tools and returning results.\"\"\"\n    if tool_name == \"calculator\":\n        operation = tool_input[\"operation\"]\n        a = tool_input[\"a\"]\n        b = tool_input[\"b\"]\n        \n        if operation == \"add\":\n            result = a + b\n        elif operation == \"subtract\":\n            result = a - b\n        elif operation == \"multiply\":\n            result = a * b\n        elif operation == \"divide\":\n            result = a / b if b != 0 else \"Error: Division by zero\"\n        else:\n            result = \"Error: Unknown operation\"\n            \n        return f\"The result of {a} {operation} {b} is {result}\"\n    \n    elif tool_name == \"get_current_time\":\n        # Simulate getting current time\n        return f\"The current time is {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (simulated)\"\n    \n    elif tool_name == \"get_weather\":\n        location = tool_input[\"location\"]\n        units = tool_input.get(\"units\", \"fahrenheit\")\n        # Simulate weather data\n        return f\"The weather in {location} is 72°F (22°C), sunny with light winds (simulated)\"\n    \n    return \"Tool execution not implemented\"\n\n# Create a simple calculator tool for this example\nsimple_calc_tool = ToolCall(\n    name=\"calculator\", \n    description=\"Perform basic math operations\",\n    input_schema=CalculatorQuery.model_json_schema()\n)\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4-20250514\",\n    provider=\"anthropic\",\n    tools=Tools(tools=[simple_calc_tool])\n)\n\nclient = Client(llm_config)\n\n# Initial request\nmessages = [\n    {\"role\": \"user\", \"content\": \"What is 25 times 4?\"}\n]\n\nresponse = await client.messages.create(messages=messages)\n\nprint(\"Initial response:\")\nprint(f\"Content: {response.content}\")\n\n# Check if there are tool calls in the response\ntool_calls = [content for content in response.raw_response.content if hasattr(content, 'type') and content.type == 'tool_use']\n\nif tool_calls:\n    print(f\"\\nFound {len(tool_calls)} tool call(s):\")\n    \n    # Add the assistant's response to conversation\n    messages.append({\n        \"role\": \"assistant\", \n        \"content\": response.raw_response.content\n    })\n    \n    # Execute each tool and add results\n    for tool_call in tool_calls:\n        print(f\"Executing tool: {tool_call.name}\")\n        print(f\"Input: {tool_call.input}\")\n        \n        # Execute the tool\n        result = execute_tool(tool_call.name, tool_call.input)\n        print(f\"Result: {result}\")\n        \n        # Add tool result to conversation\n        messages.append({\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"tool_result\",\n                    \"tool_use_id\": tool_call.id,\n                    \"content\": result\n                }\n            ]\n        })\n    \n    # Get final response with tool results\n    final_response = await client.messages.create(messages=messages)\n    print(f\"\\nFinal response with tool results:\")\n    print(f\"Content: {final_response.content}\")\nelse:\n    print(\"No tool calls found in response\")",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Summary\n\n✅ **Function calling works across all providers**: Anthropic, OpenAI, Google, and Azure\n✅ **Unified interface**: Define tools once, use everywhere  \n✅ **Fallback support**: Tools are automatically passed to backup models\n✅ **Type safety**: Use Pydantic schemas for tool parameters\n✅ **Cross-provider compatibility**: Same tool definitions work with different providers\n\nThe v-router package now provides a truly unified interface for function calling across all major LLM providers!",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}