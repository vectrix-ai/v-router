{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart\n",
    "\n",
    "## Basic Example\n",
    "\n",
    "First, let's start with a basic example, to see how easy it is to switch between providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Hello! It's nice to meet you.\n",
      "Model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n"
     ]
    }
   ],
   "source": [
    "from v_router import Client, LLM, BackupModel\n",
    "\n",
    "llm_config = LLM(\n",
    "        model_name=\"claude-sonnet-4\",\n",
    "        provider=\"anthropic\",\n",
    "        max_tokens=100,\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Say hello in one sentence.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fallback Example\n",
    "Here we see what happens when you set a model fallback, if the main model fails VRouter will try the fallback models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Primary model failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-6'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 2 + 2 equals 4.\n",
      "Model: gpt-4o-2024-08-06\n",
      "Provider: openai\n"
     ]
    }
   ],
   "source": [
    "llm_config = LLM(\n",
    "        model_name=\"claude-6\",  # Primary model\n",
    "        provider=\"anthropic\",\n",
    "        max_tokens=100,\n",
    "        backup_models=[\n",
    "            BackupModel(\n",
    "                model=LLM(\n",
    "                    model_name=\"gpt-4o\",\n",
    "                    provider=\"openai\"\n",
    "                ),\n",
    "                priority=1  # First fallback\n",
    "            ),\n",
    "            BackupModel(\n",
    "                model=LLM(\n",
    "                    model_name=\"gemini-1.5-pro\",\n",
    "                    provider=\"google\"\n",
    "                ),\n",
    "                priority=2  # Second fallback\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "client = Client(llm_config)\n",
    "\n",
    "# This will try claude-3-opus first, then gpt-4, then gemini-1.5-pro\n",
    "response = await client.messages.create(\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What's 2+2?\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Provider Switch\n",
    "You can set try other providers = True.\n",
    "If a call fails the system will try to call another provider with the same model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Primary model failed: Error code: 429 - {'error': {'code': 429, 'message': 'Quota exceeded for aiplatform.googleapis.com/online_prediction_input_tokens_per_minute_per_base_model with base model: anthropic-claude-opus-4. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai.', 'status': 'RESOURCE_EXHAUSTED'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "Model: claude-opus-4-20250514\n",
      "Provider: anthropic\n"
     ]
    }
   ],
   "source": [
    "llm_config = LLM(\n",
    "        model_name=\"claude-opus-4\",\n",
    "        provider=\"vertexai\",  # Try Anthropic first\n",
    "        max_tokens=100,\n",
    "        try_other_providers=True  # Enable cross-provider fallback\n",
    "    )\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Tell me a short joke.\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "print(f\"Response: {response.content}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
