{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: Function Calling and Tools\n",
    "\n",
    "This notebook demonstrates how to use function calling (tools) with v-router across different LLM providers.\n",
    "\n",
    "## What is Function Calling?\n",
    "\n",
    "Function calling allows LLMs to interact with external tools and APIs by generating structured function calls. v-router provides:\n",
    "- **Unified tool interface** across all providers (Anthropic, OpenAI, Google, Azure)\n",
    "- **Automatic tool inheritance** for fallback models\n",
    "- **Type-safe tool definitions** using Pydantic schemas\n",
    "- **Consistent response format** for tool calls across providers\n",
    "\n",
    "## Core Tool Components\n",
    "\n",
    "### Request Models\n",
    "- **`ToolCall`**: Definition of a single tool with name, description, and input schema\n",
    "- **`Tools`**: Collection of multiple tools to pass to the LLM\n",
    "- **`LLM`**: Configuration that includes tools alongside model parameters\n",
    "\n",
    "### Response Models\n",
    "- **`Response.tool_use`**: List of tool calls made by the model\n",
    "- **`ToolUse`**: Individual tool call with name, ID, and parsed arguments\n",
    "- **`Response.content`**: Text content alongside tool calls\n",
    "- **Unified format**: Same structure across all providers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Function Calling\n",
    "\n",
    "Let's start with a simple weather tool example to demonstrate the fundamentals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:13,720 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response model: claude-sonnet-4-20250514\n",
      "Provider: anthropic\n",
      "\n",
      "Content blocks (1):\n",
      "  Block 0: type=text, role=assistant\n",
      "    Text: I'll check the current weather in San Francisco for you.\n",
      "\n",
      "Tool uses (1):\n",
      "  Tool 0: get_weather\n",
      "    ID: toolu_015UTVMAEaqsaoM3DCr4vpAo\n",
      "    Arguments: {'location': 'San Francisco, CA'}\n",
      "\n",
      "Usage:\n",
      "  Input tokens: 462\n",
      "  Output tokens: 69\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from v_router import Client, LLM\n",
    "from v_router.classes.tools import ToolCall, Tools\n",
    "\n",
    "# Define the tool schema using Pydantic\n",
    "class WeatherQuery(BaseModel):\n",
    "    \"\"\"Schema for weather query parameters.\"\"\"\n",
    "    location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\")\n",
    "    units: str = Field(\"fahrenheit\", description=\"Temperature units: fahrenheit or celsius\")\n",
    "\n",
    "# Create a tool definition\n",
    "weather_tool = ToolCall(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather in a given location\",\n",
    "    input_schema=WeatherQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "# Pass tools as a list\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    tools=[weather_tool]  # Clean and simple\n",
    ")\n",
    "\n",
    "# Alternative syntax (also works):\n",
    "# tools=Tools(tools=[weather_tool])\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "# Make a request that should trigger the tool\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's the weather like in San Francisco?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Response model: {response.model}\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "\n",
    "# Check the unified response format\n",
    "print(f\"\\nContent blocks ({len(response.content)}):\")\n",
    "for i, content in enumerate(response.content):\n",
    "    print(f\"  Block {i}: type={content.type}, role={content.role}\")\n",
    "    if content.type == \"text\":\n",
    "        print(f\"    Text: {content.text}\")\n",
    "\n",
    "print(f\"\\nTool uses ({len(response.tool_use)}):\")\n",
    "for i, tool_use in enumerate(response.tool_use):\n",
    "    print(f\"  Tool {i}: {tool_use.name}\")\n",
    "    print(f\"    ID: {tool_use.id}\")\n",
    "    print(f\"    Arguments: {tool_use.arguments}\")\n",
    "\n",
    "print(f\"\\nUsage:\")\n",
    "print(f\"  Input tokens: {response.usage.input_tokens}\")\n",
    "print(f\"  Output tokens: {response.usage.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Tools Example\n",
    "\n",
    "You can define multiple tools for more complex workflows. Each tool is defined with its own Pydantic schema for type safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:16,150 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider: anthropic\n",
      "Model: claude-sonnet-4-20250514\n",
      "\n",
      "üìù Content blocks: 1\n",
      "  - text: I'll help you with both of those questions!\n",
      "\n",
      "üîß Tool uses: 2\n",
      "  - calculator: {'operation': 'multiply', 'a': 15, 'b': 23}\n",
      "  - get_current_time: {'timezone': 'America/New_York'}\n"
     ]
    }
   ],
   "source": [
    "# Define multiple tool schemas\n",
    "class CalculatorQuery(BaseModel):\n",
    "    \"\"\"Schema for calculator operations.\"\"\"\n",
    "    operation: str = Field(..., description=\"The mathematical operation: add, subtract, multiply, divide\")\n",
    "    a: float = Field(..., description=\"First number\")\n",
    "    b: float = Field(..., description=\"Second number\")\n",
    "\n",
    "class TimeQuery(BaseModel):\n",
    "    \"\"\"Schema for time queries.\"\"\"\n",
    "    timezone: str = Field(\"UTC\", description=\"Timezone (e.g., UTC, America/New_York, Europe/London)\")\n",
    "\n",
    "# Create multiple tools\n",
    "calculator_tool = ToolCall(\n",
    "    name=\"calculator\",\n",
    "    description=\"Perform basic mathematical operations\",\n",
    "    input_schema=CalculatorQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "time_tool = ToolCall(\n",
    "    name=\"get_current_time\",\n",
    "    description=\"Get the current time in a specified timezone\",\n",
    "    input_schema=TimeQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "weather_tool = ToolCall(\n",
    "    name=\"get_weather\",\n",
    "    description=\"Get the current weather in a given location\",\n",
    "    input_schema=WeatherQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "# Pass multiple tools as a list\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    tools=[calculator_tool, time_tool, weather_tool]\n",
    ")\n",
    "\n",
    "# Alternative syntax: tools=Tools(tools=[calculator_tool, time_tool, weather_tool])\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "# Test with a query that could use multiple tools\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's 15 * 23? Also, what time is it in New York?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "\n",
    "# Show the unified response format\n",
    "print(f\"\\nüìù Content blocks: {len(response.content)}\")\n",
    "for content in response.content:\n",
    "    print(f\"  - {content.type}: {content.text[:50]}...\" if len(content.text) > 50 else f\"  - {content.type}: {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool uses: {len(response.tool_use)}\")\n",
    "for tool in response.tool_use:\n",
    "    print(f\"  - {tool.name}: {tool.arguments}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Provider Function Calling\n",
    "\n",
    "The same tools work across different providers! This demonstrates v-router's unified interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:19,298 - v_router.router - INFO - Trying primary model: gpt-4 on openai\u001b[0m\n",
      "\u001b[32m2025-06-09 01:37:21,593 - v_router.router - INFO - Trying primary model: gemini-1.5-pro on google\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== OpenAI Provider ===\n",
      "Provider: openai\n",
      "Model: gpt-4-0613\n",
      "\n",
      "üìù Content: 0 blocks\n",
      "\n",
      "üîß Tool uses: 1\n",
      "  Tool: calculator\n",
      "  Arguments: {'operation': 'divide', 'a': 42, 'b': 7}\n",
      "\n",
      "=== Google Provider ===\n",
      "Provider: google\n",
      "Model: gemini-1.5-pro\n",
      "\n",
      "üìù Content: 0 blocks\n",
      "\n",
      "üîß Tool uses: 1\n",
      "  Tool: calculator\n",
      "  Arguments: {'b': 9, 'a': 8, 'operation': 'multiply'}\n",
      "\n",
      "‚úÖ Notice how all providers now return the same Response format!\n",
      "‚úÖ And the same tools syntax works everywhere!\n"
     ]
    }
   ],
   "source": [
    "# Same tools, different provider - OpenAI\n",
    "llm_config_openai = LLM(\n",
    "    model_name=\"gpt-4\",\n",
    "    provider=\"openai\",\n",
    "    tools=[calculator_tool, time_tool, weather_tool]  # Same syntax across providers\n",
    ")\n",
    "\n",
    "client_openai = Client(llm_config_openai)\n",
    "\n",
    "response_openai = await client_openai.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Calculate 42 divided by 7\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== OpenAI Provider ===\")\n",
    "print(f\"Provider: {response_openai.provider}\")\n",
    "print(f\"Model: {response_openai.model}\")\n",
    "\n",
    "# Unified response format across all providers!\n",
    "print(f\"\\nüìù Content: {len(response_openai.content)} blocks\")\n",
    "for content in response_openai.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool uses: {len(response_openai.tool_use)}\")\n",
    "for tool_use in response_openai.tool_use:\n",
    "    print(f\"  Tool: {tool_use.name}\")\n",
    "    print(f\"  Arguments: {tool_use.arguments}\")\n",
    "\n",
    "# Let's also try with Google\n",
    "llm_config_google = LLM(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    provider=\"google\",\n",
    "    tools=[calculator_tool]  # Just the calculator for Google\n",
    ")\n",
    "\n",
    "client_google = Client(llm_config_google)\n",
    "\n",
    "response_google = await client_google.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is 8 times 9?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Google Provider ===\")\n",
    "print(f\"Provider: {response_google.provider}\")\n",
    "print(f\"Model: {response_google.model}\")\n",
    "\n",
    "# Same unified format for Google!\n",
    "print(f\"\\nüìù Content: {len(response_google.content)} blocks\")\n",
    "for content in response_google.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool uses: {len(response_google.tool_use)}\")\n",
    "for tool_use in response_google.tool_use:\n",
    "    print(f\"  Tool: {tool_use.name}\")\n",
    "    print(f\"  Arguments: {tool_use.arguments}\")\n",
    "\n",
    "print(\"\\n‚úÖ Notice how all providers now return the same Response format!\")\n",
    "print(\"‚úÖ And the same tools syntax works everywhere!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling with Fallbacks\n",
    "\n",
    "Tools work seamlessly with the fallback system. If the primary model fails, the backup models will automatically inherit the same tools.\n",
    "\n",
    "### How Tool Inheritance Works:\n",
    "1. **Primary Model**: Defines tools in the main LLM configuration\n",
    "2. **Automatic Inheritance**: Backup models inherit tools from primary (no need to redefine)\n",
    "3. **Provider Translation**: Tools are automatically translated to each provider's format\n",
    "4. **Seamless Fallback**: If primary fails, backup models have the same capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:22,424 - v_router.router - INFO - Trying primary model: claude-nonexistent on anthropic\u001b[0m\n",
      "\u001b[33m2025-06-09 01:37:22,699 - v_router.router - WARNING - Primary model failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-nonexistent'}}\u001b[0m\n",
      "\u001b[32m2025-06-09 01:37:22,700 - v_router.router - INFO - Trying backup model: gpt-4 on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Provider used: openai\n",
      "Model used: gpt-4-0613\n",
      "\n",
      "‚úÖ Tools work seamlessly with fallbacks!\n",
      "Content blocks: 0\n",
      "\n",
      "Tool calls: 1\n",
      "  Tool: calculator\n",
      "  Arguments: {'operation': 'divide', 'a': 100, 'b': 5}\n",
      "\n",
      "üí° The backup model inherited tools from the primary configuration!\n",
      "‚ú® Using the simplified API: tools=[tool1, tool2, tool3]\n"
     ]
    }
   ],
   "source": [
    "from v_router import BackupModel\n",
    "\n",
    "# Create a configuration with a non-existent primary model and fallbacks\n",
    "llm_config_with_fallback = LLM(\n",
    "    model_name=\"claude-nonexistent\",  # This will fail\n",
    "    provider=\"anthropic\",\n",
    "    tools=[calculator_tool, time_tool, weather_tool],  # ‚ú® Simplified API in primary config\n",
    "    backup_models=[\n",
    "        BackupModel(\n",
    "            model=LLM(\n",
    "                model_name=\"gpt-4\",\n",
    "                provider=\"openai\"\n",
    "                # Note: No tools specified here - they'll be inherited from primary\n",
    "            ),\n",
    "            priority=1\n",
    "        ),\n",
    "        BackupModel(\n",
    "            model=LLM(\n",
    "                model_name=\"gemini-1.5-pro\", \n",
    "                provider=\"google\"\n",
    "                # Note: No tools specified here - they'll be inherited from primary\n",
    "            ),\n",
    "            priority=2\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "client_fallback = Client(llm_config_with_fallback)\n",
    "\n",
    "# This will fail on the primary model but succeed on the fallback with tools intact\n",
    "response_fallback = await client_fallback.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Please calculate 100 divided by 5\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Provider used: {response_fallback.provider}\")\n",
    "print(f\"Model used: {response_fallback.model}\")\n",
    "\n",
    "# Check if tools were used (unified format)\n",
    "print(f\"\\n‚úÖ Tools work seamlessly with fallbacks!\")\n",
    "print(f\"Content blocks: {len(response_fallback.content)}\")\n",
    "for content in response_fallback.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text}\")\n",
    "\n",
    "if response_fallback.tool_use:\n",
    "    print(f\"\\nTool calls: {len(response_fallback.tool_use)}\")\n",
    "    for tool_use in response_fallback.tool_use:\n",
    "        print(f\"  Tool: {tool_use.name}\")\n",
    "        print(f\"  Arguments: {tool_use.arguments}\")\n",
    "        \n",
    "print(f\"\\nüí° The backup model inherited tools from the primary configuration!\")\n",
    "print(f\"‚ú® Using the simplified API: tools=[tool1, tool2, tool3]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Tool Responses\n",
    "\n",
    "In a real application, you'd want to execute the tools and provide results back to the model. Here's a complete example of the full tool calling workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:25,096 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n",
      "\u001b[32m2025-06-09 01:37:27,562 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial response:\n",
      "Provider: anthropic\n",
      "Model: claude-sonnet-4-20250514\n",
      "\n",
      "Content blocks: 1\n",
      "  Text: I'll calculate 25 times 4 for you.\n",
      "\n",
      "Found 1 tool call(s):\n",
      "Executing tool: calculator\n",
      "Input: {'operation': 'multiply', 'a': 25, 'b': 4}\n",
      "Result: The result of 25 multiply 4 is 100\n",
      "\n",
      "Final response with tool results:\n",
      "Content blocks: 0\n",
      "\n",
      "‚ú® Notice how clean the tools setup is: tools=[simple_calc_tool]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def execute_tool(tool_name: str, tool_input: dict) -> str:\n",
    "    \"\"\"Simulate executing tools and returning results.\"\"\"\n",
    "    if tool_name == \"calculator\":\n",
    "        operation = tool_input[\"operation\"]\n",
    "        a = tool_input[\"a\"]\n",
    "        b = tool_input[\"b\"]\n",
    "        \n",
    "        if operation == \"add\":\n",
    "            result = a + b\n",
    "        elif operation == \"subtract\":\n",
    "            result = a - b\n",
    "        elif operation == \"multiply\":\n",
    "            result = a * b\n",
    "        elif operation == \"divide\":\n",
    "            result = a / b if b != 0 else \"Error: Division by zero\"\n",
    "        else:\n",
    "            result = \"Error: Unknown operation\"\n",
    "            \n",
    "        return f\"The result of {a} {operation} {b} is {result}\"\n",
    "    \n",
    "    elif tool_name == \"get_current_time\":\n",
    "        # Simulate getting current time\n",
    "        return f\"The current time is {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} (simulated)\"\n",
    "    \n",
    "    elif tool_name == \"get_weather\":\n",
    "        location = tool_input[\"location\"]\n",
    "        units = tool_input.get(\"units\", \"fahrenheit\")\n",
    "        # Simulate weather data\n",
    "        return f\"The weather in {location} is 72¬∞F (22¬∞C), sunny with light winds (simulated)\"\n",
    "    \n",
    "    return \"Tool execution not implemented\"\n",
    "\n",
    "# Create a simple calculator tool for this example\n",
    "simple_calc_tool = ToolCall(\n",
    "    name=\"calculator\", \n",
    "    description=\"Perform basic math operations\",\n",
    "    input_schema=CalculatorQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "# ‚ú® Simplified API in action!\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    tools=[simple_calc_tool]  # Much cleaner than Tools(tools=[simple_calc_tool])\n",
    ")\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "# Initial request\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is 25 times 4?\"}\n",
    "]\n",
    "\n",
    "response = await client.messages.create(messages=messages)\n",
    "\n",
    "print(\"Initial response:\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "\n",
    "# Display content blocks\n",
    "print(f\"\\nContent blocks: {len(response.content)}\")\n",
    "for content in response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text}\")\n",
    "\n",
    "# Check if there are tool calls in the unified format\n",
    "if response.tool_use:\n",
    "    print(f\"\\nFound {len(response.tool_use)} tool call(s):\")\n",
    "    \n",
    "    # Add the assistant's text response (if any) to the conversation\n",
    "    assistant_text = \"\"\n",
    "    if response.content:\n",
    "        assistant_text = response.content[0].text\n",
    "    \n",
    "    messages.append({\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": assistant_text\n",
    "    })\n",
    "    \n",
    "    # Execute each tool and add results\n",
    "    for tool_use in response.tool_use:\n",
    "        print(f\"Executing tool: {tool_use.name}\")\n",
    "        print(f\"Input: {tool_use.arguments}\")\n",
    "        \n",
    "        # Execute the tool\n",
    "        result = execute_tool(tool_use.name, tool_use.arguments)\n",
    "        print(f\"Result: {result}\")\n",
    "        \n",
    "        # Add tool result to conversation as a user message\n",
    "        messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Tool result: {result}\"\n",
    "        })\n",
    "    \n",
    "    # Get final response with tool results\n",
    "    final_response = await client.messages.create(messages=messages)\n",
    "    print(f\"\\nFinal response with tool results:\")\n",
    "    print(f\"Content blocks: {len(final_response.content)}\")\n",
    "    for content in final_response.content:\n",
    "        if content.type == \"text\":\n",
    "            print(f\"  Text: {content.text}\")\n",
    "else:\n",
    "    print(\"\\nNo tool calls found in response\")\n",
    "\n",
    "print(\"\\n‚ú® Notice how clean the tools setup is: tools=[simple_calc_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Tool Patterns\n",
    "\n",
    "Let's explore some advanced patterns for tool usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:29,966 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Advanced Tool Usage ===\n",
      "Provider: anthropic\n",
      "Model: claude-sonnet-4-20250514\n",
      "\n",
      "üìù Content: 1 blocks\n",
      "  Text: I'll help you with both tasks - searching for recent machine learning developments and calculating 1...\n",
      "\n",
      "üîß Tool calls: 2\n",
      "  Tool: web_search\n",
      "  Arguments: {\n",
      "  \"query\": \"recent machine learning trends developments 2024\",\n",
      "  \"max_results\": 5\n",
      "}\n",
      "  Tool: calculator\n",
      "  Arguments: {\n",
      "  \"operation\": \"multiply\",\n",
      "  \"a\": 2000,\n",
      "  \"b\": 0.15\n",
      "}\n",
      "\n",
      "üí° Complex tools with nested schemas work seamlessly!\n",
      "‚ú® Using the clean syntax: tools=[tool1, tool2, tool3, tool4]\n"
     ]
    }
   ],
   "source": [
    "# Complex tool with nested data structures\n",
    "from typing import List, Optional\n",
    "\n",
    "class SearchResult(BaseModel):\n",
    "    \"\"\"Individual search result.\"\"\"\n",
    "    title: str\n",
    "    url: str\n",
    "    snippet: str\n",
    "\n",
    "class WebSearchQuery(BaseModel):\n",
    "    \"\"\"Schema for web search operations.\"\"\"\n",
    "    query: str = Field(..., description=\"Search query string\")\n",
    "    max_results: int = Field(5, description=\"Maximum number of results to return\")\n",
    "    include_snippets: bool = Field(True, description=\"Whether to include content snippets\")\n",
    "    domains: Optional[List[str]] = Field(None, description=\"Specific domains to search within\")\n",
    "\n",
    "class DataAnalysisQuery(BaseModel):\n",
    "    \"\"\"Schema for data analysis operations.\"\"\"\n",
    "    data_source: str = Field(..., description=\"Path or identifier for the data source\")\n",
    "    analysis_type: str = Field(..., description=\"Type of analysis: summary, correlation, trend, distribution\")\n",
    "    columns: Optional[List[str]] = Field(None, description=\"Specific columns to analyze\")\n",
    "    filters: Optional[dict] = Field(None, description=\"Filters to apply to the data\")\n",
    "\n",
    "# Create advanced tools\n",
    "web_search_tool = ToolCall(\n",
    "    name=\"web_search\",\n",
    "    description=\"Search the web for information and return relevant results\",\n",
    "    input_schema=WebSearchQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "data_analysis_tool = ToolCall(\n",
    "    name=\"analyze_data\",\n",
    "    description=\"Perform statistical analysis on datasets\",\n",
    "    input_schema=DataAnalysisQuery.model_json_schema()\n",
    ")\n",
    "\n",
    "# ‚ú® Simplified API with complex tools!\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    tools=[calculator_tool, weather_tool, web_search_tool, data_analysis_tool]  # Clean list syntax!\n",
    ")\n",
    "\n",
    "# Compare with old verbose way:\n",
    "# tools=Tools(tools=[calculator_tool, weather_tool, web_search_tool, data_analysis_tool])\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"I need to research machine learning trends. Can you search for recent ML developments and also calculate what 15% of 2000 is?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Advanced Tool Usage ===\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"Model: {response.model}\")\n",
    "\n",
    "print(f\"\\nüìù Content: {len(response.content)} blocks\")\n",
    "for content in response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  Text: {content.text[:100]}...\" if len(content.text) > 100 else f\"  Text: {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool calls: {len(response.tool_use)}\")\n",
    "for tool_use in response.tool_use:\n",
    "    print(f\"  Tool: {tool_use.name}\")\n",
    "    print(f\"  Arguments: {json.dumps(tool_use.arguments, indent=2)}\")\n",
    "\n",
    "print(f\"\\nüí° Complex tools with nested schemas work seamlessly!\")\n",
    "print(f\"‚ú® Using the clean syntax: tools=[tool1, tool2, tool3, tool4]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Tool Response Format\n",
    "\n",
    "Let's examine the unified tool response format in detail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controlling Tool Usage with tool_choice\n",
    "\n",
    "v-router now supports fine-grained control over when and how tools are used through the `tool_choice` parameter. This allows you to:\n",
    "\n",
    "- **Force specific tools** to be called\n",
    "- **Require any tool** to be used\n",
    "- **Prevent tool usage** entirely\n",
    "- **Let the model decide** (default behavior)\n",
    "\n",
    "### Tool Choice Options:\n",
    "\n",
    "1. **`None` or `\"auto\"`** (default): Model decides whether to use tools\n",
    "2. **`\"any\"`**: Model must use one of the provided tools  \n",
    "3. **`\"none\"`**: Model is prevented from using tools\n",
    "4. **`str` (tool name)**: Force the model to use a specific tool\n",
    "5. **`dict`**: Provider-specific format for advanced control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: Force a Specific Tool\n",
    "\n",
    "When you want to guarantee that a specific tool is used, regardless of the user's input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:33,526 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Forced Tool Usage ===\n",
      "Provider: anthropic\n",
      "\n",
      "üìù Content:\n",
      "\n",
      "üîß Tool calls (forced):\n",
      "  Tool: get_weather (forced to use weather tool)\n",
      "  Arguments: {'location': '<UNKNOWN>'}\n",
      "\n",
      "‚úÖ The model was forced to use the weather tool even though the user asked about math!\n",
      "‚ú® Using clean syntax: tools=[weather_tool, calculator_tool, time_tool]\n"
     ]
    }
   ],
   "source": [
    "# Force the model to use the weather tool\n",
    "llm_config_force_weather = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    tools=[weather_tool, calculator_tool, time_tool],  # ‚ú® Simplified list syntax\n",
    "    tool_choice=\"get_weather\"  # Force this specific tool\n",
    ")\n",
    "\n",
    "client_force = Client(llm_config_force_weather)\n",
    "\n",
    "# Even though we ask about math, the model will be forced to use the weather tool\n",
    "response = await client_force.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello! Can you help me with some math?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Forced Tool Usage ===\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"\\nüìù Content:\")\n",
    "for content in response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool calls (forced):\")\n",
    "for tool_use in response.tool_use:\n",
    "    print(f\"  Tool: {tool_use.name} (forced to use weather tool)\")\n",
    "    print(f\"  Arguments: {tool_use.arguments}\")\n",
    "\n",
    "print(\"\\n‚úÖ The model was forced to use the weather tool even though the user asked about math!\")\n",
    "print(\"‚ú® Using clean syntax: tools=[weather_tool, calculator_tool, time_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Require Any Tool Usage\n",
    "\n",
    "When you want to ensure the model uses one of the available tools, but let it choose which one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:35,540 - v_router.router - INFO - Trying primary model: gpt-4 on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Required Tool Usage ===\n",
      "Provider: openai\n",
      "\n",
      "üìù Content:\n",
      "\n",
      "üîß Tool calls (required):\n",
      "  Tool chosen: get_weather\n",
      "  Arguments: {'location': 'San Francisco, CA'}\n",
      "\n",
      "‚úÖ The model was required to use a tool and chose one that seemed most appropriate!\n",
      "‚ú® Simplified: tools=[weather_tool, calculator_tool, time_tool]\n"
     ]
    }
   ],
   "source": [
    "# Require the model to use any of the available tools\n",
    "llm_config_require_any = LLM(\n",
    "    model_name=\"gpt-4\",\n",
    "    provider=\"openai\",\n",
    "    tools=[weather_tool, calculator_tool, time_tool],  # ‚ú® Clean list syntax\n",
    "    tool_choice=\"any\"  # Must use one of the provided tools\n",
    ")\n",
    "\n",
    "client_require = Client(llm_config_require_any)\n",
    "\n",
    "# Ask a general question - the model will choose an appropriate tool\n",
    "response = await client_require.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hi there! How are you doing today?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Required Tool Usage ===\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"\\nüìù Content:\")\n",
    "for content in response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool calls (required):\")\n",
    "for tool_use in response.tool_use:\n",
    "    print(f\"  Tool chosen: {tool_use.name}\")\n",
    "    print(f\"  Arguments: {tool_use.arguments}\")\n",
    "\n",
    "print(\"\\n‚úÖ The model was required to use a tool and chose one that seemed most appropriate!\")\n",
    "print(\"‚ú® Simplified: tools=[weather_tool, calculator_tool, time_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: Disable Tool Usage\n",
    "\n",
    "When you want to prevent the model from using tools, even if they're available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:37,734 - v_router.router - INFO - Trying primary model: gemini-1.5-pro on google\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Disabled Tool Usage ===\n",
      "Provider: google\n",
      "\n",
      "üìù Content:\n",
      "  50 times 20 is 1000.\n",
      "\n",
      "\n",
      "üîß Tool calls: 0\n",
      "  No tools used (disabled)\n",
      "\n",
      "‚úÖ Tools were disabled, so the model provided a text response instead!\n",
      "‚ú® Clean setup: tools=[weather_tool, calculator_tool, time_tool]\n"
     ]
    }
   ],
   "source": [
    "# Prevent the model from using any tools\n",
    "llm_config_no_tools = LLM(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    provider=\"google\", \n",
    "    tools=[weather_tool, calculator_tool, time_tool],  # ‚ú® Tools available in clean syntax...\n",
    "    tool_choice=\"none\"  # ...but model is prevented from using them\n",
    ")\n",
    "\n",
    "client_no_tools = Client(llm_config_no_tools)\n",
    "\n",
    "# Ask something that would normally trigger a tool call\n",
    "response = await client_no_tools.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What's 50 times 20? Please calculate this for me.\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Disabled Tool Usage ===\")\n",
    "print(f\"Provider: {response.provider}\")\n",
    "print(f\"\\nüìù Content:\")\n",
    "for content in response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool calls: {len(response.tool_use)}\")\n",
    "if response.tool_use:\n",
    "    for tool_use in response.tool_use:\n",
    "        print(f\"  Tool: {tool_use.name}\")\n",
    "else:\n",
    "    print(\"  No tools used (disabled)\")\n",
    "\n",
    "print(\"\\n‚úÖ Tools were disabled, so the model provided a text response instead!\")\n",
    "print(\"‚ú® Clean setup: tools=[weather_tool, calculator_tool, time_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Auto Mode (Default Behavior)\n",
    "\n",
    "This is the default behavior - the model decides whether to use tools based on the context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:38,299 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Auto Mode Test 1: 'Hello! How are you today?' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:41,278 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Response: Hello! I'm doing well, thank you for asking! I'm here and re...\n",
      "üîß Tools used: 0\n",
      "  - No tools (model decided they weren't needed)\n",
      "\n",
      "=== Auto Mode Test 2: 'What's 15 * 32?' ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:44,163 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù Response: I'll calculate 15 * 32 for you....\n",
      "üîß Tools used: 1\n",
      "  - calculator: {'operation': 'multiply', 'a': 15, 'b': 32}\n",
      "\n",
      "=== Auto Mode Test 3: 'What time is it in Tokyo?' ===\n",
      "üìù Response: I'll get the current time in Tokyo for you....\n",
      "üîß Tools used: 1\n",
      "  - get_current_time: {'timezone': 'Asia/Tokyo'}\n",
      "\n",
      "‚úÖ In auto mode, the model intelligently chooses when to use tools!\n",
      "‚ú® Setup: tools=[weather_tool, calculator_tool, time_tool]\n"
     ]
    }
   ],
   "source": [
    "# Default behavior - model decides whether to use tools\n",
    "llm_config_auto = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\", \n",
    "    provider=\"anthropic\",\n",
    "    tools=[weather_tool, calculator_tool, time_tool],  # ‚ú® Simplified syntax\n",
    "    tool_choice=\"auto\"  # Explicit auto mode (same as None or not specified)\n",
    ")\n",
    "\n",
    "client_auto = Client(llm_config_auto)\n",
    "\n",
    "# Test with different types of queries\n",
    "test_queries = [\n",
    "    \"Hello! How are you today?\",  # Likely no tools\n",
    "    \"What's 15 * 32?\",           # Likely calculator tool\n",
    "    \"What time is it in Tokyo?\"  # Likely time tool\n",
    "]\n",
    "\n",
    "for i, query in enumerate(test_queries):\n",
    "    print(f\"\\n=== Auto Mode Test {i+1}: '{query}' ===\")\n",
    "    \n",
    "    response = await client_auto.messages.create(\n",
    "        messages=[{\"role\": \"user\", \"content\": query}]\n",
    "    )\n",
    "    \n",
    "    print(f\"üìù Response: {response.content[0].text[:60]}...\" if response.content else \"No text content\")\n",
    "    print(f\"üîß Tools used: {len(response.tool_use)}\")\n",
    "    \n",
    "    if response.tool_use:\n",
    "        for tool_use in response.tool_use:\n",
    "            print(f\"  - {tool_use.name}: {tool_use.arguments}\")\n",
    "    else:\n",
    "        print(\"  - No tools (model decided they weren't needed)\")\n",
    "\n",
    "print(\"\\n‚úÖ In auto mode, the model intelligently chooses when to use tools!\")\n",
    "print(\"‚ú® Setup: tools=[weather_tool, calculator_tool, time_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: Provider-Specific Format\n",
    "\n",
    "For advanced use cases, you can use provider-specific tool_choice formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:46,079 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Provider-Specific tool_choice Examples ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:48,435 - v_router.router - INFO - Trying primary model: gpt-4 on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Anthropic with provider-specific format:\n",
      "  Tools used: 1\n",
      "  Tool: calculator\n",
      "\n",
      "üü† OpenAI with provider-specific format:\n",
      "  Tools used: 1\n",
      "  Tool: calculator\n",
      "\n",
      "‚úÖ Provider-specific formats give you maximum control!\n",
      "‚ú® Both using simplified: tools=[calculator_tool]\n"
     ]
    }
   ],
   "source": [
    "# Provider-specific format examples\n",
    "print(\"=== Provider-Specific tool_choice Examples ===\\n\")\n",
    "\n",
    "# Anthropic-specific format\n",
    "llm_anthropic_specific = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\", \n",
    "    tools=[calculator_tool],  # ‚ú® Clean list syntax\n",
    "    tool_choice={\"type\": \"tool\", \"name\": \"calculator\"}  # Anthropic-specific format\n",
    ")\n",
    "\n",
    "client_anthropic = Client(llm_anthropic_specific)\n",
    "\n",
    "response = await client_anthropic.messages.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"I need help with something\"}]\n",
    ")\n",
    "\n",
    "print(\"üîµ Anthropic with provider-specific format:\")\n",
    "print(f\"  Tools used: {len(response.tool_use)}\")\n",
    "if response.tool_use:\n",
    "    print(f\"  Tool: {response.tool_use[0].name}\")\n",
    "\n",
    "# OpenAI-specific format  \n",
    "llm_openai_specific = LLM(\n",
    "    model_name=\"gpt-4\",\n",
    "    provider=\"openai\",\n",
    "    tools=[calculator_tool],  # ‚ú® Clean list syntax\n",
    "    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"calculator\"}}  # OpenAI-specific format\n",
    ")\n",
    "\n",
    "client_openai = Client(llm_openai_specific)\n",
    "\n",
    "response = await client_openai.messages.create(\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Help me please\"}]\n",
    ")\n",
    "\n",
    "print(\"\\nüü† OpenAI with provider-specific format:\")\n",
    "print(f\"  Tools used: {len(response.tool_use)}\")\n",
    "if response.tool_use:\n",
    "    print(f\"  Tool: {response.tool_use[0].name}\")\n",
    "\n",
    "print(\"\\n‚úÖ Provider-specific formats give you maximum control!\")\n",
    "print(\"‚ú® Both using simplified: tools=[calculator_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tool Choice with Fallbacks\n",
    "\n",
    "The `tool_choice` parameter works seamlessly with fallback models - backup models inherit the tool choice behavior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:50,292 - v_router.router - INFO - Trying primary model: claude-nonexistent-model on anthropic\u001b[0m\n",
      "\u001b[33m2025-06-09 01:37:50,581 - v_router.router - WARNING - Primary model failed: Error code: 404 - {'type': 'error', 'error': {'type': 'not_found_error', 'message': 'model: claude-nonexistent-model'}}\u001b[0m\n",
      "\u001b[32m2025-06-09 01:37:50,582 - v_router.router - INFO - Trying backup model: gpt-4 on openai\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Tool Choice with Fallbacks ===\n",
      "Provider used: openai (fallback)\n",
      "Model used: gpt-4-0613\n",
      "\n",
      "üìù Content:\n",
      "\n",
      "üîß Tool calls (inherited forced behavior):\n",
      "  Tool: calculator (forced via inheritance)\n",
      "  Arguments: {'operation': 'add', 'a': 5, 'b': 10}\n",
      "\n",
      "‚úÖ Backup models inherit tool_choice behavior from the primary configuration!\n",
      "‚ú® Primary config: tools=[calculator_tool, weather_tool]\n"
     ]
    }
   ],
   "source": [
    "# Tool choice with fallbacks\n",
    "llm_with_fallback = LLM(\n",
    "    model_name=\"claude-nonexistent-model\",  # This will fail\n",
    "    provider=\"anthropic\",\n",
    "    tools=[calculator_tool, weather_tool],  # ‚ú® Simplified list syntax\n",
    "    tool_choice=\"calculator\",  # Force calculator tool\n",
    "    backup_models=[\n",
    "        BackupModel(\n",
    "            model=LLM(\n",
    "                model_name=\"gpt-4\",\n",
    "                provider=\"openai\"\n",
    "                # Note: tools and tool_choice inherited from primary\n",
    "            ),\n",
    "            priority=1\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "client_fallback = Client(llm_with_fallback)\n",
    "\n",
    "response = await client_fallback.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Hello there!\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Tool Choice with Fallbacks ===\")\n",
    "print(f\"Provider used: {response.provider} (fallback)\")\n",
    "print(f\"Model used: {response.model}\")\n",
    "\n",
    "print(f\"\\nüìù Content:\")\n",
    "for content in response.content:\n",
    "    if content.type == \"text\":\n",
    "        print(f\"  {content.text}\")\n",
    "\n",
    "print(f\"\\nüîß Tool calls (inherited forced behavior):\")\n",
    "if response.tool_use:\n",
    "    for tool_use in response.tool_use:\n",
    "        print(f\"  Tool: {tool_use.name} (forced via inheritance)\")\n",
    "        print(f\"  Arguments: {tool_use.arguments}\")\n",
    "else:\n",
    "    print(\"  No tools used\")\n",
    "\n",
    "print(\"\\n‚úÖ Backup models inherit tool_choice behavior from the primary configuration!\")\n",
    "print(\"‚ú® Primary config: tools=[calculator_tool, weather_tool]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-06-09 01:37:52,117 - v_router.router - INFO - Trying primary model: claude-sonnet-4-20250514 on anthropic\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Detailed Response Structure Analysis:\n",
      "\n",
      "1. Basic Response Info:\n",
      "   ‚îú‚îÄ‚îÄ Provider: anthropic\n",
      "   ‚îú‚îÄ‚îÄ Model: claude-sonnet-4-20250514\n",
      "   ‚îî‚îÄ‚îÄ Response Type: Response\n",
      "\n",
      "2. Content Blocks (1 total):\n",
      "   Block 0:\n",
      "   ‚îú‚îÄ‚îÄ Type: text\n",
      "   ‚îú‚îÄ‚îÄ Role: assistant\n",
      "   ‚îî‚îÄ‚îÄ Text: I'll calculate 7 * 8 for you.\n",
      "\n",
      "3. Tool Use Blocks (1 total):\n",
      "   Tool 0:\n",
      "   ‚îú‚îÄ‚îÄ Name: calculator\n",
      "   ‚îú‚îÄ‚îÄ ID: toolu_01RVXahhFHgp7nfkrfSCkNnT\n",
      "   ‚îú‚îÄ‚îÄ Arguments Type: dict\n",
      "   ‚îî‚îÄ‚îÄ Arguments: {'operation': 'multiply', 'a': 7, 'b': 8}\n",
      "\n",
      "4. Usage Information:\n",
      "   ‚îú‚îÄ‚îÄ Input Tokens: 470\n",
      "   ‚îú‚îÄ‚îÄ Output Tokens: 99\n",
      "   ‚îî‚îÄ‚îÄ Total Tokens: 569\n",
      "\n",
      "5. Raw Response:\n",
      "   ‚îú‚îÄ‚îÄ Type: dict\n",
      "   ‚îî‚îÄ‚îÄ Available for provider-specific processing\n",
      "\n",
      "‚úÖ This exact structure works across ALL providers!\n",
      "‚ú® Setup with: tools=[calculator_tool]\n"
     ]
    }
   ],
   "source": [
    "# Simple tool call to examine response structure\n",
    "llm_config = LLM(\n",
    "    model_name=\"claude-sonnet-4-20250514\",\n",
    "    provider=\"anthropic\",\n",
    "    tools=[calculator_tool]  # ‚ú® Clean and simple!\n",
    ")\n",
    "\n",
    "client = Client(llm_config)\n",
    "\n",
    "response = await client.messages.create(\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Calculate 7 * 8\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"üîç Detailed Response Structure Analysis:\")\n",
    "print(f\"\\n1. Basic Response Info:\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Provider: {response.provider}\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Model: {response.model}\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ Response Type: {type(response).__name__}\")\n",
    "\n",
    "print(f\"\\n2. Content Blocks ({len(response.content)} total):\")\n",
    "for i, content in enumerate(response.content):\n",
    "    print(f\"   Block {i}:\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Type: {content.type}\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Role: {content.role}\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ Text: {content.text[:50]}...\" if len(content.text) > 50 else f\"   ‚îî‚îÄ‚îÄ Text: {content.text}\")\n",
    "\n",
    "print(f\"\\n3. Tool Use Blocks ({len(response.tool_use)} total):\")\n",
    "for i, tool_use in enumerate(response.tool_use):\n",
    "    print(f\"   Tool {i}:\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Name: {tool_use.name}\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ ID: {tool_use.id}\")\n",
    "    print(f\"   ‚îú‚îÄ‚îÄ Arguments Type: {type(tool_use.arguments).__name__}\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ Arguments: {tool_use.arguments}\")\n",
    "\n",
    "print(f\"\\n4. Usage Information:\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Input Tokens: {response.usage.input_tokens}\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Output Tokens: {response.usage.output_tokens}\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ Total Tokens: {response.usage.input_tokens + response.usage.output_tokens}\")\n",
    "\n",
    "print(f\"\\n5. Raw Response:\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ Type: {type(response.raw_response).__name__}\")\n",
    "print(f\"   ‚îî‚îÄ‚îÄ Available for provider-specific processing\")\n",
    "\n",
    "print(\"\\n‚úÖ This exact structure works across ALL providers!\")\n",
    "print(\"‚ú® Setup with: tools=[calculator_tool]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "\n",
    "‚úÖ **Unified Tool Interface**: Same tool definitions work across Anthropic, OpenAI, Google, and Azure  \n",
    "‚úÖ **Type-Safe Schemas**: Use Pydantic models for robust tool parameter validation  \n",
    "‚úÖ **Tool Choice Control**: Force specific tools, require any tool, or disable tools entirely  \n",
    "‚úÖ **Automatic Tool Inheritance**: Backup models inherit tools and tool_choice from primary configuration  \n",
    "‚úÖ **Consistent Response Format**: Same tool response structure across all providers  \n",
    "‚úÖ **Complex Tool Support**: Handle nested data structures and multiple parameters  \n",
    "‚úÖ **Full Conversation Flow**: Execute tools and continue conversations with results  \n",
    "‚úÖ **‚ú® NEW: Simplified API**: Use `tools=[tool1, tool2]` instead of `tools=Tools(tools=[tool1, tool2])`\n",
    "\n",
    "### New Simplified Tools API:\n",
    "\n",
    "#### ‚ú® Before vs After:\n",
    "```python\n",
    "# OLD (still works for backward compatibility):\n",
    "tools=Tools(tools=[weather_tool, calculator_tool])\n",
    "\n",
    "# NEW (much cleaner!):\n",
    "tools=[weather_tool, calculator_tool]\n",
    "```\n",
    "\n",
    "Both syntaxes work! The new one is just cleaner and more intuitive.\n",
    "\n",
    "### Tool Request Models:\n",
    "- **`ToolCall`**: Individual tool definition (name, description, schema)\n",
    "- **`Tools`**: Collection of tools (can be created automatically from list)\n",
    "- **`LLM.tools`**: Now accepts both `Tools` object or `List[ToolCall]` \n",
    "- **`LLM.tool_choice`**: Control when and how tools are used\n",
    "- **Pydantic Models**: Type-safe parameter schemas\n",
    "\n",
    "### Tool Response Models:\n",
    "- **`Response.tool_use`**: List of tool calls made by the model\n",
    "- **`ToolUse`**: Individual tool call with name, ID, and arguments\n",
    "- **`Response.content`**: Text content alongside tool calls\n",
    "- **`Response.usage`**: Token usage including tool call tokens\n",
    "\n",
    "### Tool Choice Options:\n",
    "- **`None` or `\"auto\"`**: Model decides whether to use tools (default)\n",
    "- **`\"any\"`**: Model must use one of the provided tools\n",
    "- **`\"none\"`**: Model is prevented from using tools\n",
    "- **`str` (tool name)**: Force model to use a specific tool\n",
    "- **`dict`**: Provider-specific format for advanced control\n",
    "\n",
    "### Advanced Patterns:\n",
    "- **Multi-step Workflows**: Chain tool calls together\n",
    "- **Complex Schemas**: Use nested data structures and optional parameters\n",
    "- **Provider Fallback**: Tools work seamlessly with backup models\n",
    "- **Tool Result Handling**: Full conversation flow with tool execution\n",
    "- **Forced Tool Usage**: Guarantee specific tool behavior for consistent workflows\n",
    "\n",
    "### Next Steps:\n",
    "- Check out `quickstart_models.ipynb` for basic model usage and fallbacks\n",
    "- Explore custom tool implementations for your specific use cases\n",
    "- See the full documentation for advanced tool patterns\n",
    "\n",
    "v-router provides the most comprehensive and unified function calling interface across all major LLM providers!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
