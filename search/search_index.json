{"config":{"lang":["en"],"separator":"[\\s\\u200b\\-_,:!=\\[\\]()\"/<>]","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"v-router","text":"<p>A unified LLM interface that provides automatic fallback between different LLM providers. Route your AI requests seamlessly across Anthropic, OpenAI, Google, and Azure with intelligent failover strategies and a consistent API.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li> <p>\ud83d\ude80 Automatic Fallback: Seamlessly switch between models and providers when failures occur. Configure backup models and cross-provider fallback strategies. Learn more \u2192</p> </li> <li> <p>\ud83d\udd17 Unified API: Same interface works across all major LLM providers. Write once, run anywhere with consistent request/response formats. Explore the API \u2192</p> </li> <li> <p>\u26a1 Smart Routing: Intelligent model selection based on availability and configuration. Automatic model name mapping across providers. See provider configuration \u2192</p> </li> <li> <p>\ud83d\udee0\ufe0f Function Calling: Unified tool calling interface across all providers. Use the same function definitions everywhere. Control tool usage with force, disable, or auto modes. Function calling guide \u2192</p> </li> <li> <p>\ud83d\uddbc\ufe0f Multimodal Support: Send images and PDFs with automatic format conversion. Support for vision models across all providers. Multimodal examples \u2192</p> </li> <li> <p>\u2699\ufe0f Flexible Configuration: Fine-tune parameters, backup models, and provider priorities. Extensive customization options. Configuration guide \u2192</p> </li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from v_router import Client, LLM\n\n# Create an LLM configuration with automatic fallback\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    backup_models=[\n        {\"model\": LLM(model_name=\"gpt-4o\", provider=\"openai\"), \"priority\": 1},\n        {\"model\": LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"), \"priority\": 2}\n    ]\n)\n\n# Create a client\nclient = Client(llm_config)\n\n# Send a message - automatically falls back if primary model fails\nresponse = await client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in one sentence.\"}\n    ]\n)\n\nprint(f\"Response: {response.content[0].text}\")\nprint(f\"Model used: {response.model} ({response.provider})\")\n</code></pre>"},{"location":"#supported-providers","title":"Supported Providers","text":"Provider Models Features Anthropic Claude 3 (Opus, Sonnet, Haiku), Claude 4 (Opus, Sonnet) Function calling, Images, PDFs OpenAI GPT-4, GPT-4 Turbo, GPT-4.1, GPT-3.5 Function calling, Images Google AI Gemini Pro, Gemini 1.5 (Pro, Flash), Gemini 2.0 Flash Function calling, Images, PDFs Azure OpenAI GPT-4, GPT-4 Turbo, GPT-4.1, GPT-3.5 Function calling, Images Vertex AI Claude 3/4 &amp; Gemini models via Google Cloud Function calling, Images, PDFs"},{"location":"#why-v-router","title":"Why v-router?","text":""},{"location":"#reliability-through-redundancy","title":"Reliability Through Redundancy","text":"<p>Never worry about API outages or rate limits again. v-router automatically routes requests to backup models and alternative providers when failures occur.</p>"},{"location":"#simplified-integration","title":"Simplified Integration","text":"<p>Write your code once and deploy it anywhere. The unified interface abstracts away provider-specific differences while maintaining full feature parity.</p>"},{"location":"#cost-optimization","title":"Cost Optimization","text":"<p>Easily switch between providers based on cost, performance, or availability. Configure fallback strategies that optimize for your specific use case.</p>"},{"location":"#future-proof-architecture","title":"Future-Proof Architecture","text":"<p>Add new providers and models without changing your application code. The modular architecture makes it easy to extend and customize.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to get started? Follow our quick start guide:</p> <p>Get Started View Examples</p>"},{"location":"#development-roadmap","title":"Development Roadmap","text":"<ul> <li>[x] Chat Completions: Unified interface across providers </li> <li>[x] Function Calling: Tool calling support with force/disable/auto modes</li> <li>[x] Multimodal Support: Images, PDFs, and document processing</li> <li>[ ] Streaming: Real-time response streaming</li> <li>[ ] AWS Bedrock: Additional provider support</li> <li>[ ] JSON Mode: Structured output generation</li> <li>[ ] Prompt Caching: Optimization for repeated prompts</li> <li>[ ] Ollama Support: Local model integration</li> </ul>"},{"location":"#community","title":"Community","text":"<ul> <li>Repository: GitHub</li> <li>Package: PyPI</li> <li>Issues: GitHub Issues</li> <li>Email: ben@vectrix.ai</li> </ul> <p>v-router - Making LLM integration simple, reliable, and unified across all providers.</p>"},{"location":"api/client/","title":"Client API","text":"<p>The <code>Client</code> class is the main entry point for v-router. It provides a unified interface for working with multiple LLM providers and handles automatic fallback when primary models fail.</p>"},{"location":"api/client/#client-class","title":"Client Class","text":"<p>::: v_router.Client     handler: python     options:       show_source: false       show_root_heading: true       show_root_toc_entry: true</p>"},{"location":"api/client/#constructor","title":"Constructor","text":"<pre><code>from v_router import Client, LLM\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    max_tokens=1000\n)\n\nclient = Client(llm_config)\n</code></pre> <p>Parameters:</p> <ul> <li><code>llm_config</code> (LLM): The LLM configuration object defining the model, provider, and parameters</li> <li><code>**provider_kwargs</code>: Additional keyword arguments passed to the provider</li> </ul>"},{"location":"api/client/#messages-api","title":"Messages API","text":"<p>The <code>messages</code> attribute provides access to the Messages API, which handles chat completions:</p>"},{"location":"api/client/#messagescreate","title":"messages.create()","text":"<p>Create a chat completion with automatic fallback support.</p> <pre><code>response = await client.messages.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ],\n    **kwargs\n)\n</code></pre> <p>Parameters:</p> <ul> <li><code>messages</code> (List[Dict[str, str]]): List of message dictionaries with 'role' and 'content' keys</li> <li><code>**kwargs</code>: Additional parameters passed to the provider (e.g., temperature override)</li> </ul> <p>Returns:</p> <ul> <li><code>Response</code>: Unified response object with content, usage, and metadata</li> </ul>"},{"location":"api/client/#usage-examples","title":"Usage Examples","text":""},{"location":"api/client/#basic-usage","title":"Basic Usage","text":"<pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def basic_example():\n    llm_config = LLM(\n        model_name=\"gpt-4o\",\n        provider=\"openai\",\n        max_tokens=500,\n        temperature=0.7\n    )\n\n    client = Client(llm_config)\n\n    response = await client.messages.create(\n        messages=[\n            {\"role\": \"user\", \"content\": \"Explain machine learning in simple terms\"}\n        ]\n    )\n\n    print(f\"Response: {response.content[0].text}\")\n    print(f\"Model used: {response.model}\")\n    print(f\"Tokens: {response.usage.total_tokens}\")\n\nasyncio.run(basic_example())\n</code></pre>"},{"location":"api/client/#with-automatic-fallback","title":"With Automatic Fallback","text":"<pre><code>from v_router import Client, LLM, BackupModel\n\nasync def fallback_example():\n    llm_config = LLM(\n        model_name=\"claude-6\",  # This model doesn't exist yet\n        provider=\"anthropic\",\n        backup_models=[\n            BackupModel(\n                model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                priority=1\n            ),\n            BackupModel(\n                model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n                priority=2\n            )\n        ]\n    )\n\n    client = Client(llm_config)\n\n    # Will automatically fallback to GPT-4o when Claude-6 fails\n    response = await client.messages.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n\n    print(f\"Successfully used: {response.model} from {response.provider}\")\n\nasyncio.run(fallback_example())\n</code></pre>"},{"location":"api/client/#function-calling","title":"Function Calling","text":"<pre><code>from v_router import Client, LLM\nfrom v_router.classes.tools import ToolCall, Tools\nfrom pydantic import BaseModel, Field\n\nclass WeatherQuery(BaseModel):\n    location: str = Field(..., description=\"City and state\")\n    units: str = Field(\"fahrenheit\", description=\"Temperature units\")\n\nasync def function_calling_example():\n    weather_tool = ToolCall(\n        name=\"get_weather\",\n        description=\"Get weather information\",\n        input_schema=WeatherQuery.model_json_schema()\n    )\n\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        tools=Tools(tools=[weather_tool])\n    )\n\n    client = Client(llm_config)\n\n    response = await client.messages.create(\n        messages=[{\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}]\n    )\n\n    if response.tool_use:\n        for tool_call in response.tool_use:\n            print(f\"Tool: {tool_call.name}\")\n            print(f\"Arguments: {tool_call.arguments}\")\n\nasyncio.run(function_calling_example())\n</code></pre>"},{"location":"api/client/#multimodal-content","title":"Multimodal Content","text":"<pre><code>async def multimodal_example():\n    client = Client(\n        LLM(model_name=\"claude-sonnet-4\", provider=\"anthropic\")\n    )\n\n    # Send image by file path (automatically converted)\n    response = await client.messages.create(\n        messages=[\n            {\"role\": \"user\", \"content\": \"/path/to/image.jpg\"},\n            {\"role\": \"user\", \"content\": \"Describe this image\"}\n        ]\n    )\n\n    print(response.content[0].text)\n\nasyncio.run(multimodal_example())\n</code></pre>"},{"location":"api/client/#provider-specific-configuration","title":"Provider-Specific Configuration","text":"<pre><code>async def provider_config_example():\n    # OpenAI with custom parameters\n    openai_config = LLM(\n        model_name=\"gpt-4o\",\n        provider=\"openai\",\n        max_tokens=1000,\n        temperature=0.8,\n        openai_kwargs={\n            \"presence_penalty\": 0.1,\n            \"frequency_penalty\": 0.1\n        }\n    )\n\n    # Anthropic with custom parameters\n    anthropic_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        anthropic_kwargs={\n            \"stream\": False,\n            \"extra_headers\": {\"Custom-Header\": \"value\"}\n        }\n    )\n\n    # Use either configuration\n    client = Client(openai_config)\n    response = await client.messages.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n\nasyncio.run(provider_config_example())\n</code></pre>"},{"location":"api/client/#error-handling","title":"Error Handling","text":"<p>The Client automatically handles many error scenarios through fallback mechanisms, but you can also implement your own error handling:</p> <pre><code>async def error_handling_example():\n    client = Client(\n        LLM(model_name=\"claude-sonnet-4\", provider=\"anthropic\")\n    )\n\n    try:\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n        )\n        print(f\"Success: {response.content[0].text}\")\n\n    except Exception as e:\n        print(f\"All providers failed: {e}\")\n        # Handle the error appropriately\n\nasyncio.run(error_handling_example())\n</code></pre>"},{"location":"api/client/#best-practices","title":"Best Practices","text":""},{"location":"api/client/#configuration-management","title":"Configuration Management","text":"<pre><code># Use environment-specific configurations\ndef get_production_config():\n    return LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        max_tokens=2000,\n        temperature=0.3,  # Lower temperature for production\n        backup_models=[\n            BackupModel(\n                model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                priority=1\n            )\n        ]\n    )\n\ndef get_development_config():\n    return LLM(\n        model_name=\"gpt-3.5\",  # Cheaper for development\n        provider=\"openai\",\n        max_tokens=500,\n        temperature=0.7\n    )\n</code></pre>"},{"location":"api/client/#resource-management","title":"Resource Management","text":"<pre><code># Use context managers for resource cleanup\nasync def resource_management_example():\n    async with Client(llm_config) as client:\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n        )\n        return response\n</code></pre>"},{"location":"api/client/#monitoring-and-logging","title":"Monitoring and Logging","text":"<pre><code>import logging\nfrom v_router import setup_logger\n\n# Enable detailed logging\nsetup_logger(\"v_router\", level=logging.INFO)\n\nasync def monitored_request():\n    client = Client(llm_config)\n\n    response = await client.messages.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\n\n    # Log usage for monitoring\n    logging.info(f\"Request completed: {response.model}, tokens: {response.usage.total_tokens}\")\n\n    return response\n</code></pre>"},{"location":"api/client/#related-documentation","title":"Related Documentation","text":"<ul> <li>LLM Configuration: Configure models and providers</li> <li>Messages API: Message format and options</li> <li>Response Format: Understanding response objects</li> <li>Function Calling Guide: Using tools with the client</li> <li>Provider Configuration: Provider-specific settings</li> </ul>"},{"location":"api/llm/","title":"LLM Configuration","text":"<p>The <code>LLM</code> class defines the configuration for Large Language Models in v-router. It specifies which model to use, the provider, generation parameters, and fallback strategies.</p>"},{"location":"api/llm/#llm-class","title":"LLM Class","text":"<pre><code>from v_router import LLM\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    max_tokens=1000,\n    temperature=0.7\n)\n</code></pre>"},{"location":"api/llm/#parameters","title":"Parameters","text":""},{"location":"api/llm/#required-parameters","title":"Required Parameters","text":"<p><code>model_name</code> (str) Name of the LLM model to use. v-router automatically maps generic names to provider-specific versions.</p> <p>Examples: - <code>\"claude-sonnet-4\"</code> \u2192 <code>\"claude-sonnet-4-20250514\"</code> (Anthropic) or <code>\"claude-sonnet-4@20250514\"</code> (Vertex AI) - <code>\"gpt-4o\"</code> \u2192 <code>\"gpt-4o\"</code> (OpenAI/Azure) - <code>\"gemini-1.5-pro\"</code> \u2192 <code>\"gemini-1.5-pro-latest\"</code> (Google/Vertex AI)</p> <p><code>provider</code> (Literal[\"openai\", \"anthropic\", \"azure\", \"google\", \"vertexai\"]) The LLM provider to use:</p> <ul> <li><code>\"openai\"</code>: OpenAI API</li> <li><code>\"anthropic\"</code>: Anthropic API</li> <li><code>\"azure\"</code>: Azure OpenAI Service</li> <li><code>\"google\"</code>: Google AI Studio</li> <li><code>\"vertexai\"</code>: Google Cloud Vertex AI (supports both Claude and Gemini models)</li> </ul>"},{"location":"api/llm/#optional-parameters","title":"Optional Parameters","text":"<p><code>max_tokens</code> (Optional[int] = None) Maximum number of tokens to generate. If not specified, uses the model's default maximum.</p> <p><code>temperature</code> (Optional[float] = 0) Sampling temperature (0-1). Higher values make output more random, lower values more deterministic.</p> <p><code>backup_models</code> (List[BackupModel] = []) List of backup models to try if the primary model fails. See BackupModel below.</p> <p><code>try_other_providers</code> (bool = False) If True, automatically tries the same model on other providers if the primary provider fails.</p> <p><code>tools</code> (Optional[Tools] = None) Function calling tools available to the model. See Tools documentation.</p> <p><code>tool_choice</code> (Optional[Union[str, Dict[str, Any]]] = None) Controls how the model uses tools. Options:</p> <ul> <li><code>None</code> or <code>\"auto\"</code>: Model decides whether to use tools (default)</li> <li><code>\"any\"</code>: Model must use one of the provided tools</li> <li><code>\"none\"</code>: Model is prevented from using tools</li> <li><code>str</code> (tool name): Force the model to use a specific tool</li> <li><code>dict</code>: Provider-specific format for advanced control</li> </ul>"},{"location":"api/llm/#provider-specific-parameters","title":"Provider-Specific Parameters","text":"<p><code>anthropic_kwargs</code> (dict = {}) Additional parameters passed to the Anthropic provider:</p> <pre><code>LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    anthropic_kwargs={\n        \"stream\": False,\n        \"extra_headers\": {\"Custom-Header\": \"value\"}\n    }\n)\n</code></pre> <p><code>openai_kwargs</code> (dict = {}) Additional parameters passed to OpenAI provider:</p> <pre><code>LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    openai_kwargs={\n        \"presence_penalty\": 0.1,\n        \"frequency_penalty\": 0.1,\n        \"logit_bias\": {},\n        \"user\": \"user-123\"\n    }\n)\n</code></pre> <p><code>google_kwargs</code> (dict = {}) Additional parameters passed to Google providers:</p> <pre><code>LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\",\n    google_kwargs={\n        \"safety_settings\": {\n            \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_MEDIUM_AND_ABOVE\"\n        },\n        \"generation_config\": {\n            \"candidate_count\": 1\n        }\n    }\n)\n</code></pre>"},{"location":"api/llm/#backupmodel-class","title":"BackupModel Class","text":"<p>Defines a backup model with priority ordering:</p> <pre><code>from v_router import BackupModel, LLM\n\nbackup = BackupModel(\n    model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n    priority=1  # Lower numbers = higher priority\n)\n</code></pre>"},{"location":"api/llm/#parameters_1","title":"Parameters","text":"<p><code>model</code> (LLM) The backup LLM configuration to use.</p> <p><code>priority</code> (int) Priority level (1 = highest priority). Must be unique across all backup models.</p>"},{"location":"api/llm/#usage-examples","title":"Usage Examples","text":""},{"location":"api/llm/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from v_router import LLM\n\n# Simple configuration\nbasic_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\"\n)\n\n# With generation parameters\ntuned_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    max_tokens=2000,\n    temperature=0.8,\n    system_prompt=\"You are a creative writing assistant\"\n)\n</code></pre>"},{"location":"api/llm/#fallback-configuration","title":"Fallback Configuration","text":"<pre><code>from v_router import LLM, BackupModel\n\n# Multiple backup models\nrobust_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    backup_models=[\n        BackupModel(\n            model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n            priority=1\n        ),\n        BackupModel(\n            model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n            priority=2\n        ),\n        BackupModel(\n            model=LLM(model_name=\"gpt-4\", provider=\"azure\"),\n            priority=3\n        )\n    ],\n    try_other_providers=True  # Also try same model on other providers\n)\n</code></pre>"},{"location":"api/llm/#cross-provider-configuration","title":"Cross-Provider Configuration","text":"<pre><code># Try the same model across multiple providers\ncross_provider_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"vertexai\",  # Try Vertex AI first\n    try_other_providers=True,  # Fall back to Anthropic if needed\n    max_tokens=1000\n)\n</code></pre>"},{"location":"api/llm/#function-calling-configuration","title":"Function Calling Configuration","text":"<pre><code>from v_router.classes.tools import ToolCall, Tools\nfrom pydantic import BaseModel, Field\n\nclass WeatherQuery(BaseModel):\n    location: str = Field(..., description=\"City name\")\n\nweather_tool = ToolCall(\n    name=\"get_weather\",\n    description=\"Get weather information\",\n    input_schema=WeatherQuery.model_json_schema()\n)\n\nfunction_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=Tools(tools=[weather_tool]),\n    tool_choice=\"auto\"  # Optional: control tool usage\n)\n</code></pre>"},{"location":"api/llm/#tool-choice-configuration","title":"Tool Choice Configuration","text":"<p>Control when and how tools are used:</p> <pre><code># Force a specific tool\nforce_weather = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\", \n    tools=Tools(tools=[weather_tool, calculator_tool]),\n    tool_choice=\"get_weather\"  # Force weather tool\n)\n\n# Require any tool usage\nrequire_tools = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    tools=Tools(tools=[weather_tool, calculator_tool]),\n    tool_choice=\"any\"  # Must use one of the tools\n)\n\n# Disable tool usage\nno_tools = LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\",\n    tools=Tools(tools=[weather_tool, calculator_tool]),\n    tool_choice=\"none\"  # Prevent tool usage\n)\n\n# Provider-specific format (Anthropic)\nanthropic_specific = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=Tools(tools=[calculator_tool]),\n    tool_choice={\"type\": \"tool\", \"name\": \"calculator\"}\n)\n\n# Provider-specific format (OpenAI)\nopenai_specific = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\", \n    tools=Tools(tools=[calculator_tool]),\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"calculator\"}}\n)\n</code></pre>"},{"location":"api/llm/#model-name-mapping","title":"Model Name Mapping","text":"<p>v-router automatically maps generic model names to provider-specific versions based on <code>models.yml</code>:</p>"},{"location":"api/llm/#anthropic-models","title":"Anthropic Models","text":"Generic Name Anthropic API Vertex AI <code>claude-3-opus</code> <code>claude-3-opus-20240229</code> <code>claude-3-opus@20240229</code> <code>claude-3-sonnet</code> <code>claude-3-sonnet-20240229</code> <code>claude-3-sonnet@20240229</code> <code>claude-3-haiku</code> <code>claude-3-haiku-20240307</code> <code>claude-3-haiku@20240307</code> <code>claude-sonnet-4</code> <code>claude-sonnet-4-20250514</code> <code>claude-sonnet-4@20250514</code> <code>claude-opus-4</code> <code>claude-opus-4-20250514</code> <code>claude-opus-4@20250514</code>"},{"location":"api/llm/#openai-models","title":"OpenAI Models","text":"Generic Name OpenAI/Azure API <code>gpt-4</code> <code>gpt-4</code> <code>gpt-4-turbo</code> <code>gpt-4-turbo-preview</code> <code>gpt-4.1</code> <code>gpt-4.1</code> <code>gpt-3.5</code> <code>gpt-3.5-turbo</code>"},{"location":"api/llm/#google-models","title":"Google Models","text":"Generic Name Google AI/Vertex API <code>gemini-pro</code> <code>gemini-1.0-pro</code> <code>gemini-1.5-pro</code> <code>gemini-1.5-pro-latest</code> <code>gemini-1.5-flash</code> <code>gemini-1.5-flash-latest</code> <code>gemini-2.0-flash</code> <code>gemini-2.0-flash-001</code>"},{"location":"api/llm/#validation","title":"Validation","text":"<p>The LLM class includes validation for configuration parameters:</p>"},{"location":"api/llm/#priority-validation","title":"Priority Validation","text":"<pre><code># \u274c This will raise an error (duplicate priorities)\nLLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    backup_models=[\n        BackupModel(model=LLM(model_name=\"gpt-4o\", provider=\"openai\"), priority=1),\n        BackupModel(model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"), priority=1)  # Duplicate!\n    ]\n)\n\n# \u2705 This is correct (unique priorities)\nLLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    backup_models=[\n        BackupModel(model=LLM(model_name=\"gpt-4o\", provider=\"openai\"), priority=1),\n        BackupModel(model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"), priority=2)\n    ]\n)\n</code></pre>"},{"location":"api/llm/#parameter-validation","title":"Parameter Validation","text":"<pre><code># \u274c Invalid priority (must be &gt;= 1)\nBackupModel(\n    model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n    priority=0  # Error!\n)\n\n# \u274c Invalid temperature (must be 0-1)\nLLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    temperature=2.0  # Error!\n)\n</code></pre>"},{"location":"api/llm/#methods","title":"Methods","text":""},{"location":"api/llm/#get_ordered_backup_models","title":"get_ordered_backup_models()","text":"<p>Returns backup models sorted by priority (lowest number first):</p> <pre><code>config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    backup_models=[\n        BackupModel(model=LLM(model_name=\"gpt-4o\", provider=\"openai\"), priority=2),\n        BackupModel(model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"), priority=1)\n    ]\n)\n\nordered_backups = config.get_ordered_backup_models()\n# Returns: [gemini-1.5-pro (priority 1), gpt-4o (priority 2)]\n</code></pre>"},{"location":"api/llm/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"api/llm/#environment-specific-configuration","title":"Environment-Specific Configuration","text":"<pre><code>import os\n\ndef get_llm_config(environment: str) -&gt; LLM:\n    if environment == \"production\":\n        return LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\",\n            max_tokens=2000,\n            temperature=0.3,  # More deterministic for production\n            backup_models=[\n                BackupModel(\n                    model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                    priority=1\n                )\n            ]\n        )\n    else:\n        return LLM(\n            model_name=\"gpt-3.5\",  # Cheaper for development\n            provider=\"openai\",\n            max_tokens=500,\n            temperature=0.7\n        )\n\nconfig = get_llm_config(os.getenv(\"ENVIRONMENT\", \"development\"))\n</code></pre>"},{"location":"api/llm/#dynamic-model-selection","title":"Dynamic Model Selection","text":"<pre><code>def get_model_for_task(task_type: str) -&gt; LLM:\n    if task_type == \"creative_writing\":\n        return LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\",\n            temperature=0.9,  # High creativity\n            max_tokens=3000\n        )\n    elif task_type == \"code_analysis\":\n        return LLM(\n            model_name=\"gpt-4o\",\n            provider=\"openai\",\n            temperature=0.1,  # Low creativity, high precision\n            max_tokens=2000\n        )\n    elif task_type == \"data_analysis\":\n        return LLM(\n            model_name=\"gemini-1.5-pro\",\n            provider=\"google\",\n            temperature=0.3,\n            max_tokens=4000\n        )\n    else:\n        # Default configuration\n        return LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\"\n        )\n</code></pre>"},{"location":"api/llm/#best-practices","title":"Best Practices","text":""},{"location":"api/llm/#configuration-management","title":"Configuration Management","text":"<ol> <li>Use backup models for production systems</li> <li>Set appropriate max_tokens to control costs</li> <li>Tune temperature based on use case</li> <li>Enable cross-provider fallback for critical applications</li> </ol>"},{"location":"api/llm/#security","title":"Security","text":"<ol> <li>Never hardcode API keys in LLM configurations</li> <li>Use environment variables for sensitive parameters</li> <li>Validate all inputs before creating configurations</li> </ol>"},{"location":"api/llm/#performance","title":"Performance","text":"<ol> <li>Choose appropriate models for your use case</li> <li>Use faster models for backup when appropriate</li> <li>Monitor token usage and adjust limits accordingly</li> </ol>"},{"location":"api/llm/#related-documentation","title":"Related Documentation","text":"<ul> <li>Client API: Using LLM configurations with the client</li> <li>Function Calling: Adding tools to LLM configurations</li> <li>Provider Configuration: Provider-specific settings</li> <li>Examples: Real-world configuration examples</li> </ul>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>This page provides basic examples to help you get started with v-router. These examples cover the most common use cases and patterns.</p>"},{"location":"examples/basic/#simple-chat-completion","title":"Simple Chat Completion","text":"<p>The most basic use case - sending a message and getting a response:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def simple_chat():\n    # Configure LLM\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        max_tokens=500\n    )\n\n    # Create client\n    client = Client(llm_config)\n\n    # Send message\n    response = await client.messages.create(\n        messages=[\n            {\"role\": \"user\", \"content\": \"What is machine learning?\"}\n        ]\n    )\n\n    print(f\"Response: {response.content[0].text}\")\n    print(f\"Model: {response.model}\")\n    print(f\"Tokens used: {response.usage.total_tokens}\")\n\n# Run the example\nasyncio.run(simple_chat())\n</code></pre>"},{"location":"examples/basic/#multi-turn-conversation","title":"Multi-Turn Conversation","text":"<p>Building a conversation with multiple exchanges:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def conversation_example():\n    client = Client(\n        LLM(model_name=\"gpt-4o\", provider=\"openai\")\n    )\n\n    # Start conversation\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful Python tutor.\"},\n        {\"role\": \"user\", \"content\": \"How do I create a list in Python?\"}\n    ]\n\n    # First response\n    response = await client.messages.create(messages=messages)\n    print(f\"Assistant: {response.content[0].text}\")\n\n    # Add assistant response to conversation\n    messages.append({\n        \"role\": \"assistant\", \n        \"content\": response.content[0].text\n    })\n\n    # Continue conversation\n    messages.append({\n        \"role\": \"user\", \n        \"content\": \"Can you show me how to add items to it?\"\n    })\n\n    # Second response\n    response = await client.messages.create(messages=messages)\n    print(f\"Assistant: {response.content[0].text}\")\n\nasyncio.run(conversation_example())\n</code></pre>"},{"location":"examples/basic/#provider-comparison","title":"Provider Comparison","text":"<p>Try the same request with different providers:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def compare_providers():\n    prompt = \"Explain quantum computing in exactly 50 words.\"\n\n    providers = [\n        (\"claude-sonnet-4\", \"anthropic\"),\n        (\"gpt-4o\", \"openai\"),\n        (\"gemini-1.5-pro\", \"google\")\n    ]\n\n    for model_name, provider in providers:\n        client = Client(\n            LLM(\n                model_name=model_name,\n                provider=provider,\n                max_tokens=100\n            )\n        )\n\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        print(f\"\\n{provider.upper()} ({response.model}):\")\n        print(response.content[0].text)\n        print(f\"Tokens: {response.usage.total_tokens}\")\n\nasyncio.run(compare_providers())\n</code></pre>"},{"location":"examples/basic/#temperature-and-creativity-control","title":"Temperature and Creativity Control","text":"<p>Comparing different temperature settings:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def temperature_comparison():\n    prompt = \"Write a short creative story about a robot.\"\n\n    temperatures = [0.1, 0.5, 0.9]\n\n    for temp in temperatures:\n        client = Client(\n            LLM(\n                model_name=\"claude-sonnet-4\",\n                provider=\"anthropic\",\n                temperature=temp,\n                max_tokens=200\n            )\n        )\n\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        print(f\"\\n--- Temperature: {temp} ---\")\n        print(response.content[0].text)\n\nasyncio.run(temperature_comparison())\n</code></pre>"},{"location":"examples/basic/#automatic-fallback-example","title":"Automatic Fallback Example","text":"<p>Demonstrating automatic fallback when the primary model fails:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM, BackupModel\n\nasync def fallback_example():\n    # Configure with a model that doesn't exist as primary\n    llm_config = LLM(\n        model_name=\"claude-ultra-5\",  # This doesn't exist (yet!)\n        provider=\"anthropic\",\n        backup_models=[\n            BackupModel(\n                model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                priority=1\n            ),\n            BackupModel(\n                model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n                priority=2\n            )\n        ]\n    )\n\n    client = Client(llm_config)\n\n    try:\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello! What model are you?\"}]\n        )\n\n        print(f\"Success! Used: {response.model} from {response.provider}\")\n        print(f\"Response: {response.content[0].text}\")\n\n    except Exception as e:\n        print(f\"All providers failed: {e}\")\n\nasyncio.run(fallback_example())\n</code></pre>"},{"location":"examples/basic/#cross-provider-model-testing","title":"Cross-Provider Model Testing","text":"<p>Test the same model across different providers:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def cross_provider_test():\n    model_name = \"claude-sonnet-4\"\n    providers = [\"anthropic\", \"vertexai\"]  # Same model, different providers\n\n    prompt = \"Explain the benefits of using multiple LLM providers.\"\n\n    for provider in providers:\n        try:\n            client = Client(\n                LLM(model_name=model_name, provider=provider)\n            )\n\n            response = await client.messages.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n\n            print(f\"\\n{provider.upper()}:\")\n            print(f\"Model: {response.model}\")\n            print(f\"Response: {response.content[0].text[:200]}...\")\n\n        except Exception as e:\n            print(f\"{provider} failed: {e}\")\n\nasyncio.run(cross_provider_test())\n</code></pre>"},{"location":"examples/basic/#system-prompts-and-personas","title":"System Prompts and Personas","text":"<p>Using system prompts to create different AI personas:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def persona_examples():\n    personas = {\n        \"helpful_assistant\": \"You are a helpful and friendly assistant.\",\n        \"expert_programmer\": \"You are an expert programmer who explains code clearly and concisely.\",\n        \"creative_writer\": \"You are a creative writer who tells engaging stories with vivid descriptions.\",\n        \"data_scientist\": \"You are a data scientist who explains complex concepts in simple terms.\"\n    }\n\n    question = \"How would you approach solving a complex problem?\"\n\n    for persona_name, system_prompt in personas.items():\n        client = Client(\n            LLM(\n                model_name=\"gpt-4o\",\n                provider=\"openai\",\n                max_tokens=150\n            )\n        )\n\n        response = await client.messages.create(\n            messages=[\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": question}\n            ]\n        )\n\n        print(f\"\\n--- {persona_name.replace('_', ' ').title()} ---\")\n        print(response.content[0].text)\n\nasyncio.run(persona_examples())\n</code></pre>"},{"location":"examples/basic/#token-usage-monitoring","title":"Token Usage Monitoring","text":"<p>Monitor and control token usage across requests:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def token_monitoring():\n    client = Client(\n        LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\",\n            max_tokens=100  # Limit response length\n        )\n    )\n\n    requests = [\n        \"What is AI?\",\n        \"Explain machine learning in detail.\",\n        \"Write a haiku about programming.\",\n        \"What are the benefits of cloud computing?\"\n    ]\n\n    total_tokens = 0\n\n    for i, prompt in enumerate(requests, 1):\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": prompt}]\n        )\n\n        usage = response.usage\n        total_tokens += usage.total_tokens\n\n        print(f\"\\nRequest {i}: {prompt}\")\n        print(f\"Response: {response.content[0].text}\")\n        print(f\"Tokens - Input: {usage.input_tokens}, Output: {usage.output_tokens}, Total: {usage.total_tokens}\")\n\n    print(f\"\\nTotal tokens used across all requests: {total_tokens}\")\n\nasyncio.run(token_monitoring())\n</code></pre>"},{"location":"examples/basic/#error-handling-patterns","title":"Error Handling Patterns","text":"<p>Proper error handling with v-router:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def error_handling_example():\n    # Configuration that might fail\n    client = Client(\n        LLM(\n            model_name=\"nonexistent-model\",\n            provider=\"anthropic\"\n        )\n    )\n\n    try:\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n        )\n        print(f\"Success: {response.content[0].text}\")\n\n    except ValueError as e:\n        print(f\"Configuration error: {e}\")\n    except ConnectionError as e:\n        print(f\"Network error: {e}\")\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n\n    # Better approach: Use fallback models\n    robust_client = Client(\n        LLM(\n            model_name=\"nonexistent-model\",\n            provider=\"anthropic\",\n            backup_models=[\n                BackupModel(\n                    model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                    priority=1\n                )\n            ]\n        )\n    )\n\n    try:\n        response = await robust_client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n        )\n        print(f\"Fallback success: {response.model} - {response.content[0].text}\")\n\n    except Exception as e:\n        print(f\"All providers failed: {e}\")\n\nasyncio.run(error_handling_example())\n</code></pre>"},{"location":"examples/basic/#batch-processing","title":"Batch Processing","text":"<p>Process multiple requests efficiently:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def batch_processing():\n    client = Client(\n        LLM(model_name=\"gemini-1.5-flash\", provider=\"google\")  # Fast model for batch processing\n    )\n\n    # Batch of questions\n    questions = [\n        \"What is Python?\",\n        \"What is JavaScript?\", \n        \"What is Rust?\",\n        \"What is Go?\",\n        \"What is Swift?\"\n    ]\n\n    # Process all questions concurrently\n    async def process_question(question):\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": question}]\n        )\n        return {\n            \"question\": question,\n            \"answer\": response.content[0].text,\n            \"tokens\": response.usage.total_tokens\n        }\n\n    # Run all requests concurrently\n    results = await asyncio.gather(*[\n        process_question(q) for q in questions\n    ])\n\n    # Print results\n    total_tokens = 0\n    for result in results:\n        print(f\"\\nQ: {result['question']}\")\n        print(f\"A: {result['answer']}\")\n        print(f\"Tokens: {result['tokens']}\")\n        total_tokens += result['tokens']\n\n    print(f\"\\nTotal tokens used: {total_tokens}\")\n\nasyncio.run(batch_processing())\n</code></pre>"},{"location":"examples/basic/#configuration-management","title":"Configuration Management","text":"<p>Manage different configurations for different use cases:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM, BackupModel\n\nclass ConfigManager:\n    @staticmethod\n    def get_development_config():\n        \"\"\"Fast, cheap configuration for development.\"\"\"\n        return LLM(\n            model_name=\"gpt-3.5\",\n            provider=\"openai\",\n            max_tokens=500,\n            temperature=0.7\n        )\n\n    @staticmethod\n    def get_production_config():\n        \"\"\"Robust configuration with fallbacks for production.\"\"\"\n        return LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\",\n            max_tokens=2000,\n            temperature=0.3,\n            backup_models=[\n                BackupModel(\n                    model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                    priority=1\n                ),\n                BackupModel(\n                    model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n                    priority=2\n                )\n            ],\n            try_other_providers=True\n        )\n\n    @staticmethod\n    def get_creative_config():\n        \"\"\"High creativity configuration for creative tasks.\"\"\"\n        return LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\",\n            max_tokens=3000,\n            temperature=0.9\n        )\n\nasync def config_management_example():\n    import os\n\n    # Choose configuration based on environment\n    env = os.getenv(\"ENVIRONMENT\", \"development\")\n\n    if env == \"production\":\n        config = ConfigManager.get_production_config()\n    elif env == \"creative\":\n        config = ConfigManager.get_creative_config()\n    else:\n        config = ConfigManager.get_development_config()\n\n    client = Client(config)\n\n    response = await client.messages.create(\n        messages=[{\"role\": \"user\", \"content\": \"Tell me about v-router\"}]\n    )\n\n    print(f\"Environment: {env}\")\n    print(f\"Model used: {response.model}\")\n    print(f\"Response: {response.content[0].text}\")\n\nasyncio.run(config_management_example())\n</code></pre>"},{"location":"examples/basic/#performance-comparison","title":"Performance Comparison","text":"<p>Compare response times across providers:</p> <pre><code>import asyncio\nimport time\nfrom v_router import Client, LLM\n\nasync def performance_comparison():\n    providers_configs = [\n        (\"claude-sonnet-4\", \"anthropic\"),\n        (\"gpt-4o\", \"openai\"),\n        (\"gemini-1.5-flash\", \"google\")  # Flash is optimized for speed\n    ]\n\n    prompt = \"Explain the concept of recursion in programming.\"\n\n    results = []\n\n    for model_name, provider in providers_configs:\n        client = Client(\n            LLM(\n                model_name=model_name,\n                provider=provider,\n                max_tokens=200\n            )\n        )\n\n        # Measure response time\n        start_time = time.time()\n\n        try:\n            response = await client.messages.create(\n                messages=[{\"role\": \"user\", \"content\": prompt}]\n            )\n\n            end_time = time.time()\n            response_time = end_time - start_time\n\n            results.append({\n                \"provider\": provider,\n                \"model\": response.model,\n                \"response_time\": response_time,\n                \"tokens\": response.usage.total_tokens,\n                \"tokens_per_second\": response.usage.total_tokens / response_time\n            })\n\n        except Exception as e:\n            print(f\"{provider} failed: {e}\")\n\n    # Print performance comparison\n    print(\"Performance Comparison:\")\n    print(\"-\" * 80)\n    print(f\"{'Provider':&lt;15} {'Model':&lt;25} {'Time (s)':&lt;10} {'Tokens':&lt;8} {'Tok/sec':&lt;8}\")\n    print(\"-\" * 80)\n\n    for result in sorted(results, key=lambda x: x['response_time']):\n        print(f\"{result['provider']:&lt;15} {result['model']:&lt;25} {result['response_time']:&lt;10.2f} {result['tokens']:&lt;8} {result['tokens_per_second']:&lt;8.1f}\")\n\nasyncio.run(performance_comparison())\n</code></pre>"},{"location":"examples/basic/#best-practices-example","title":"Best Practices Example","text":"<p>A comprehensive example showing best practices:</p> <pre><code>import asyncio\nimport logging\nimport os\nfrom v_router import Client, LLM, BackupModel, setup_logger\n\n# Set up logging\nsetup_logger(\"v_router\", level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass ProductionClient:\n    def __init__(self):\n        self.client = self._create_client()\n\n    def _create_client(self):\n        \"\"\"Create a production-ready client configuration.\"\"\"\n        config = LLM(\n            model_name=\"claude-sonnet-4\",\n            provider=\"anthropic\",\n            max_tokens=2000,\n            temperature=0.3,\n            backup_models=[\n                BackupModel(\n                    model=LLM(\n                        model_name=\"gpt-4o\",\n                        provider=\"openai\",\n                        max_tokens=2000,\n                        temperature=0.3\n                    ),\n                    priority=1\n                ),\n                BackupModel(\n                    model=LLM(\n                        model_name=\"gemini-1.5-pro\",\n                        provider=\"google\",\n                        max_tokens=2000,\n                        temperature=0.3\n                    ),\n                    priority=2\n                )\n            ],\n            try_other_providers=True\n        )\n\n        return Client(config)\n\n    async def process_request(self, messages, max_retries=3):\n        \"\"\"Process a request with retry logic and monitoring.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                logger.info(f\"Processing request (attempt {attempt + 1})\")\n\n                response = await self.client.messages.create(messages=messages)\n\n                # Log successful request\n                logger.info(\n                    f\"Request successful: {response.model} ({response.provider}), \"\n                    f\"tokens: {response.usage.total_tokens}\"\n                )\n\n                return response\n\n            except Exception as e:\n                logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n\n                if attempt == max_retries - 1:\n                    logger.error(\"All retry attempts failed\")\n                    raise\n\n                # Wait before retry\n                await asyncio.sleep(2 ** attempt)  # Exponential backoff\n\nasync def best_practices_example():\n    # Check environment variables\n    required_keys = [\"ANTHROPIC_API_KEY\", \"OPENAI_API_KEY\"]\n    missing_keys = [key for key in required_keys if not os.getenv(key)]\n\n    if missing_keys:\n        print(f\"Missing API keys: {missing_keys}\")\n        return\n\n    client = ProductionClient()\n\n    # Process a request\n    try:\n        response = await client.process_request(\n            messages=[\n                {\"role\": \"user\", \"content\": \"Explain the benefits of using v-router for production LLM applications.\"}\n            ]\n        )\n\n        print(f\"Success: {response.content[0].text}\")\n\n    except Exception as e:\n        print(f\"Request failed after all retries: {e}\")\n\n# Run the example\nasyncio.run(best_practices_example())\n</code></pre>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<p>These basic examples should help you get started with v-router. For more advanced use cases, check out:</p> <ul> <li>Function Calling Examples</li> <li>Multimodal Content Examples</li> <li>Advanced Patterns</li> </ul> <p>Function Calling \u2192 Advanced Examples \u2192</p>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>v-router requires API keys for the providers you want to use. This guide covers how to set up authentication and configure various options.</p>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>Set up authentication for your providers using environment variables:</p>"},{"location":"getting-started/configuration/#anthropic","title":"Anthropic","text":"<pre><code>export ANTHROPIC_API_KEY=\"your-anthropic-key-here\"\n</code></pre> <p>Get your API key from Anthropic Console.</p>"},{"location":"getting-started/configuration/#openai","title":"OpenAI","text":"<pre><code>export OPENAI_API_KEY=\"your-openai-key-here\"\n</code></pre> <p>Get your API key from OpenAI Platform.</p>"},{"location":"getting-started/configuration/#google-ai-studio","title":"Google AI Studio","text":"<pre><code>export GOOGLE_API_KEY=\"your-google-ai-key-here\"\n</code></pre> <p>Get your API key from Google AI Studio.</p>"},{"location":"getting-started/configuration/#google-cloud-vertex-ai","title":"Google Cloud (Vertex AI)","text":"<p>For Vertex AI, you need service account credentials:</p> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account.json\"\nexport GCP_PROJECT_ID=\"your-project-id\"\nexport GCP_LOCATION=\"us-central1\"  # Optional, defaults to us-central1\n</code></pre> <p>Set up service account:</p> <ol> <li>Go to Google Cloud Console</li> <li>Enable the Vertex AI API</li> <li>Create a service account with Vertex AI permissions</li> <li>Download the JSON key file</li> <li>Set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable</li> </ol>"},{"location":"getting-started/configuration/#azure-openai","title":"Azure OpenAI","text":"<pre><code>export AZURE_OPENAI_API_KEY=\"your-azure-key-here\"\nexport AZURE_OPENAI_ENDPOINT=\"https://your-resource.openai.azure.com/\"\n</code></pre> <p>Get your credentials from Azure Portal.</p>"},{"location":"getting-started/configuration/#configuration-file","title":"Configuration File","text":"<p>You can also use a <code>.env</code> file in your project root:</p> .env<pre><code># Anthropic\nANTHROPIC_API_KEY=your-anthropic-key-here\n\n# OpenAI\nOPENAI_API_KEY=your-openai-key-here\n\n# Google AI\nGOOGLE_API_KEY=your-google-ai-key-here\n\n# Google Cloud (Vertex AI)\nGOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json\nGCP_PROJECT_ID=your-project-id\nGCP_LOCATION=us-central1\n\n# Azure OpenAI\nAZURE_OPENAI_API_KEY=your-azure-key-here\nAZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\n</code></pre> <p>v-router automatically loads <code>.env</code> files using <code>python-dotenv</code>.</p>"},{"location":"getting-started/configuration/#llm-configuration-options","title":"LLM Configuration Options","text":"<p>The <code>LLM</code> class supports many configuration options:</p>"},{"location":"getting-started/configuration/#basic-configuration","title":"Basic Configuration","text":"<pre><code>from v_router import LLM\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",     # Model to use\n    provider=\"anthropic\",             # Provider to use\n    max_tokens=1000,                  # Max tokens to generate\n    temperature=0.7,                  # Creativity (0-1)\n    system_prompt=\"You are helpful\"   # System message\n)\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from v_router import LLM, BackupModel\n\nllm_config = LLM(\n    # Core settings\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n\n    # Generation parameters\n    max_tokens=2000,\n    temperature=0.8,\n    top_p=0.9,\n    top_k=40,\n\n    # System configuration\n    system_prompt=\"You are an expert Python developer\",\n\n    # Backup models\n    backup_models=[\n        BackupModel(\n            model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n            priority=1\n        ),\n        BackupModel(\n            model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n            priority=2\n        )\n    ],\n\n    # Cross-provider fallback\n    try_other_providers=True,\n\n    # Tool configuration\n    tools=None,  # Will be covered in function calling guide\n\n    # Provider-specific options\n    anthropic_kwargs={\"stream\": False},\n    openai_kwargs={\"presence_penalty\": 0.1},\n    google_kwargs={\"safety_settings\": {}},\n)\n</code></pre>"},{"location":"getting-started/configuration/#model-name-mapping","title":"Model Name Mapping","text":"<p>v-router automatically maps generic model names to provider-specific versions using the <code>models.yml</code> configuration:</p>"},{"location":"getting-started/configuration/#available-models","title":"Available Models","text":"Claude ModelsGPT ModelsGemini Models <pre><code># These all work across Anthropic and Vertex AI\nLLM(model_name=\"claude-3-opus\", provider=\"anthropic\")\nLLM(model_name=\"claude-3-sonnet\", provider=\"anthropic\") \nLLM(model_name=\"claude-3-haiku\", provider=\"anthropic\")\nLLM(model_name=\"claude-sonnet-4\", provider=\"anthropic\")\nLLM(model_name=\"claude-opus-4\", provider=\"anthropic\")\n\n# Same models on Vertex AI\nLLM(model_name=\"claude-sonnet-4\", provider=\"vertexai\")\n</code></pre> <pre><code># OpenAI and Azure OpenAI\nLLM(model_name=\"gpt-4\", provider=\"openai\")\nLLM(model_name=\"gpt-4-turbo\", provider=\"openai\")\nLLM(model_name=\"gpt-4.1\", provider=\"openai\")\nLLM(model_name=\"gpt-3.5\", provider=\"openai\")\n\n# Same models on Azure\nLLM(model_name=\"gpt-4\", provider=\"azure\")\n</code></pre> <pre><code># Google AI and Vertex AI\nLLM(model_name=\"gemini-pro\", provider=\"google\")\nLLM(model_name=\"gemini-1.5-pro\", provider=\"google\")\nLLM(model_name=\"gemini-1.5-flash\", provider=\"google\")\nLLM(model_name=\"gemini-2.0-flash\", provider=\"google\")\n\n# Same models on Vertex AI\nLLM(model_name=\"gemini-1.5-pro\", provider=\"vertexai\")\n</code></pre>"},{"location":"getting-started/configuration/#custom-model-names","title":"Custom Model Names","text":"<p>You can also use provider-specific model names directly:</p> <pre><code># Use exact provider model names\nLLM(model_name=\"claude-sonnet-4-20250514\", provider=\"anthropic\")\nLLM(model_name=\"gpt-4-turbo-preview\", provider=\"openai\")\nLLM(model_name=\"gemini-1.5-pro-latest\", provider=\"google\")\n</code></pre>"},{"location":"getting-started/configuration/#provider-specific-configuration","title":"Provider-Specific Configuration","text":""},{"location":"getting-started/configuration/#anthropic-configuration","title":"Anthropic Configuration","text":"<pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    anthropic_kwargs={\n        \"stream\": False,\n        \"extra_headers\": {\"Custom-Header\": \"value\"}\n    }\n)\n</code></pre>"},{"location":"getting-started/configuration/#openai-configuration","title":"OpenAI Configuration","text":"<pre><code>llm_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    openai_kwargs={\n        \"presence_penalty\": 0.1,\n        \"frequency_penalty\": 0.1,\n        \"logit_bias\": {},\n        \"user\": \"user-123\"\n    }\n)\n</code></pre>"},{"location":"getting-started/configuration/#google-configuration","title":"Google Configuration","text":"<pre><code>llm_config = LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\",\n    google_kwargs={\n        \"safety_settings\": {\n            \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_MEDIUM_AND_ABOVE\"\n        },\n        \"generation_config\": {\n            \"candidate_count\": 1\n        }\n    }\n)\n</code></pre>"},{"location":"getting-started/configuration/#vertex-ai-configuration","title":"Vertex AI Configuration","text":"<pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"vertexai\",\n    vertex_kwargs={\n        \"location\": \"us-central1\",\n        \"project\": \"my-project-id\"\n    }\n)\n</code></pre>"},{"location":"getting-started/configuration/#backup-model-configuration","title":"Backup Model Configuration","text":"<p>Configure multiple fallback strategies:</p> <pre><code>from v_router import LLM, BackupModel\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n\n    # Backup models in priority order\n    backup_models=[\n        # Try OpenAI GPT-4 first\n        BackupModel(\n            model=LLM(\n                model_name=\"gpt-4o\",\n                provider=\"openai\",\n                max_tokens=1000,\n                temperature=0.7\n            ),\n            priority=1\n        ),\n        # Then try Google Gemini\n        BackupModel(\n            model=LLM(\n                model_name=\"gemini-1.5-pro\",\n                provider=\"google\",\n                max_tokens=1000,\n                temperature=0.7\n            ),\n            priority=2\n        ),\n        # Finally try Azure OpenAI\n        BackupModel(\n            model=LLM(\n                model_name=\"gpt-4\",\n                provider=\"azure\",\n                max_tokens=1000,\n                temperature=0.7\n            ),\n            priority=3\n        )\n    ],\n\n    # Also try the same model on other providers\n    try_other_providers=True\n)\n</code></pre>"},{"location":"getting-started/configuration/#logging-configuration","title":"Logging Configuration","text":"<p>Enable detailed logging to debug issues:</p> <pre><code>import logging\nfrom v_router import setup_logger\n\n# Enable debug logging\nsetup_logger(\"v_router\", level=logging.DEBUG)\n\n# Or configure specific loggers\nlogging.getLogger(\"v_router.router\").setLevel(logging.INFO)\nlogging.getLogger(\"v_router.providers\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"getting-started/configuration/#validation","title":"Validation","text":"<p>v-router validates your configuration and provides helpful error messages:</p> <pre><code>from v_router import LLM\n\ntry:\n    # This will raise an error if the API key is missing\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\"\n    )\n    client = Client(llm_config)\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"getting-started/configuration/#testing-configuration","title":"Testing Configuration","text":"<p>Test your configuration with a simple request:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def test_config():\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\"\n    )\n\n    client = Client(llm_config)\n\n    try:\n        response = await client.messages.create(\n            messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n        )\n        print(f\"\u2705 Configuration working! Model: {response.model}\")\n    except Exception as e:\n        print(f\"\u274c Configuration error: {e}\")\n\nasyncio.run(test_config())\n</code></pre>"},{"location":"getting-started/configuration/#best-practices","title":"Best Practices","text":""},{"location":"getting-started/configuration/#security","title":"Security","text":"<ul> <li>Never commit API keys to version control</li> <li>Use environment variables or secure secret management</li> <li>Rotate API keys regularly</li> <li>Monitor API usage and costs</li> </ul>"},{"location":"getting-started/configuration/#performance","title":"Performance","text":"<ul> <li>Configure appropriate <code>max_tokens</code> limits</li> <li>Use backup models to ensure availability</li> <li>Consider using faster models for development</li> </ul>"},{"location":"getting-started/configuration/#cost-management","title":"Cost Management","text":"<ul> <li>Monitor token usage with <code>response.usage</code></li> <li>Use cheaper models for backup when appropriate</li> <li>Set <code>max_tokens</code> to control costs</li> </ul>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<p>Now that you have v-router configured:</p> <ol> <li>Try the quick start guide</li> <li>Learn about function calling</li> <li>Explore multimodal content</li> </ol> <p>Quick Start Guide \u2192 Function Calling \u2192</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>v-router is available on PyPI and can be installed using pip or other Python package managers.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13.3+: v-router requires Python 3.13.3 or higher</li> <li>API Keys: You'll need API keys for the providers you want to use</li> </ul>"},{"location":"getting-started/installation/#install-with-pip","title":"Install with pip","text":"<pre><code>pip install v-router\n</code></pre>"},{"location":"getting-started/installation/#install-with-uv-recommended","title":"Install with uv (Recommended)","text":"<p>uv is a fast Python package installer and resolver:</p> <pre><code>uv add v-router\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute to v-router or run the latest development version:</p> <pre><code># Clone the repository\ngit clone https://github.com/vectrix-ai/v-router.git\ncd v-router\n\n# Install with development dependencies\nuv sync --all-extras\n\n# Install pre-commit hooks (optional)\nuv run pre-commit install\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation by running:</p> <pre><code>import v_router\nprint(v_router.__version__)\n</code></pre> <p>Or check the available components:</p> <pre><code>from v_router import Client, LLM, BackupModel\nprint(\"v-router installed successfully!\")\n</code></pre>"},{"location":"getting-started/installation/#provider-dependencies","title":"Provider Dependencies","text":"<p>v-router automatically installs the required dependencies for all supported providers:</p> <ul> <li>Anthropic: <code>anthropic[vertex]&gt;=0.52.0</code></li> <li>OpenAI: <code>openai&gt;=1.82.0</code></li> <li>Google AI: <code>google-genai&gt;=1.16.1</code></li> <li>Google Cloud (Vertex AI): <code>google-cloud-aiplatform&gt;=1.94.0</code></li> <li>Configuration: <code>pyyaml&gt;=6.0.2</code></li> <li>Logging: <code>colorlog&gt;=6.9.0</code></li> </ul> <p>All dependencies are installed automatically when you install v-router.</p>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For development and testing:</p> <pre><code># Development tools\nuv add --dev pytest pytest-asyncio ruff pre-commit\n\n# Documentation\nuv add --dev mkdocs-material\n\n# Jupyter notebooks (for examples)\nuv add --dev ipykernel python-dotenv\n</code></pre>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: Linux, macOS, Windows</li> <li>Memory: Minimum 512MB RAM</li> <li>Network: Internet connection required for API calls</li> </ul>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#importerror-no-module-named-v_router","title":"ImportError: No module named 'v_router'","text":"<p>Make sure you've installed v-router in the correct Python environment:</p> <pre><code># Check your Python version\npython --version\n\n# Check installed packages\npip list | grep v-router\n</code></pre>"},{"location":"getting-started/installation/#modulenotfounderror-for-provider-libraries","title":"ModuleNotFoundError for provider libraries","text":"<p>If you see errors about missing provider libraries, try reinstalling:</p> <pre><code>pip install --upgrade v-router\n</code></pre>"},{"location":"getting-started/installation/#api-key-issues","title":"API Key Issues","text":"<p>Make sure your environment variables are set correctly. See the Configuration guide for details.</p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues:</p> <ol> <li>Check the troubleshooting section</li> <li>Search existing issues</li> <li>Create a new issue with:</li> <li>Python version</li> <li>v-router version</li> <li>Operating system</li> <li>Full error message</li> <li>Minimal code example</li> </ol>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that you have v-router installed, you can:</p> <ol> <li>Set up your configuration</li> <li>Try the quick start guide</li> <li>Explore the examples</li> </ol> <p>Continue to Configuration \u2192</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with v-router in minutes. This guide covers the basic usage patterns and core concepts.</p>"},{"location":"getting-started/quick-start/#your-first-request","title":"Your First Request","text":"<p>Let's start with a simple example that shows v-router's core functionality:</p> <pre><code>import asyncio\nfrom v_router import Client, LLM\n\nasync def main():\n    # Create an LLM configuration\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        max_tokens=100,\n        temperature=0.7\n    )\n\n    # Create a client\n    client = Client(llm_config)\n\n    # Send a message\n    response = await client.messages.create(\n        messages=[\n            {\"role\": \"user\", \"content\": \"Hello! Explain quantum computing in one sentence.\"}\n        ]\n    )\n\n    print(f\"Response: {response.content[0].text}\")\n    print(f\"Model: {response.model}\")\n    print(f\"Provider: {response.provider}\")\n\n# Run the example\nasyncio.run(main())\n</code></pre> <p>API Keys Required</p> <p>Make sure you have your API keys configured before running this example. See the Configuration guide for details.</p>"},{"location":"getting-started/quick-start/#key-concepts","title":"Key Concepts","text":""},{"location":"getting-started/quick-start/#llm-configuration","title":"LLM Configuration","text":"<p>The <code>LLM</code> class defines your model configuration:</p> <pre><code>from v_router import LLM\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",  # Model to use\n    provider=\"anthropic\",          # Provider to use\n    max_tokens=1000,              # Maximum tokens to generate\n    temperature=0.7,              # Creativity level (0-1)\n    system_prompt=\"You are a helpful assistant\"  # System message\n)\n</code></pre>"},{"location":"getting-started/quick-start/#client-and-messages","title":"Client and Messages","text":"<p>The <code>Client</code> provides the main interface, with a <code>messages</code> API similar to provider SDKs:</p> <pre><code>from v_router import Client\n\nclient = Client(llm_config)\n\n# The messages API matches OpenAI/Anthropic patterns\nresponse = await client.messages.create(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n        {\"role\": \"user\", \"content\": \"What's the weather like?\"}\n    ]\n)\n</code></pre>"},{"location":"getting-started/quick-start/#response-format","title":"Response Format","text":"<p>All providers return the same unified response format:</p> <pre><code># Response object\nprint(response.content[0].text)    # The generated text\nprint(response.model)              # Model that was actually used\nprint(response.provider)           # Provider that was used\nprint(response.usage.total_tokens) # Token usage information\n\n# Access the raw provider response if needed\nprint(response.raw_response)\n</code></pre>"},{"location":"getting-started/quick-start/#adding-automatic-fallback","title":"Adding Automatic Fallback","text":"<p>One of v-router's key features is automatic fallback when your primary model fails:</p> <pre><code>from v_router import Client, LLM, BackupModel\n\n# Configure primary model with backups\nllm_config = LLM(\n    model_name=\"claude-6\",  # This might fail (doesn't exist yet)\n    provider=\"anthropic\",\n    backup_models=[\n        BackupModel(\n            model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n            priority=1  # Try this first as backup\n        ),\n        BackupModel(\n            model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n            priority=2  # Try this second\n        )\n    ]\n)\n\nclient = Client(llm_config)\n\n# If claude-6 fails, automatically tries gpt-4o, then gemini-1.5-pro\nresponse = await client.messages.create(\n    messages=[{\"role\": \"user\", \"content\": \"What's 2+2?\"}]\n)\n\nprint(f\"Successfully used: {response.model} from {response.provider}\")\n</code></pre>"},{"location":"getting-started/quick-start/#cross-provider-switching","title":"Cross-Provider Switching","text":"<p>Enable automatic switching between providers for the same model:</p> <pre><code>llm_config = LLM(\n    model_name=\"claude-opus-4\",\n    provider=\"vertexai\",  # Try Vertex AI first\n    try_other_providers=True  # Fall back to direct Anthropic if needed\n)\n\nclient = Client(llm_config)\n\n# Will try Vertex AI first, then Anthropic directly\nresponse = await client.messages.create(\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke.\"}]\n)\n</code></pre>"},{"location":"getting-started/quick-start/#working-with-different-providers","title":"Working with Different Providers","text":"<p>v-router automatically maps model names across providers. Here are some examples:</p> AnthropicOpenAIGoogle AIVertex AI <pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\"\n)\n</code></pre> <pre><code>llm_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\"\n)\n</code></pre> <pre><code>llm_config = LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\"\n)\n</code></pre> <pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",  # Same model name\n    provider=\"vertexai\"            # Different provider\n)\n</code></pre>"},{"location":"getting-started/quick-start/#error-handling","title":"Error Handling","text":"<p>v-router provides detailed error information when requests fail:</p> <pre><code>try:\n    response = await client.messages.create(\n        messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n    )\nexcept Exception as e:\n    print(f\"Request failed: {e}\")\n    # v-router will have already tried backup models if configured\n</code></pre>"},{"location":"getting-started/quick-start/#function-calling-preview","title":"Function Calling Preview","text":"<p>v-router provides unified function calling across all providers with fine-grained control:</p> <pre><code>from v_router.classes.tools import ToolCall, Tools\nfrom pydantic import BaseModel, Field\n\n# Define a tool\nclass Calculator(BaseModel):\n    expression: str = Field(..., description=\"Mathematical expression to evaluate\")\n\ncalc_tool = ToolCall(\n    name=\"calculator\",\n    description=\"Perform calculations\",\n    input_schema=Calculator.model_json_schema()\n)\n\n# Configure with tool control\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=Tools(tools=[calc_tool]),\n    tool_choice=\"auto\"  # \"auto\", \"any\", \"none\", or tool name\n)\n\nclient = Client(llm_config)\nresponse = await client.messages.create(\n    messages=[{\"role\": \"user\", \"content\": \"What's 15 * 23?\"}]\n)\n\n# Check for tool calls\nif response.tool_use:\n    for tool_call in response.tool_use:\n        print(f\"Tool: {tool_call.name}, Args: {tool_call.arguments}\")\n</code></pre> <p>Tool Choice Control</p> <p>Use <code>tool_choice=\"calculator\"</code> to force the calculator tool, <code>\"any\"</code> to require any tool, or <code>\"none\"</code> to disable tools entirely.</p>"},{"location":"getting-started/quick-start/#complete-example","title":"Complete Example","text":"<p>Here's a complete example that demonstrates multiple features:</p> <pre><code>import asyncio\nimport os\nfrom v_router import Client, LLM, BackupModel\n\nasync def demo():\n    # Configure with multiple fallback options\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        max_tokens=500,\n        temperature=0.8,\n        system_prompt=\"You are a creative writing assistant\",\n        backup_models=[\n            BackupModel(\n                model=LLM(\n                    model_name=\"gpt-4o\", \n                    provider=\"openai\",\n                    max_tokens=500,\n                    temperature=0.8\n                ),\n                priority=1\n            )\n        ],\n        try_other_providers=True\n    )\n\n    client = Client(llm_config)\n\n    # Send a creative writing request\n    response = await client.messages.create(\n        messages=[\n            {\n                \"role\": \"user\", \n                \"content\": \"Write a short story about a robot learning to paint\"\n            }\n        ]\n    )\n\n    print(\"=== Creative Story ===\")\n    print(response.content[0].text)\n    print(f\"\\nGenerated by: {response.model} ({response.provider})\")\n    print(f\"Tokens used: {response.usage.total_tokens}\")\n\n# Make sure you have API keys set\nif __name__ == \"__main__\":\n    # Check for required environment variables\n    required_keys = [\"ANTHROPIC_API_KEY\", \"OPENAI_API_KEY\"]\n    missing_keys = [key for key in required_keys if not os.getenv(key)]\n\n    if missing_keys:\n        print(f\"Please set these environment variables: {missing_keys}\")\n        print(\"See the configuration guide for details.\")\n    else:\n        asyncio.run(demo())\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<p>Now that you understand the basics, explore more advanced features:</p> <ul> <li>Function Calling: Use tools and function calls</li> <li>Multimodal Content: Send images and PDFs</li> <li>Configuration: Set up API keys and advanced options</li> <li>Examples: See more detailed examples</li> </ul> <p>Explore Function Calling \u2192 See More Examples \u2192</p>"},{"location":"guide/function-calling/","title":"Function Calling","text":"<p>v-router provides unified function calling (tool use) across all providers. Define your tools once and use them with any LLM, whether it's Anthropic's Claude, OpenAI's GPT models, or Google's Gemini.</p>"},{"location":"guide/function-calling/#key-features","title":"Key Features","text":"<ul> <li>Unified Tool Interface: Same tool definitions work across all providers</li> <li>Flexible Tool Configuration: Pass tools as a list or Tools object</li> <li>Tool Choice Control: Force specific tools, require any tool, or disable tools entirely</li> <li>Type-Safe Schemas: Use Pydantic models for robust tool parameter validation</li> <li>Automatic Tool Inheritance: Backup models inherit tools and tool_choice from primary configuration</li> <li>Consistent Response Format: Same tool response structure across all providers</li> </ul>"},{"location":"guide/function-calling/#quick-start","title":"Quick Start","text":"<p>Here's a simple example that works across all providers:</p> <pre><code>import asyncio\nfrom pydantic import BaseModel, Field\nfrom v_router import Client, LLM\nfrom v_router.classes.tools import ToolCall, Tools\n\n# Define tool schema using Pydantic\nclass WeatherQuery(BaseModel):\n    location: str = Field(..., description=\"City and state, e.g. San Francisco, CA\")\n    units: str = Field(\"fahrenheit\", description=\"Temperature units (celsius/fahrenheit)\")\n\n# Create tool definition\nweather_tool = ToolCall(\n    name=\"get_weather\",\n    description=\"Get current weather for a location\",\n    input_schema=WeatherQuery.model_json_schema()\n)\n\nasync def main():\n    # Configure LLM with tools\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        tools=[weather_tool]  # Pass tools as a list\n    )\n\n    # Alternative syntax (also works):\n    # tools=Tools(tools=[weather_tool])\n\n    client = Client(llm_config)\n\n    # Make request that will trigger tool use\n    response = await client.messages.create(\n        messages=[\n            {\"role\": \"user\", \"content\": \"What's the weather like in Paris, France?\"}\n        ]\n    )\n\n    # Check for tool calls\n    if response.tool_use:\n        for tool_call in response.tool_use:\n            print(f\"Tool: {tool_call.name}\")\n            print(f\"Arguments: {tool_call.arguments}\")\n\n            # In a real app, you'd execute the function here\n            if tool_call.name == \"get_weather\":\n                # Simulate weather API call\n                weather_data = {\"temperature\": \"22\u00b0C\", \"condition\": \"sunny\"}\n                print(f\"Weather result: {weather_data}\")\n    else:\n        print(f\"Response: {response.content[0].text}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guide/function-calling/#tool-configuration-options","title":"Tool Configuration Options","text":"<p>v-router supports flexible tool configuration for your LLM:</p>"},{"location":"guide/function-calling/#direct-list-syntax","title":"Direct List Syntax","text":"<pre><code># Pass tools directly as a list\ntools=[tool1, tool2, tool3]\n</code></pre>"},{"location":"guide/function-calling/#tools-object-syntax","title":"Tools Object Syntax","text":"<pre><code># Wrap tools in a Tools object\ntools=Tools(tools=[tool1, tool2, tool3])\n</code></pre>"},{"location":"guide/function-calling/#compatibility","title":"Compatibility","text":"<p>Both syntaxes work identically and can be used interchangeably. When you pass a list of <code>ToolCall</code> objects, v-router automatically wraps them in a <code>Tools</code> object internally, ensuring all validation and functionality remains exactly the same.</p>"},{"location":"guide/function-calling/#tool-definition","title":"Tool Definition","text":""},{"location":"guide/function-calling/#using-pydantic-recommended","title":"Using Pydantic (Recommended)","text":"<p>Pydantic provides type safety and automatic schema generation:</p> <pre><code>from pydantic import BaseModel, Field\nfrom typing import List, Optional\nfrom v_router.classes.tools import ToolCall\n\nclass DatabaseQuery(BaseModel):\n    table: str = Field(..., description=\"Database table name\")\n    columns: List[str] = Field(..., description=\"Columns to select\")\n    where_clause: Optional[str] = Field(None, description=\"SQL WHERE clause\")\n    limit: int = Field(10, description=\"Maximum number of results\")\n\ndb_tool = ToolCall(\n    name=\"query_database\",\n    description=\"Execute a SELECT query on the database\",\n    input_schema=DatabaseQuery.model_json_schema()\n)\n</code></pre>"},{"location":"guide/function-calling/#manual-schema-definition","title":"Manual Schema Definition","text":"<p>You can also define schemas manually:</p> <pre><code>from v_router.classes.tools import ToolCall\n\nmanual_tool = ToolCall(\n    name=\"calculate_distance\",\n    description=\"Calculate distance between two points\",\n    input_schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"lat1\": {\"type\": \"number\", \"description\": \"Latitude of first point\"},\n            \"lon1\": {\"type\": \"number\", \"description\": \"Longitude of first point\"},\n            \"lat2\": {\"type\": \"number\", \"description\": \"Latitude of second point\"},\n            \"lon2\": {\"type\": \"number\", \"description\": \"Longitude of second point\"}\n        },\n        \"required\": [\"lat1\", \"lon1\", \"lat2\", \"lon2\"]\n    }\n)\n</code></pre>"},{"location":"guide/function-calling/#multiple-tools","title":"Multiple Tools","text":"<p>Configure multiple tools for complex workflows:</p> <pre><code>from pydantic import BaseModel, Field\nfrom v_router.classes.tools import ToolCall, Tools\n\n# File operations\nclass FileOperation(BaseModel):\n    path: str = Field(..., description=\"File path\")\n    content: Optional[str] = Field(None, description=\"File content for write operations\")\n\n# Email operations  \nclass EmailMessage(BaseModel):\n    to: str = Field(..., description=\"Recipient email address\")\n    subject: str = Field(..., description=\"Email subject\")\n    body: str = Field(..., description=\"Email body\")\n\n# Mathematical operations\nclass MathOperation(BaseModel):\n    expression: str = Field(..., description=\"Mathematical expression to evaluate\")\n\n# Create tools\nfile_tool = ToolCall(\n    name=\"file_operations\",\n    description=\"Read or write files\",\n    input_schema=FileOperation.model_json_schema()\n)\n\nemail_tool = ToolCall(\n    name=\"send_email\", \n    description=\"Send an email message\",\n    input_schema=EmailMessage.model_json_schema()\n)\n\nmath_tool = ToolCall(\n    name=\"calculate\",\n    description=\"Perform mathematical calculations\",\n    input_schema=MathOperation.model_json_schema()\n)\n\n# Configure LLM with multiple tools\nllm_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    tools=[file_tool, email_tool, math_tool]  # Pass as list\n)\n\n# Alternative syntax:\n# tools=Tools(tools=[file_tool, email_tool, math_tool])\n</code></pre>"},{"location":"guide/function-calling/#controlling-tool-usage-with-tool_choice","title":"Controlling Tool Usage with tool_choice","text":"<p>v-router provides fine-grained control over when and how tools are used through the <code>tool_choice</code> parameter:</p>"},{"location":"guide/function-calling/#tool-choice-options","title":"Tool Choice Options","text":"<ul> <li><code>None</code> or <code>\"auto\"</code> (default): Model decides whether to use tools</li> <li><code>\"any\"</code>: Model must use one of the provided tools  </li> <li><code>\"none\"</code>: Model is prevented from using tools</li> <li><code>str</code> (tool name): Force the model to use a specific tool</li> <li><code>dict</code>: Provider-specific format for advanced control</li> </ul>"},{"location":"guide/function-calling/#force-a-specific-tool","title":"Force a Specific Tool","text":"<p>When you want to guarantee that a specific tool is used:</p> <pre><code>from v_router import Client, LLM\nfrom v_router.classes.tools import Tools\n\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=[weather_tool, calculator_tool],\n    tool_choice=\"get_weather\"  # Force this specific tool\n)\n\nclient = Client(llm_config)\n\n# Even for non-weather queries, the weather tool will be forced\nresponse = await client.messages.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello! Can you help me with math?\"}]\n)\n</code></pre>"},{"location":"guide/function-calling/#require-any-tool-usage","title":"Require Any Tool Usage","text":"<p>When you want to ensure the model uses one of the available tools:</p> <pre><code>llm_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    tools=[weather_tool, calculator_tool, search_tool],\n    tool_choice=\"any\"  # Must use one of the provided tools\n)\n\n# The model will choose the most appropriate tool from those available\n</code></pre>"},{"location":"guide/function-calling/#disable-tool-usage","title":"Disable Tool Usage","text":"<p>When you want to prevent the model from using tools:</p> <pre><code>llm_config = LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\",\n    tools=[weather_tool, calculator_tool],  # Tools available but...\n    tool_choice=\"none\"  # ...model prevented from using them\n)\n\n# Model will provide text-only responses\n</code></pre>"},{"location":"guide/function-calling/#auto-mode-default","title":"Auto Mode (Default)","text":"<p>The default behavior where the model intelligently decides:</p> <pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=[weather_tool, calculator_tool],\n    tool_choice=\"auto\"  # Explicit auto mode (same as None or not specified)\n)\n\n# Model will use tools when appropriate, ignore them when not needed\n</code></pre>"},{"location":"guide/function-calling/#provider-specific-formats","title":"Provider-Specific Formats","text":"<p>For advanced control, use provider-native formats:</p> AnthropicOpenAI <pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=[calculator_tool],\n    tool_choice={\"type\": \"tool\", \"name\": \"calculator\"}\n)\n</code></pre> <pre><code>llm_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\",\n    tools=[calculator_tool],\n    tool_choice={\"type\": \"function\", \"function\": {\"name\": \"calculator\"}}\n)\n</code></pre>"},{"location":"guide/function-calling/#tool-choice-with-fallbacks","title":"Tool Choice with Fallbacks","text":"<p>The <code>tool_choice</code> parameter works seamlessly with fallback models:</p> <pre><code>from v_router import BackupModel\n\nllm_config = LLM(\n    model_name=\"claude-primary\",\n    provider=\"anthropic\",\n    tools=[calculator_tool, weather_tool],\n    tool_choice=\"calculator\",  # Force calculator tool\n    backup_models=[\n        BackupModel(\n            model=LLM(\n                model_name=\"gpt-4o\",\n                provider=\"openai\"\n                # tools and tool_choice inherited from primary\n            ),\n            priority=1\n        )\n    ]\n)\n\n# Backup models inherit the forced tool behavior\n</code></pre>"},{"location":"guide/function-calling/#tool-execution-loop","title":"Tool Execution Loop","text":"<p>Here's a complete example showing how to handle tool calls and continue the conversation:</p> <pre><code>import asyncio\nimport json\nfrom v_router import Client, LLM\nfrom v_router.classes.tools import ToolCall, Tools\nfrom pydantic import BaseModel, Field\n\nclass Calculator(BaseModel):\n    expression: str = Field(..., description=\"Mathematical expression to evaluate\")\n\nclass WebSearch(BaseModel):\n    query: str = Field(..., description=\"Search query\")\n\nasync def execute_tool(tool_name: str, arguments: dict):\n    \"\"\"Execute the requested tool and return results.\"\"\"\n    if tool_name == \"calculate\":\n        try:\n            # Safe evaluation of simple math expressions\n            result = eval(arguments[\"expression\"])\n            return {\"result\": result}\n        except Exception as e:\n            return {\"error\": f\"Calculation error: {e}\"}\n\n    elif tool_name == \"web_search\":\n        # Simulate web search\n        query = arguments[\"query\"]\n        return {\n            \"results\": [\n                {\"title\": f\"Result for {query}\", \"url\": \"https://example.com\"},\n                {\"title\": f\"More about {query}\", \"url\": \"https://example.org\"}\n            ]\n        }\n\n    return {\"error\": f\"Unknown tool: {tool_name}\"}\n\nasync def chat_with_tools():\n    # Define tools\n    calc_tool = ToolCall(\n        name=\"calculate\",\n        description=\"Perform mathematical calculations\",\n        input_schema=Calculator.model_json_schema()\n    )\n\n    search_tool = ToolCall(\n        name=\"web_search\", \n        description=\"Search the web for information\",\n        input_schema=WebSearch.model_json_schema()\n    )\n\n    # Create client with tools\n    llm_config = LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",\n        tools=[calc_tool, search_tool]\n    )\n\n    client = Client(llm_config)\n\n    # Start conversation\n    messages = [\n        {\"role\": \"user\", \"content\": \"What is 15 * 23, and can you search for information about quantum computing?\"}\n    ]\n\n    while True:\n        response = await client.messages.create(messages=messages)\n\n        # Add assistant message\n        messages.append({\n            \"role\": \"assistant\", \n            \"content\": response.content[0].text if response.content else \"\"\n        })\n\n        # Handle tool calls\n        if response.tool_use:\n            for tool_call in response.tool_use:\n                print(f\"\ud83d\udd27 Executing {tool_call.name}...\")\n\n                # Execute the tool\n                result = await execute_tool(tool_call.name, tool_call.arguments)\n\n                # Add tool result to conversation\n                messages.append({\n                    \"role\": \"user\",\n                    \"content\": f\"Tool result for {tool_call.name}: {json.dumps(result)}\"\n                })\n\n            # Continue conversation with tool results\n            continue\n        else:\n            # No more tool calls, show final response\n            print(f\"\ud83e\udd16 {response.content[0].text}\")\n            break\n\nasyncio.run(chat_with_tools())\n</code></pre>"},{"location":"guide/function-calling/#provider-compatibility","title":"Provider Compatibility","text":"<p>All function calling features work across providers:</p> Anthropic ClaudeOpenAI GPTGoogle GeminiVertex AI <pre><code>llm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=[your_tool]\n)\n</code></pre> <pre><code>llm_config = LLM(\n    model_name=\"gpt-4o\",\n    provider=\"openai\", \n    tools=[your_tool]\n)\n</code></pre> <pre><code>llm_config = LLM(\n    model_name=\"gemini-1.5-pro\",\n    provider=\"google\",\n    tools=[your_tool]\n)\n</code></pre> <pre><code># Works with both Claude and Gemini on Vertex\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"vertexai\",\n    tools=[your_tool]\n)\n</code></pre>"},{"location":"guide/function-calling/#advanced-tool-patterns","title":"Advanced Tool Patterns","text":""},{"location":"guide/function-calling/#conditional-tool-availability","title":"Conditional Tool Availability","text":"<p>Control which tools are available based on context:</p> <pre><code>def get_tools_for_user(user_role: str) -&gt; Tools:\n    \"\"\"Return tools based on user permissions.\"\"\"\n    base_tools = [search_tool, weather_tool]\n\n    if user_role == \"admin\":\n        base_tools.extend([file_tool, database_tool])\n    elif user_role == \"developer\":\n        base_tools.extend([code_tool, git_tool])\n\n    return Tools(tools=base_tools)\n\n# Use conditional tools\nuser_tools = get_tools_for_user(\"admin\")  # Returns Tools object\nllm_config = LLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    tools=user_tools  # Works with Tools object or list of ToolCall\n)\n\n# Or pass the tools list directly:\n# tools=get_tools_for_user(\"admin\").tools\n</code></pre>"},{"location":"guide/function-calling/#tool-chaining","title":"Tool Chaining","text":"<p>Chain tools together for complex workflows:</p> <pre><code>async def research_and_summarize(topic: str):\n    \"\"\"Chain web search and summarization tools.\"\"\"\n\n    # Step 1: Search for information\n    search_response = await client.messages.create(\n        messages=[{\n            \"role\": \"user\", \n            \"content\": f\"Search for recent information about {topic}\"\n        }]\n    )\n\n    # Execute search tool\n    search_results = []\n    if search_response.tool_use:\n        for tool_call in search_response.tool_use:\n            if tool_call.name == \"web_search\":\n                results = await execute_tool(tool_call.name, tool_call.arguments)\n                search_results.extend(results.get(\"results\", []))\n\n    # Step 2: Summarize findings\n    summary_response = await client.messages.create(\n        messages=[{\n            \"role\": \"user\",\n            \"content\": f\"Summarize these search results about {topic}: {search_results}\"\n        }]\n    )\n\n    return summary_response.content[0].text\n</code></pre>"},{"location":"guide/function-calling/#error-handling-in-tools","title":"Error Handling in Tools","text":"<p>Implement robust error handling:</p> <pre><code>async def safe_tool_execution(tool_name: str, arguments: dict):\n    \"\"\"Execute tools with comprehensive error handling.\"\"\"\n    try:\n        if tool_name == \"risky_operation\":\n            # Validate inputs\n            if not arguments.get(\"required_param\"):\n                return {\"error\": \"Missing required parameter\"}\n\n            # Perform operation with timeout\n            import asyncio\n            result = await asyncio.wait_for(\n                risky_operation(arguments),\n                timeout=30.0\n            )\n\n            return {\"success\": True, \"result\": result}\n\n    except asyncio.TimeoutError:\n        return {\"error\": \"Operation timed out\"}\n    except Exception as e:\n        return {\"error\": f\"Tool execution failed: {str(e)}\"}\n</code></pre>"},{"location":"guide/function-calling/#real-world-examples","title":"Real-World Examples","text":""},{"location":"guide/function-calling/#code-analysis-tool","title":"Code Analysis Tool","text":"<pre><code>from pydantic import BaseModel, Field\n\nclass CodeAnalysis(BaseModel):\n    code: str = Field(..., description=\"Code to analyze\")\n    language: str = Field(\"python\", description=\"Programming language\")\n    analysis_type: str = Field(\"quality\", description=\"Type: quality, security, performance\")\n\ncode_tool = ToolCall(\n    name=\"analyze_code\",\n    description=\"Analyze code for quality, security, or performance issues\",\n    input_schema=CodeAnalysis.model_json_schema()\n)\n\nasync def analyze_code(code: str, language: str, analysis_type: str):\n    \"\"\"Implement actual code analysis logic.\"\"\"\n    # This would integrate with tools like pylint, bandit, etc.\n    return {\n        \"issues\": [\"Line 5: Unused variable\", \"Line 12: Potential security risk\"],\n        \"score\": 85,\n        \"recommendations\": [\"Use more descriptive variable names\"]\n    }\n</code></pre>"},{"location":"guide/function-calling/#database-operations","title":"Database Operations","text":"<pre><code>class DatabaseQuery(BaseModel):\n    query: str = Field(..., description=\"SQL query to execute\")\n    database: str = Field(\"main\", description=\"Database name\")\n    read_only: bool = Field(True, description=\"Whether query is read-only\")\n\ndb_tool = ToolCall(\n    name=\"execute_sql\",\n    description=\"Execute SQL queries on the database\",\n    input_schema=DatabaseQuery.model_json_schema()\n)\n\nasync def execute_sql(query: str, database: str, read_only: bool):\n    \"\"\"Execute SQL with safety checks.\"\"\"\n    if not read_only and \"SELECT\" not in query.upper():\n        return {\"error\": \"Write operations require read_only=False\"}\n\n    # Execute query safely...\n    return {\"rows\": [], \"affected\": 0}\n</code></pre>"},{"location":"guide/function-calling/#best-practices","title":"Best Practices","text":""},{"location":"guide/function-calling/#tool-design","title":"Tool Design","text":"<ol> <li>Clear Descriptions: Make tool purposes obvious</li> <li>Strong Typing: Use Pydantic for schema validation</li> <li>Error Handling: Return structured error responses</li> <li>Documentation: Include examples in descriptions</li> </ol>"},{"location":"guide/function-calling/#performance","title":"Performance","text":"<ol> <li>Async Operations: Use async/await for I/O operations</li> <li>Timeouts: Set reasonable timeouts for external calls</li> <li>Caching: Cache frequently accessed data</li> <li>Parallel Execution: Execute independent tools concurrently</li> </ol>"},{"location":"guide/function-calling/#security","title":"Security","text":"<ol> <li>Input Validation: Validate all tool inputs</li> <li>Permissions: Check user permissions before tool execution</li> <li>Sandboxing: Isolate dangerous operations</li> <li>Audit Logging: Log all tool executions</li> </ol>"},{"location":"guide/function-calling/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/function-calling/#common-issues","title":"Common Issues","text":""},{"location":"guide/function-calling/#tool-not-called","title":"Tool Not Called","text":"<pre><code># \u274c Vague description\nToolCall(name=\"helper\", description=\"Helps with stuff\")\n\n# \u2705 Clear description  \nToolCall(name=\"calculate_tax\", description=\"Calculate tax amount for a given income and tax rate\")\n</code></pre>"},{"location":"guide/function-calling/#schema-errors","title":"Schema Errors","text":"<pre><code># \u274c Missing required fields\n{\"type\": \"object\", \"properties\": {}}\n\n# \u2705 Complete schema\n{\n    \"type\": \"object\", \n    \"properties\": {\"amount\": {\"type\": \"number\"}},\n    \"required\": [\"amount\"]\n}\n</code></pre>"},{"location":"guide/function-calling/#tool-execution-failures","title":"Tool Execution Failures","text":"<pre><code># Always return structured responses\nasync def execute_tool(name, args):\n    try:\n        result = await actual_tool_function(args)\n        return {\"success\": True, \"data\": result}\n    except Exception as e:\n        return {\"success\": False, \"error\": str(e)}\n</code></pre>"},{"location":"guide/function-calling/#next-steps","title":"Next Steps","text":"<ul> <li>Explore multimodal content</li> <li>Learn about provider configuration</li> <li>See advanced examples</li> </ul> <p>Multimodal Content \u2192 Advanced Examples \u2192</p>"},{"location":"providers/overview/","title":"Provider Overview","text":"<p>v-router supports multiple LLM providers, each with their own strengths and capabilities. This page provides an overview of all supported providers and helps you choose the right one for your use case.</p>"},{"location":"providers/overview/#supported-providers","title":"Supported Providers","text":"Provider Models Function Calling Multimodal Streaming* Cost Anthropic Claude 3, Claude 4 \u2705 \u2705 Images, PDFs \ud83d\udea7 $$$ OpenAI GPT-4, GPT-3.5 \u2705 \u2705 Images \ud83d\udea7 $$$ Google AI Gemini 1.5, 2.0 \u2705 \u2705 Images, PDFs \ud83d\udea7 $$ Azure OpenAI GPT-4, GPT-3.5 \u2705 \u2705 Images \ud83d\udea7 $$$ Vertex AI Claude + Gemini \u2705 \u2705 Images, PDFs \ud83d\udea7 $$ <p>*Streaming support is planned for future releases</p>"},{"location":"providers/overview/#provider-comparison","title":"Provider Comparison","text":""},{"location":"providers/overview/#anthropic","title":"Anthropic","text":"<ul> <li>Best for: High-quality reasoning, analysis, creative writing</li> <li>Models: Claude 3 (Opus, Sonnet, Haiku), Claude 4 (Opus, Sonnet)</li> <li>Strengths: Excellent instruction following, safety, long context</li> <li>Use cases: Complex analysis, creative tasks, code review</li> </ul>"},{"location":"providers/overview/#openai","title":"OpenAI","text":"<ul> <li>Best for: General-purpose tasks, wide ecosystem support</li> <li>Models: GPT-4, GPT-4 Turbo, GPT-4.1, GPT-3.5</li> <li>Strengths: Fast inference, broad capabilities, mature API</li> <li>Use cases: Chat applications, content generation, code assistance</li> </ul>"},{"location":"providers/overview/#google-ai","title":"Google AI","text":"<ul> <li>Best for: Cost-effective multimodal tasks, fast inference</li> <li>Models: Gemini Pro, Gemini 1.5 (Pro, Flash), Gemini 2.0 Flash</li> <li>Strengths: Fast, cost-effective, good multimodal support</li> <li>Use cases: Real-time applications, image analysis, cost-sensitive projects</li> </ul>"},{"location":"providers/overview/#azure-openai","title":"Azure OpenAI","text":"<ul> <li>Best for: Enterprise deployments, compliance requirements</li> <li>Models: Same as OpenAI (GPT-4, GPT-3.5)</li> <li>Strengths: Enterprise features, compliance, data residency</li> <li>Use cases: Enterprise applications, regulated industries</li> </ul>"},{"location":"providers/overview/#vertex-ai","title":"Vertex AI","text":"<ul> <li>Best for: Google Cloud integration, unified access to multiple models</li> <li>Models: Claude 3/4 and Gemini models via Google Cloud</li> <li>Strengths: Single platform for multiple models, enterprise features</li> <li>Use cases: Google Cloud environments, multi-model workflows</li> </ul>"},{"location":"providers/overview/#model-capabilities","title":"Model Capabilities","text":""},{"location":"providers/overview/#text-generation-quality","title":"Text Generation Quality","text":"Provider Creative Writing Technical Analysis Code Generation Anthropic Claude \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 OpenAI GPT-4 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 Google Gemini \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50"},{"location":"providers/overview/#multimodal-capabilities","title":"Multimodal Capabilities","text":"Provider Image Understanding PDF Processing Document Analysis Anthropic \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50\u2b50 OpenAI \u2b50\u2b50\u2b50\u2b50 \u274c \u2b50\u2b50\u2b50 Google \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50\u2b50"},{"location":"providers/overview/#performance-cost","title":"Performance &amp; Cost","text":"Provider Latency Throughput Cost (Relative) Anthropic Medium Medium High OpenAI Fast High High Google Very Fast Very High Low Azure Fast High High Vertex AI Medium High Medium"},{"location":"providers/overview/#choosing-a-provider","title":"Choosing a Provider","text":""},{"location":"providers/overview/#for-development-prototyping","title":"For Development &amp; Prototyping","text":"<pre><code># Start with Google for cost-effectiveness\nLLM(model_name=\"gemini-1.5-pro\", provider=\"google\")\n</code></pre>"},{"location":"providers/overview/#for-production-systems","title":"For Production Systems","text":"<pre><code># Use multiple providers with fallback\nLLM(\n    model_name=\"claude-sonnet-4\",\n    provider=\"anthropic\",\n    backup_models=[\n        BackupModel(model=LLM(model_name=\"gpt-4o\", provider=\"openai\"), priority=1),\n        BackupModel(model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"), priority=2)\n    ],\n    try_other_providers=True\n)\n</code></pre>"},{"location":"providers/overview/#for-enterprise","title":"For Enterprise","text":"<pre><code># Azure for compliance or Vertex for Google Cloud\nLLM(model_name=\"gpt-4\", provider=\"azure\")\n# or\nLLM(model_name=\"claude-sonnet-4\", provider=\"vertexai\")\n</code></pre>"},{"location":"providers/overview/#provider-specific-features","title":"Provider-Specific Features","text":""},{"location":"providers/overview/#anthropic-features","title":"Anthropic Features","text":"<ul> <li>Constitutional AI for safety</li> <li>Large context windows (up to 200K tokens)</li> <li>Excellent at following complex instructions</li> <li>Strong performance on reasoning tasks</li> </ul>"},{"location":"providers/overview/#openai-features","title":"OpenAI Features","text":"<ul> <li>Function calling with JSON schemas</li> <li>Large ecosystem and community</li> <li>Regular model updates</li> <li>Good balance of speed and quality</li> </ul>"},{"location":"providers/overview/#google-features","title":"Google Features","text":"<ul> <li>Very fast inference</li> <li>Cost-effective pricing</li> <li>Integrated with Google Cloud services</li> <li>Strong multimodal capabilities</li> </ul>"},{"location":"providers/overview/#azure-features","title":"Azure Features","text":"<ul> <li>Enterprise-grade security</li> <li>Data residency options</li> <li>SLA guarantees</li> <li>Integration with Microsoft ecosystem</li> </ul>"},{"location":"providers/overview/#vertex-ai-features","title":"Vertex AI Features","text":"<ul> <li>Access to multiple model families</li> <li>Enterprise security and compliance</li> <li>Integrated monitoring and logging</li> <li>Custom model training capabilities</li> </ul>"},{"location":"providers/overview/#authentication-setup","title":"Authentication Setup","text":"<p>Each provider requires different authentication methods:</p> AnthropicOpenAIGoogle AIAzure OpenAIVertex AI <pre><code>export ANTHROPIC_API_KEY=\"your-key\"\n</code></pre> <pre><code>export OPENAI_API_KEY=\"your-key\"\n</code></pre> <pre><code>export GOOGLE_API_KEY=\"your-key\"\n</code></pre> <pre><code>export AZURE_OPENAI_API_KEY=\"your-key\"\nexport AZURE_OPENAI_ENDPOINT=\"your-endpoint\"\n</code></pre> <pre><code>export GOOGLE_APPLICATION_CREDENTIALS=\"path/to/service-account.json\"\nexport GCP_PROJECT_ID=\"your-project\"\n</code></pre>"},{"location":"providers/overview/#cross-provider-compatibility","title":"Cross-Provider Compatibility","text":"<p>v-router ensures consistent behavior across all providers:</p>"},{"location":"providers/overview/#unified-request-format","title":"Unified Request Format","text":"<pre><code># Same request format works with all providers\nresponse = await client.messages.create(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n)\n</code></pre>"},{"location":"providers/overview/#unified-response-format","title":"Unified Response Format","text":"<pre><code># Same response structure from all providers\nprint(response.content[0].text)\nprint(response.model)\nprint(response.provider)\nprint(response.usage.total_tokens)\n</code></pre>"},{"location":"providers/overview/#function-calling-compatibility","title":"Function Calling Compatibility","text":"<pre><code># Same tool definition works across providers\ntool = ToolCall(\n    name=\"get_weather\",\n    description=\"Get weather information\",\n    input_schema=WeatherQuery.model_json_schema()\n)\n\n# Works with Anthropic\nLLM(model_name=\"claude-sonnet-4\", provider=\"anthropic\", tools=Tools(tools=[tool]))\n\n# Works with OpenAI\nLLM(model_name=\"gpt-4o\", provider=\"openai\", tools=Tools(tools=[tool]))\n\n# Works with Google\nLLM(model_name=\"gemini-1.5-pro\", provider=\"google\", tools=Tools(tools=[tool]))\n</code></pre>"},{"location":"providers/overview/#best-practices","title":"Best Practices","text":""},{"location":"providers/overview/#provider-selection-strategy","title":"Provider Selection Strategy","text":"<ol> <li>Primary Provider: Choose based on your main use case</li> <li>Backup Providers: Configure 2-3 alternatives for reliability</li> <li>Cost Optimization: Use cheaper models for backup when appropriate</li> <li>Geographic Considerations: Choose providers with regional availability</li> </ol>"},{"location":"providers/overview/#multi-provider-configuration","title":"Multi-Provider Configuration","text":"<pre><code>def get_production_config():\n    return LLM(\n        model_name=\"claude-sonnet-4\",\n        provider=\"anthropic\",  # Primary: Best quality\n        backup_models=[\n            BackupModel(\n                model=LLM(model_name=\"gpt-4o\", provider=\"openai\"),\n                priority=1  # Backup: Good balance\n            ),\n            BackupModel(\n                model=LLM(model_name=\"gemini-1.5-pro\", provider=\"google\"),\n                priority=2  # Fallback: Cost-effective\n            )\n        ],\n        try_other_providers=True  # Try same model on other providers\n    )\n</code></pre>"},{"location":"providers/overview/#monitoring-and-observability","title":"Monitoring and Observability","text":"<pre><code># Track which providers are being used\nresponse = await client.messages.create(messages=messages)\nprint(f\"Used: {response.model} from {response.provider}\")\n\n# Monitor token usage across providers\nusage_by_provider = {}\nusage_by_provider[response.provider] = usage_by_provider.get(response.provider, 0) + response.usage.total_tokens\n</code></pre>"},{"location":"providers/overview/#provider-roadmap","title":"Provider Roadmap","text":""},{"location":"providers/overview/#planned-additions","title":"Planned Additions","text":"<ul> <li>AWS Bedrock: Access to Claude and other models via AWS</li> <li>Ollama: Local model support for on-premises deployment</li> <li>Cohere: Additional text generation capabilities</li> <li>Replicate: Access to open-source models</li> </ul>"},{"location":"providers/overview/#feature-roadmap","title":"Feature Roadmap","text":"<ul> <li>Streaming: Real-time response streaming across all providers</li> <li>Batch Processing: Efficient batch requests</li> <li>Caching: Response caching for cost optimization</li> <li>Rate Limiting: Built-in rate limiting and retry logic</li> </ul>"},{"location":"providers/overview/#migration-guide","title":"Migration Guide","text":""},{"location":"providers/overview/#from-direct-provider-apis","title":"From Direct Provider APIs","text":"<p>If you're currently using provider APIs directly:</p> From Anthropic SDKFrom OpenAI SDK <pre><code># Before (Anthropic SDK)\nimport anthropic\nclient = anthropic.Anthropic()\nresponse = client.messages.create(\n    model=\"claude-sonnet-4-20250514\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# After (v-router)\nfrom v_router import Client, LLM\nclient = Client(LLM(model_name=\"claude-sonnet-4\", provider=\"anthropic\"))\nresponse = await client.messages.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre> <pre><code># Before (OpenAI SDK)\nimport openai\nclient = openai.OpenAI()\nresponse = client.chat.completions.create(\n    model=\"gpt-4o\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n\n# After (v-router)\nfrom v_router import Client, LLM\nclient = Client(LLM(model_name=\"gpt-4o\", provider=\"openai\"))\nresponse = await client.messages.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}]\n)\n</code></pre>"},{"location":"providers/overview/#benefits-of-migration","title":"Benefits of Migration","text":"<ol> <li>Unified Interface: Same code works with all providers</li> <li>Automatic Fallback: Reliability through redundancy</li> <li>Easy Switching: Change providers without code changes</li> <li>Future-Proof: New providers added without breaking changes</li> </ol>"},{"location":"providers/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Configure specific providers</li> <li>Learn about automatic fallback</li> <li>See provider examples</li> </ul> <p>Provider Configuration \u2192 Examples \u2192</p>"}]}